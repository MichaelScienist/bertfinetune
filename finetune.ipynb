{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In interactive notebook, the `spark` object is already created.\n",
    "Instructors tested with 1 driver, 6 executors of small e4 (24 cores, 192GB memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Launch spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span>tokenizer = DistilBertTokenizer.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'distilbert-base-uncased'</span>)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 # Tokenize your data (as you did before)</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>19 train_encodings = tokenizer(train_texts.tolist(), truncation=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span>val_encodings = tokenizer(val_texts.tolist(), truncation=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 # Load pre-trained model for fine-tuning</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2488</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># input mode in this case.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._in_target_context_manager:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._switch_to_input_mode()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2488 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_one(text=text, text_pair=text_pair, **all_kwargs)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_target <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._switch_to_target_mode()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2491 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>target_encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_one(text=text_target, text_pair=text_pair_targ  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2574</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_one</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\" {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(text_pair)<span style=\"color: #808000; text-decoration-color: #808000\">}.\"</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2572 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2573 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_text_or_text_pairs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(text, text_pair)) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_pair <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">No</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2574 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.batch_encode_plus(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2575 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>batch_text_or_text_pairs=batch_text_or_text_pairs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2576 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>add_special_tokens=add_special_tokens,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2577 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>padding=padding,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2765</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">batch_encode_plus</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2762 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>**kwargs,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2763 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2764 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2765 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._batch_encode_plus(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2766 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_text_or_text_pairs=batch_text_or_text_pairs,                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>add_special_tokens=add_special_tokens,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2768 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>padding_strategy=padding_strategy,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">733</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_batch_encode_plus</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">730 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">731 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>ids, pair_ids = ids_or_pair_ids                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">732 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>733 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>first_ids = get_input_ids(ids)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">734 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>second_ids = get_input_ids(pair_ids) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> pair_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">735 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids.append((first_ids, second_ids))                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">736 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">700</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_input_ids</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">697 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>) -&gt; BatchEncoding:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">698 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_input_ids</span>(text):                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">699 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(text, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>700 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tokens = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenize(text, **kwargs)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">701 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_tokens_to_ids(tokens)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">702 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(text, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(text) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(text[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">703 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_split_into_words:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">547</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">tokenize</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> no_split_token:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">545 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tokenized_text.append(token)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">546 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>547 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tokenized_text.extend(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenize(token))                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">548 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># [\"This\", \" is\", \" something\", \"&lt;special_token_1&gt;\", \"else\"]</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">549 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tokenized_text                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">550 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\tokenization_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">244</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_tokenize</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">241 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_tokenize</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, text):                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">242 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>split_tokens = []                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">243 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.do_basic_tokenize:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>244 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.basic_tokenizer.tokenize(text, never_split=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.all_specia   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">246 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If the token is part of the never_split set</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">247 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.basic_tokenizer.never_split:                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\tokenization_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">431</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">tokenize</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">428 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>token = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_strip_accents(token)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">429 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.strip_accents:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">430 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>token = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_strip_accents(token)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>431 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>split_tokens.extend(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_split_on_punc(token, never_split))               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output_tokens = whitespace_tokenize(<span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>.join(split_tokens))                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">434 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> output_tokens                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\tokenization_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">457</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_run_split_on_punc</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">454 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = []                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">455 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> i &lt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(chars):                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">456 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>char = chars[i]                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>457 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _is_punctuation(char):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">458 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output.append([char])                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">459 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>start_new_word = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">460 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Cache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">294</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_is_punctuation</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_is_punctuation</span>(char):                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">293 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Checks whether `char` is a punctuation character.\"\"\"</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>294 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>cp = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ord</span>(char)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">295 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># We treat all non-letter/number ASCII as punctuation.</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">296 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Characters such as \"^\", \"$\", and \"`\" are not in the Unicode</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">297 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Punctuation class but we treat them as punctuation anyways, for</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m19\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0mtokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[0m\u001b[33mdistilbert-base-uncased\u001b[0m\u001b[33m'\u001b[0m)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m# Tokenize your data (as you did before)\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m19 train_encodings = tokenizer(train_texts.tolist(), truncation=\u001b[94mTrue\u001b[0m, padding=\u001b[94mTrue\u001b[0m)            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0mval_encodings = tokenizer(val_texts.tolist(), truncation=\u001b[94mTrue\u001b[0m, padding=\u001b[94mTrue\u001b[0m)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m# Load pre-trained model for fine-tuning\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m2488\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2485 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# input mode in this case.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2486 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._in_target_context_manager:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2487 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._switch_to_input_mode()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2488 \u001b[2m│   │   │   \u001b[0mencodings = \u001b[96mself\u001b[0m._call_one(text=text, text_pair=text_pair, **all_kwargs)      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2489 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m text_target \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2490 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._switch_to_target_mode()                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2491 \u001b[0m\u001b[2m│   │   │   \u001b[0mtarget_encodings = \u001b[96mself\u001b[0m._call_one(text=text_target, text_pair=text_pair_targ  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m2574\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_one\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2571 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m{\u001b[0m\u001b[96mlen\u001b[0m(text_pair)\u001b[33m}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2572 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2573 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_text_or_text_pairs = \u001b[96mlist\u001b[0m(\u001b[96mzip\u001b[0m(text, text_pair)) \u001b[94mif\u001b[0m text_pair \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2574 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.batch_encode_plus(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2575 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbatch_text_or_text_pairs=batch_text_or_text_pairs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2576 \u001b[0m\u001b[2m│   │   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2577 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpadding=padding,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m2765\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbatch_encode_plus\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2762 \u001b[0m\u001b[2m│   │   │   \u001b[0m**kwargs,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2763 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2764 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2765 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._batch_encode_plus(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2766 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_text_or_text_pairs=batch_text_or_text_pairs,                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2767 \u001b[0m\u001b[2m│   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2768 \u001b[0m\u001b[2m│   │   │   \u001b[0mpadding_strategy=padding_strategy,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m733\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_batch_encode_plus\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m730 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m731 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mids, pair_ids = ids_or_pair_ids                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m732 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m733 \u001b[2m│   │   │   \u001b[0mfirst_ids = get_input_ids(ids)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m734 \u001b[0m\u001b[2m│   │   │   \u001b[0msecond_ids = get_input_ids(pair_ids) \u001b[94mif\u001b[0m pair_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m735 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids.append((first_ids, second_ids))                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m736 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m700\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_input_ids\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m697 \u001b[0m\u001b[2m│   \u001b[0m) -> BatchEncoding:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m698 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_input_ids\u001b[0m(text):                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m699 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(text, \u001b[96mstr\u001b[0m):                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m700 \u001b[2m│   │   │   │   \u001b[0mtokens = \u001b[96mself\u001b[0m.tokenize(text, **kwargs)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m701 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.convert_tokens_to_ids(tokens)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m702 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(text, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)) \u001b[95mand\u001b[0m \u001b[96mlen\u001b[0m(text) > \u001b[94m0\u001b[0m \u001b[95mand\u001b[0m \u001b[96misinstance\u001b[0m(text[\u001b[94m0\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m703 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m is_split_into_words:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m547\u001b[0m in \u001b[92mtokenize\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m544 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m token \u001b[95min\u001b[0m no_split_token:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m545 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtokenized_text.append(token)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m546 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m547 \u001b[2m│   │   │   │   \u001b[0mtokenized_text.extend(\u001b[96mself\u001b[0m._tokenize(token))                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m548 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m549 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m tokenized_text                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m550 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m:\u001b[94m244\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_tokenize\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m241 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_tokenize\u001b[0m(\u001b[96mself\u001b[0m, text):                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m242 \u001b[0m\u001b[2m│   │   \u001b[0msplit_tokens = []                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m243 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.do_basic_tokenize:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m244 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m token \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.basic_tokenizer.tokenize(text, never_split=\u001b[96mself\u001b[0m.all_specia   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# If the token is part of the never_split set\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m247 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m token \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.basic_tokenizer.never_split:                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m:\u001b[94m431\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtokenize\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m428 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtoken = \u001b[96mself\u001b[0m._run_strip_accents(token)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m429 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.strip_accents:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m430 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtoken = \u001b[96mself\u001b[0m._run_strip_accents(token)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m431 \u001b[2m│   │   │   \u001b[0msplit_tokens.extend(\u001b[96mself\u001b[0m._run_split_on_punc(token, never_split))               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m432 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m433 \u001b[0m\u001b[2m│   │   \u001b[0moutput_tokens = whitespace_tokenize(\u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m.join(split_tokens))                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m434 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m output_tokens                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m:\u001b[94m457\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_run_split_on_punc\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m454 \u001b[0m\u001b[2m│   │   \u001b[0moutput = []                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m i < \u001b[96mlen\u001b[0m(chars):                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   │   │   \u001b[0mchar = chars[i]                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m457 \u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m _is_punctuation(char):                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m458 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput.append([char])                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstart_new_word = \u001b[94mTrue\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\Local\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m294\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_is_punctuation\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_is_punctuation\u001b[0m(char):                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m293 \u001b[0m\u001b[2m│   \u001b[0m\u001b[33m\"\"\"Checks whether `char` is a punctuation character.\"\"\"\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m294 \u001b[2m│   \u001b[0mcp = \u001b[96mord\u001b[0m(char)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# We treat all non-letter/number ASCII as punctuation.\u001b[0m                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m297 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Punctuation class but we treat them as punctuation anyways, for\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# GET data\n",
    "df = pd.read_csv('train_dataset.csv')\n",
    "df.columns = ['body', 'label']  \n",
    "\n",
    "# split \n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['body'], df['label'], test_size=.2)\n",
    "\n",
    "#  tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        # change the label to 1 or 0\n",
    "        self.labels = [1 if label == 'positive' else 0 for label in labels]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # tensor of 1 or 0\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/10\n",
      "Epoch 1, Step 5/1750 (0.29%), Loss: 0.7779\n",
      "Epoch 1, Step 10/1750 (0.57%), Loss: 0.6948\n",
      "Epoch 1, Step 15/1750 (0.86%), Loss: 0.6695\n",
      "Epoch 1, Step 20/1750 (1.14%), Loss: 0.6387\n",
      "Epoch 1, Step 25/1750 (1.43%), Loss: 0.5104\n",
      "Epoch 1, Step 30/1750 (1.71%), Loss: 0.5361\n",
      "Epoch 1, Step 35/1750 (2.00%), Loss: 0.4209\n",
      "Epoch 1, Step 40/1750 (2.29%), Loss: 0.6797\n",
      "Epoch 1, Step 45/1750 (2.57%), Loss: 0.5841\n",
      "Epoch 1, Step 50/1750 (2.86%), Loss: 0.3783\n",
      "Epoch 1, Step 55/1750 (3.14%), Loss: 0.3943\n",
      "Epoch 1, Step 60/1750 (3.43%), Loss: 0.4011\n",
      "Epoch 1, Step 65/1750 (3.71%), Loss: 0.3599\n",
      "Epoch 1, Step 70/1750 (4.00%), Loss: 0.3869\n",
      "Epoch 1, Step 75/1750 (4.29%), Loss: 0.4519\n",
      "Epoch 1, Step 80/1750 (4.57%), Loss: 0.6563\n",
      "Epoch 1, Step 85/1750 (4.86%), Loss: 0.6288\n",
      "Epoch 1, Step 90/1750 (5.14%), Loss: 0.4963\n",
      "Epoch 1, Step 95/1750 (5.43%), Loss: 0.1124\n",
      "Epoch 1, Step 100/1750 (5.71%), Loss: 0.3515\n",
      "Epoch 1, Step 105/1750 (6.00%), Loss: 0.4908\n",
      "Epoch 1, Step 110/1750 (6.29%), Loss: 0.2030\n",
      "Epoch 1, Step 115/1750 (6.57%), Loss: 0.1752\n",
      "Epoch 1, Step 120/1750 (6.86%), Loss: 0.3872\n",
      "Epoch 1, Step 125/1750 (7.14%), Loss: 0.4164\n",
      "Epoch 1, Step 130/1750 (7.43%), Loss: 0.1431\n",
      "Epoch 1, Step 135/1750 (7.71%), Loss: 0.3093\n",
      "Epoch 1, Step 140/1750 (8.00%), Loss: 0.5615\n",
      "Epoch 1, Step 145/1750 (8.29%), Loss: 0.4486\n",
      "Epoch 1, Step 150/1750 (8.57%), Loss: 0.2479\n",
      "Epoch 1, Step 155/1750 (8.86%), Loss: 0.1711\n",
      "Epoch 1, Step 160/1750 (9.14%), Loss: 0.3785\n",
      "Epoch 1, Step 165/1750 (9.43%), Loss: 0.1914\n",
      "Epoch 1, Step 170/1750 (9.71%), Loss: 0.3441\n",
      "Epoch 1, Step 175/1750 (10.00%), Loss: 0.5314\n",
      "Epoch 1, Step 180/1750 (10.29%), Loss: 0.2867\n",
      "Epoch 1, Step 185/1750 (10.57%), Loss: 0.3521\n",
      "Epoch 1, Step 190/1750 (10.86%), Loss: 0.2343\n",
      "Epoch 1, Step 195/1750 (11.14%), Loss: 0.4329\n",
      "Epoch 1, Step 200/1750 (11.43%), Loss: 0.2051\n",
      "Epoch 1, Step 205/1750 (11.71%), Loss: 0.2705\n",
      "Epoch 1, Step 210/1750 (12.00%), Loss: 0.1587\n",
      "Epoch 1, Step 215/1750 (12.29%), Loss: 0.3211\n",
      "Epoch 1, Step 220/1750 (12.57%), Loss: 0.4723\n",
      "Epoch 1, Step 225/1750 (12.86%), Loss: 0.1919\n",
      "Epoch 1, Step 230/1750 (13.14%), Loss: 0.3579\n",
      "Epoch 1, Step 235/1750 (13.43%), Loss: 0.2949\n",
      "Epoch 1, Step 240/1750 (13.71%), Loss: 0.1973\n",
      "Epoch 1, Step 245/1750 (14.00%), Loss: 0.2709\n",
      "Epoch 1, Step 250/1750 (14.29%), Loss: 0.2447\n",
      "Epoch 1, Step 255/1750 (14.57%), Loss: 0.1920\n",
      "Epoch 1, Step 260/1750 (14.86%), Loss: 0.1287\n",
      "Epoch 1, Step 265/1750 (15.14%), Loss: 0.1682\n",
      "Epoch 1, Step 270/1750 (15.43%), Loss: 0.3520\n",
      "Epoch 1, Step 275/1750 (15.71%), Loss: 0.6788\n",
      "Epoch 1, Step 280/1750 (16.00%), Loss: 0.2098\n",
      "Epoch 1, Step 285/1750 (16.29%), Loss: 0.2466\n",
      "Epoch 1, Step 290/1750 (16.57%), Loss: 0.2225\n",
      "Epoch 1, Step 295/1750 (16.86%), Loss: 0.3576\n",
      "Epoch 1, Step 300/1750 (17.14%), Loss: 0.1978\n",
      "Epoch 1, Step 305/1750 (17.43%), Loss: 0.2834\n",
      "Epoch 1, Step 310/1750 (17.71%), Loss: 0.4185\n",
      "Epoch 1, Step 315/1750 (18.00%), Loss: 0.6176\n",
      "Epoch 1, Step 320/1750 (18.29%), Loss: 0.2276\n",
      "Epoch 1, Step 325/1750 (18.57%), Loss: 0.3216\n",
      "Epoch 1, Step 330/1750 (18.86%), Loss: 0.3719\n",
      "Epoch 1, Step 335/1750 (19.14%), Loss: 0.0772\n",
      "Epoch 1, Step 340/1750 (19.43%), Loss: 0.1288\n",
      "Epoch 1, Step 345/1750 (19.71%), Loss: 0.3570\n",
      "Epoch 1, Step 350/1750 (20.00%), Loss: 0.2805\n",
      "Epoch 1, Step 355/1750 (20.29%), Loss: 0.3798\n",
      "Epoch 1, Step 360/1750 (20.57%), Loss: 0.2431\n",
      "Epoch 1, Step 365/1750 (20.86%), Loss: 0.0802\n",
      "Epoch 1, Step 370/1750 (21.14%), Loss: 0.1040\n",
      "Epoch 1, Step 375/1750 (21.43%), Loss: 0.3084\n",
      "Epoch 1, Step 380/1750 (21.71%), Loss: 0.1272\n",
      "Epoch 1, Step 385/1750 (22.00%), Loss: 0.3121\n",
      "Epoch 1, Step 390/1750 (22.29%), Loss: 0.2614\n",
      "Epoch 1, Step 395/1750 (22.57%), Loss: 0.1032\n",
      "Epoch 1, Step 400/1750 (22.86%), Loss: 0.3744\n",
      "Epoch 1, Step 405/1750 (23.14%), Loss: 0.1852\n",
      "Epoch 1, Step 410/1750 (23.43%), Loss: 0.3118\n",
      "Epoch 1, Step 415/1750 (23.71%), Loss: 0.2111\n",
      "Epoch 1, Step 420/1750 (24.00%), Loss: 0.1899\n",
      "Epoch 1, Step 425/1750 (24.29%), Loss: 0.1504\n",
      "Epoch 1, Step 430/1750 (24.57%), Loss: 0.1143\n",
      "Epoch 1, Step 435/1750 (24.86%), Loss: 0.4077\n",
      "Epoch 1, Step 440/1750 (25.14%), Loss: 0.4535\n",
      "Epoch 1, Step 445/1750 (25.43%), Loss: 0.3190\n",
      "Epoch 1, Step 450/1750 (25.71%), Loss: 0.3182\n",
      "Epoch 1, Step 455/1750 (26.00%), Loss: 0.1179\n",
      "Epoch 1, Step 460/1750 (26.29%), Loss: 0.2793\n",
      "Epoch 1, Step 465/1750 (26.57%), Loss: 0.6313\n",
      "Epoch 1, Step 470/1750 (26.86%), Loss: 0.0526\n",
      "Epoch 1, Step 475/1750 (27.14%), Loss: 0.8115\n",
      "Epoch 1, Step 480/1750 (27.43%), Loss: 0.0811\n",
      "Epoch 1, Step 485/1750 (27.71%), Loss: 0.2124\n",
      "Epoch 1, Step 490/1750 (28.00%), Loss: 0.3213\n",
      "Epoch 1, Step 495/1750 (28.29%), Loss: 0.3918\n",
      "Epoch 1, Step 500/1750 (28.57%), Loss: 0.0981\n",
      "Epoch 1, Step 505/1750 (28.86%), Loss: 0.1069\n",
      "Epoch 1, Step 510/1750 (29.14%), Loss: 0.2810\n",
      "Epoch 1, Step 515/1750 (29.43%), Loss: 0.4237\n",
      "Epoch 1, Step 520/1750 (29.71%), Loss: 0.1062\n",
      "Epoch 1, Step 525/1750 (30.00%), Loss: 0.2740\n",
      "Epoch 1, Step 530/1750 (30.29%), Loss: 0.4011\n",
      "Epoch 1, Step 535/1750 (30.57%), Loss: 0.1933\n",
      "Epoch 1, Step 540/1750 (30.86%), Loss: 0.2236\n",
      "Epoch 1, Step 545/1750 (31.14%), Loss: 0.1802\n",
      "Epoch 1, Step 550/1750 (31.43%), Loss: 0.0712\n",
      "Epoch 1, Step 555/1750 (31.71%), Loss: 0.1094\n",
      "Epoch 1, Step 560/1750 (32.00%), Loss: 0.1646\n",
      "Epoch 1, Step 565/1750 (32.29%), Loss: 0.2241\n",
      "Epoch 1, Step 570/1750 (32.57%), Loss: 0.1626\n",
      "Epoch 1, Step 575/1750 (32.86%), Loss: 0.3106\n",
      "Epoch 1, Step 580/1750 (33.14%), Loss: 0.3622\n",
      "Epoch 1, Step 585/1750 (33.43%), Loss: 0.3630\n",
      "Epoch 1, Step 590/1750 (33.71%), Loss: 0.2710\n",
      "Epoch 1, Step 595/1750 (34.00%), Loss: 0.3694\n",
      "Epoch 1, Step 600/1750 (34.29%), Loss: 0.0815\n",
      "Epoch 1, Step 605/1750 (34.57%), Loss: 0.1952\n",
      "Epoch 1, Step 610/1750 (34.86%), Loss: 0.4055\n",
      "Epoch 1, Step 615/1750 (35.14%), Loss: 0.2311\n",
      "Epoch 1, Step 620/1750 (35.43%), Loss: 0.3060\n",
      "Epoch 1, Step 625/1750 (35.71%), Loss: 0.0838\n",
      "Epoch 1, Step 630/1750 (36.00%), Loss: 0.1873\n",
      "Epoch 1, Step 635/1750 (36.29%), Loss: 0.2181\n",
      "Epoch 1, Step 640/1750 (36.57%), Loss: 0.1707\n",
      "Epoch 1, Step 645/1750 (36.86%), Loss: 0.3738\n",
      "Epoch 1, Step 650/1750 (37.14%), Loss: 0.2604\n",
      "Epoch 1, Step 655/1750 (37.43%), Loss: 0.3623\n",
      "Epoch 1, Step 660/1750 (37.71%), Loss: 0.2089\n",
      "Epoch 1, Step 665/1750 (38.00%), Loss: 0.2660\n",
      "Epoch 1, Step 670/1750 (38.29%), Loss: 0.2172\n",
      "Epoch 1, Step 675/1750 (38.57%), Loss: 0.2815\n",
      "Epoch 1, Step 680/1750 (38.86%), Loss: 0.2220\n",
      "Epoch 1, Step 685/1750 (39.14%), Loss: 0.2926\n",
      "Epoch 1, Step 690/1750 (39.43%), Loss: 0.2554\n",
      "Epoch 1, Step 695/1750 (39.71%), Loss: 0.1204\n",
      "Epoch 1, Step 700/1750 (40.00%), Loss: 0.2001\n",
      "Epoch 1, Step 705/1750 (40.29%), Loss: 0.2277\n",
      "Epoch 1, Step 710/1750 (40.57%), Loss: 0.0729\n",
      "Epoch 1, Step 715/1750 (40.86%), Loss: 0.4131\n",
      "Epoch 1, Step 720/1750 (41.14%), Loss: 0.1540\n",
      "Epoch 1, Step 725/1750 (41.43%), Loss: 0.5089\n",
      "Epoch 1, Step 730/1750 (41.71%), Loss: 0.3592\n",
      "Epoch 1, Step 735/1750 (42.00%), Loss: 0.2350\n",
      "Epoch 1, Step 740/1750 (42.29%), Loss: 0.3345\n",
      "Epoch 1, Step 745/1750 (42.57%), Loss: 0.1289\n",
      "Epoch 1, Step 750/1750 (42.86%), Loss: 0.6407\n",
      "Epoch 1, Step 755/1750 (43.14%), Loss: 0.4245\n",
      "Epoch 1, Step 760/1750 (43.43%), Loss: 0.2662\n",
      "Epoch 1, Step 765/1750 (43.71%), Loss: 0.1332\n",
      "Epoch 1, Step 770/1750 (44.00%), Loss: 0.2039\n",
      "Epoch 1, Step 775/1750 (44.29%), Loss: 0.1924\n",
      "Epoch 1, Step 780/1750 (44.57%), Loss: 0.3328\n",
      "Epoch 1, Step 785/1750 (44.86%), Loss: 0.1425\n",
      "Epoch 1, Step 790/1750 (45.14%), Loss: 0.3994\n",
      "Epoch 1, Step 795/1750 (45.43%), Loss: 0.1416\n",
      "Epoch 1, Step 800/1750 (45.71%), Loss: 0.4385\n",
      "Epoch 1, Step 805/1750 (46.00%), Loss: 0.2054\n",
      "Epoch 1, Step 810/1750 (46.29%), Loss: 0.1455\n",
      "Epoch 1, Step 815/1750 (46.57%), Loss: 0.4120\n",
      "Epoch 1, Step 820/1750 (46.86%), Loss: 0.3185\n",
      "Epoch 1, Step 825/1750 (47.14%), Loss: 0.1200\n",
      "Epoch 1, Step 830/1750 (47.43%), Loss: 0.1680\n",
      "Epoch 1, Step 835/1750 (47.71%), Loss: 0.7485\n",
      "Epoch 1, Step 840/1750 (48.00%), Loss: 0.6405\n",
      "Epoch 1, Step 845/1750 (48.29%), Loss: 0.3441\n",
      "Epoch 1, Step 850/1750 (48.57%), Loss: 0.2832\n",
      "Epoch 1, Step 855/1750 (48.86%), Loss: 0.0890\n",
      "Epoch 1, Step 860/1750 (49.14%), Loss: 0.0535\n",
      "Epoch 1, Step 865/1750 (49.43%), Loss: 0.0533\n",
      "Epoch 1, Step 870/1750 (49.71%), Loss: 0.0337\n",
      "Epoch 1, Step 875/1750 (50.00%), Loss: 0.0662\n",
      "Epoch 1, Step 880/1750 (50.29%), Loss: 0.0428\n",
      "Epoch 1, Step 885/1750 (50.57%), Loss: 0.0877\n",
      "Epoch 1, Step 890/1750 (50.86%), Loss: 0.8206\n",
      "Epoch 1, Step 895/1750 (51.14%), Loss: 0.1381\n",
      "Epoch 1, Step 900/1750 (51.43%), Loss: 0.1263\n",
      "Epoch 1, Step 905/1750 (51.71%), Loss: 0.0770\n",
      "Epoch 1, Step 910/1750 (52.00%), Loss: 0.0826\n",
      "Epoch 1, Step 915/1750 (52.29%), Loss: 0.2438\n",
      "Epoch 1, Step 920/1750 (52.57%), Loss: 0.4187\n",
      "Epoch 1, Step 925/1750 (52.86%), Loss: 0.3760\n",
      "Epoch 1, Step 930/1750 (53.14%), Loss: 0.1978\n",
      "Epoch 1, Step 935/1750 (53.43%), Loss: 0.3833\n",
      "Epoch 1, Step 940/1750 (53.71%), Loss: 0.0609\n",
      "Epoch 1, Step 945/1750 (54.00%), Loss: 0.1439\n",
      "Epoch 1, Step 950/1750 (54.29%), Loss: 0.2642\n",
      "Epoch 1, Step 955/1750 (54.57%), Loss: 0.1870\n",
      "Epoch 1, Step 960/1750 (54.86%), Loss: 0.0753\n",
      "Epoch 1, Step 965/1750 (55.14%), Loss: 0.1169\n",
      "Epoch 1, Step 970/1750 (55.43%), Loss: 0.2452\n",
      "Epoch 1, Step 975/1750 (55.71%), Loss: 0.3768\n",
      "Epoch 1, Step 980/1750 (56.00%), Loss: 0.2525\n",
      "Epoch 1, Step 985/1750 (56.29%), Loss: 0.1631\n",
      "Epoch 1, Step 990/1750 (56.57%), Loss: 0.0635\n",
      "Epoch 1, Step 995/1750 (56.86%), Loss: 0.1725\n",
      "Epoch 1, Step 1000/1750 (57.14%), Loss: 0.1646\n",
      "Epoch 1, Step 1005/1750 (57.43%), Loss: 0.0738\n",
      "Epoch 1, Step 1010/1750 (57.71%), Loss: 0.0612\n",
      "Epoch 1, Step 1015/1750 (58.00%), Loss: 0.2084\n",
      "Epoch 1, Step 1020/1750 (58.29%), Loss: 0.1259\n",
      "Epoch 1, Step 1025/1750 (58.57%), Loss: 0.3340\n",
      "Epoch 1, Step 1030/1750 (58.86%), Loss: 0.2008\n",
      "Epoch 1, Step 1035/1750 (59.14%), Loss: 0.6762\n",
      "Epoch 1, Step 1040/1750 (59.43%), Loss: 0.2187\n",
      "Epoch 1, Step 1045/1750 (59.71%), Loss: 0.2092\n",
      "Epoch 1, Step 1050/1750 (60.00%), Loss: 0.2267\n",
      "Epoch 1, Step 1055/1750 (60.29%), Loss: 0.0639\n",
      "Epoch 1, Step 1060/1750 (60.57%), Loss: 0.2232\n",
      "Epoch 1, Step 1065/1750 (60.86%), Loss: 0.3849\n",
      "Epoch 1, Step 1070/1750 (61.14%), Loss: 0.1231\n",
      "Epoch 1, Step 1075/1750 (61.43%), Loss: 0.5496\n",
      "Epoch 1, Step 1080/1750 (61.71%), Loss: 0.1175\n",
      "Epoch 1, Step 1085/1750 (62.00%), Loss: 0.2860\n",
      "Epoch 1, Step 1090/1750 (62.29%), Loss: 0.2553\n",
      "Epoch 1, Step 1095/1750 (62.57%), Loss: 0.2100\n",
      "Epoch 1, Step 1100/1750 (62.86%), Loss: 0.0346\n",
      "Epoch 1, Step 1105/1750 (63.14%), Loss: 0.1372\n",
      "Epoch 1, Step 1110/1750 (63.43%), Loss: 0.2353\n",
      "Epoch 1, Step 1115/1750 (63.71%), Loss: 0.1498\n",
      "Epoch 1, Step 1120/1750 (64.00%), Loss: 0.3462\n",
      "Epoch 1, Step 1125/1750 (64.29%), Loss: 0.5366\n",
      "Epoch 1, Step 1130/1750 (64.57%), Loss: 0.2558\n",
      "Epoch 1, Step 1135/1750 (64.86%), Loss: 0.0368\n",
      "Epoch 1, Step 1140/1750 (65.14%), Loss: 0.3023\n",
      "Epoch 1, Step 1145/1750 (65.43%), Loss: 0.0939\n",
      "Epoch 1, Step 1150/1750 (65.71%), Loss: 0.4309\n",
      "Epoch 1, Step 1155/1750 (66.00%), Loss: 0.5149\n",
      "Epoch 1, Step 1160/1750 (66.29%), Loss: 0.0578\n",
      "Epoch 1, Step 1165/1750 (66.57%), Loss: 0.3861\n",
      "Epoch 1, Step 1170/1750 (66.86%), Loss: 0.4776\n",
      "Epoch 1, Step 1175/1750 (67.14%), Loss: 0.6404\n",
      "Epoch 1, Step 1180/1750 (67.43%), Loss: 0.1315\n",
      "Epoch 1, Step 1185/1750 (67.71%), Loss: 0.2072\n",
      "Epoch 1, Step 1190/1750 (68.00%), Loss: 0.1250\n",
      "Epoch 1, Step 1195/1750 (68.29%), Loss: 0.1488\n",
      "Epoch 1, Step 1200/1750 (68.57%), Loss: 0.1832\n",
      "Epoch 1, Step 1205/1750 (68.86%), Loss: 0.2133\n",
      "Epoch 1, Step 1210/1750 (69.14%), Loss: 0.1495\n",
      "Epoch 1, Step 1215/1750 (69.43%), Loss: 0.1118\n",
      "Epoch 1, Step 1220/1750 (69.71%), Loss: 0.2792\n",
      "Epoch 1, Step 1225/1750 (70.00%), Loss: 0.3910\n",
      "Epoch 1, Step 1230/1750 (70.29%), Loss: 0.2220\n",
      "Epoch 1, Step 1235/1750 (70.57%), Loss: 0.0764\n",
      "Epoch 1, Step 1240/1750 (70.86%), Loss: 0.3709\n",
      "Epoch 1, Step 1245/1750 (71.14%), Loss: 0.7327\n",
      "Epoch 1, Step 1250/1750 (71.43%), Loss: 0.1902\n",
      "Epoch 1, Step 1255/1750 (71.71%), Loss: 0.3162\n",
      "Epoch 1, Step 1260/1750 (72.00%), Loss: 0.1332\n",
      "Epoch 1, Step 1265/1750 (72.29%), Loss: 0.5259\n",
      "Epoch 1, Step 1270/1750 (72.57%), Loss: 0.1666\n",
      "Epoch 1, Step 1275/1750 (72.86%), Loss: 0.3460\n",
      "Epoch 1, Step 1280/1750 (73.14%), Loss: 0.0881\n",
      "Epoch 1, Step 1285/1750 (73.43%), Loss: 0.2387\n",
      "Epoch 1, Step 1290/1750 (73.71%), Loss: 0.1890\n",
      "Epoch 1, Step 1295/1750 (74.00%), Loss: 0.3276\n",
      "Epoch 1, Step 1300/1750 (74.29%), Loss: 0.1563\n",
      "Epoch 1, Step 1305/1750 (74.57%), Loss: 0.3350\n",
      "Epoch 1, Step 1310/1750 (74.86%), Loss: 0.4771\n",
      "Epoch 1, Step 1315/1750 (75.14%), Loss: 0.2067\n",
      "Epoch 1, Step 1320/1750 (75.43%), Loss: 0.4359\n",
      "Epoch 1, Step 1325/1750 (75.71%), Loss: 0.2160\n",
      "Epoch 1, Step 1330/1750 (76.00%), Loss: 0.5171\n",
      "Epoch 1, Step 1335/1750 (76.29%), Loss: 0.1785\n",
      "Epoch 1, Step 1340/1750 (76.57%), Loss: 0.1529\n",
      "Epoch 1, Step 1345/1750 (76.86%), Loss: 0.2937\n",
      "Epoch 1, Step 1350/1750 (77.14%), Loss: 0.2522\n",
      "Epoch 1, Step 1355/1750 (77.43%), Loss: 0.1349\n",
      "Epoch 1, Step 1360/1750 (77.71%), Loss: 0.2192\n",
      "Epoch 1, Step 1365/1750 (78.00%), Loss: 0.0346\n",
      "Epoch 1, Step 1370/1750 (78.29%), Loss: 0.1022\n",
      "Epoch 1, Step 1375/1750 (78.57%), Loss: 0.1830\n",
      "Epoch 1, Step 1380/1750 (78.86%), Loss: 0.0291\n",
      "Epoch 1, Step 1385/1750 (79.14%), Loss: 0.1539\n",
      "Epoch 1, Step 1390/1750 (79.43%), Loss: 0.2253\n",
      "Epoch 1, Step 1395/1750 (79.71%), Loss: 0.3338\n",
      "Epoch 1, Step 1400/1750 (80.00%), Loss: 0.3153\n",
      "Epoch 1, Step 1405/1750 (80.29%), Loss: 0.1525\n",
      "Epoch 1, Step 1410/1750 (80.57%), Loss: 0.1045\n",
      "Epoch 1, Step 1415/1750 (80.86%), Loss: 0.0892\n",
      "Epoch 1, Step 1420/1750 (81.14%), Loss: 0.1020\n",
      "Epoch 1, Step 1425/1750 (81.43%), Loss: 0.0738\n",
      "Epoch 1, Step 1430/1750 (81.71%), Loss: 0.1498\n",
      "Epoch 1, Step 1435/1750 (82.00%), Loss: 0.0805\n",
      "Epoch 1, Step 1440/1750 (82.29%), Loss: 0.3993\n",
      "Epoch 1, Step 1445/1750 (82.57%), Loss: 0.2470\n",
      "Epoch 1, Step 1450/1750 (82.86%), Loss: 0.3325\n",
      "Epoch 1, Step 1455/1750 (83.14%), Loss: 0.2403\n",
      "Epoch 1, Step 1460/1750 (83.43%), Loss: 0.1240\n",
      "Epoch 1, Step 1465/1750 (83.71%), Loss: 0.1199\n",
      "Epoch 1, Step 1470/1750 (84.00%), Loss: 0.5450\n",
      "Epoch 1, Step 1475/1750 (84.29%), Loss: 0.2102\n",
      "Epoch 1, Step 1480/1750 (84.57%), Loss: 0.2824\n",
      "Epoch 1, Step 1485/1750 (84.86%), Loss: 0.1075\n",
      "Epoch 1, Step 1490/1750 (85.14%), Loss: 0.8090\n",
      "Epoch 1, Step 1495/1750 (85.43%), Loss: 0.2300\n",
      "Epoch 1, Step 1500/1750 (85.71%), Loss: 0.0767\n",
      "Epoch 1, Step 1505/1750 (86.00%), Loss: 0.1649\n",
      "Epoch 1, Step 1510/1750 (86.29%), Loss: 0.5496\n",
      "Epoch 1, Step 1515/1750 (86.57%), Loss: 0.1780\n",
      "Epoch 1, Step 1520/1750 (86.86%), Loss: 0.4915\n",
      "Epoch 1, Step 1525/1750 (87.14%), Loss: 0.1987\n",
      "Epoch 1, Step 1530/1750 (87.43%), Loss: 0.3358\n",
      "Epoch 1, Step 1535/1750 (87.71%), Loss: 0.0643\n",
      "Epoch 1, Step 1540/1750 (88.00%), Loss: 0.2241\n",
      "Epoch 1, Step 1545/1750 (88.29%), Loss: 0.3442\n",
      "Epoch 1, Step 1550/1750 (88.57%), Loss: 0.3755\n",
      "Epoch 1, Step 1555/1750 (88.86%), Loss: 0.1706\n",
      "Epoch 1, Step 1560/1750 (89.14%), Loss: 0.2342\n",
      "Epoch 1, Step 1565/1750 (89.43%), Loss: 0.3420\n",
      "Epoch 1, Step 1570/1750 (89.71%), Loss: 0.2336\n",
      "Epoch 1, Step 1575/1750 (90.00%), Loss: 0.0842\n",
      "Epoch 1, Step 1580/1750 (90.29%), Loss: 0.1254\n",
      "Epoch 1, Step 1585/1750 (90.57%), Loss: 0.3872\n",
      "Epoch 1, Step 1590/1750 (90.86%), Loss: 0.2329\n",
      "Epoch 1, Step 1595/1750 (91.14%), Loss: 0.2235\n",
      "Epoch 1, Step 1600/1750 (91.43%), Loss: 0.0671\n",
      "Epoch 1, Step 1605/1750 (91.71%), Loss: 0.5163\n",
      "Epoch 1, Step 1610/1750 (92.00%), Loss: 0.1930\n",
      "Epoch 1, Step 1615/1750 (92.29%), Loss: 0.1178\n",
      "Epoch 1, Step 1620/1750 (92.57%), Loss: 0.0481\n",
      "Epoch 1, Step 1625/1750 (92.86%), Loss: 0.1297\n",
      "Epoch 1, Step 1630/1750 (93.14%), Loss: 0.4611\n",
      "Epoch 1, Step 1635/1750 (93.43%), Loss: 0.2288\n",
      "Epoch 1, Step 1640/1750 (93.71%), Loss: 0.4529\n",
      "Epoch 1, Step 1645/1750 (94.00%), Loss: 0.1767\n",
      "Epoch 1, Step 1650/1750 (94.29%), Loss: 0.0745\n",
      "Epoch 1, Step 1655/1750 (94.57%), Loss: 0.2214\n",
      "Epoch 1, Step 1660/1750 (94.86%), Loss: 0.3247\n",
      "Epoch 1, Step 1665/1750 (95.14%), Loss: 0.0541\n",
      "Epoch 1, Step 1670/1750 (95.43%), Loss: 0.1233\n",
      "Epoch 1, Step 1675/1750 (95.71%), Loss: 0.1352\n",
      "Epoch 1, Step 1680/1750 (96.00%), Loss: 0.4883\n",
      "Epoch 1, Step 1685/1750 (96.29%), Loss: 0.1355\n",
      "Epoch 1, Step 1690/1750 (96.57%), Loss: 0.3485\n",
      "Epoch 1, Step 1695/1750 (96.86%), Loss: 0.0678\n",
      "Epoch 1, Step 1700/1750 (97.14%), Loss: 0.2321\n",
      "Epoch 1, Step 1705/1750 (97.43%), Loss: 0.1093\n",
      "Epoch 1, Step 1710/1750 (97.71%), Loss: 0.1155\n",
      "Epoch 1, Step 1715/1750 (98.00%), Loss: 0.0400\n",
      "Epoch 1, Step 1720/1750 (98.29%), Loss: 0.0222\n",
      "Epoch 1, Step 1725/1750 (98.57%), Loss: 0.2647\n",
      "Epoch 1, Step 1730/1750 (98.86%), Loss: 0.0566\n",
      "Epoch 1, Step 1735/1750 (99.14%), Loss: 0.0816\n",
      "Epoch 1, Step 1740/1750 (99.43%), Loss: 0.1252\n",
      "Epoch 1, Step 1745/1750 (99.71%), Loss: 0.2652\n",
      "Epoch 1, Step 1750/1750 (100.00%), Loss: 0.3722\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.10\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.05\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.02\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.31\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.44\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.24\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.22\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.43\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.09\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.14\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.11\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.15\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.06\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.16\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 0.24\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.24\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.16\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.26\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.13\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.30\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.28\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.22\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.26\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.40\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.32\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.04\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 0.57\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.26\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.13\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.04\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.09\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.12\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.17\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.06\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.23\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 0.48\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.10\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.19\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.28\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.27\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.23\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.34\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.23\n",
      "End of Epoch 1/10 - Average Train Loss: 0.26, Average Validation Loss: 0.21\n",
      "Starting Epoch 2/10\n",
      "Epoch 2, Step 5/1750 (0.29%), Loss: 0.0809\n",
      "Epoch 2, Step 10/1750 (0.57%), Loss: 0.3423\n",
      "Epoch 2, Step 15/1750 (0.86%), Loss: 0.0365\n",
      "Epoch 2, Step 20/1750 (1.14%), Loss: 0.0600\n",
      "Epoch 2, Step 25/1750 (1.43%), Loss: 0.0354\n",
      "Epoch 2, Step 30/1750 (1.71%), Loss: 0.0738\n",
      "Epoch 2, Step 35/1750 (2.00%), Loss: 0.0544\n",
      "Epoch 2, Step 40/1750 (2.29%), Loss: 0.0136\n",
      "Epoch 2, Step 45/1750 (2.57%), Loss: 0.1887\n",
      "Epoch 2, Step 50/1750 (2.86%), Loss: 0.0330\n",
      "Epoch 2, Step 55/1750 (3.14%), Loss: 0.2133\n",
      "Epoch 2, Step 60/1750 (3.43%), Loss: 0.0252\n",
      "Epoch 2, Step 65/1750 (3.71%), Loss: 0.0315\n",
      "Epoch 2, Step 70/1750 (4.00%), Loss: 0.1059\n",
      "Epoch 2, Step 75/1750 (4.29%), Loss: 0.2104\n",
      "Epoch 2, Step 80/1750 (4.57%), Loss: 0.0281\n",
      "Epoch 2, Step 85/1750 (4.86%), Loss: 0.1567\n",
      "Epoch 2, Step 90/1750 (5.14%), Loss: 0.1706\n",
      "Epoch 2, Step 95/1750 (5.43%), Loss: 0.0355\n",
      "Epoch 2, Step 100/1750 (5.71%), Loss: 0.2852\n",
      "Epoch 2, Step 105/1750 (6.00%), Loss: 0.0345\n",
      "Epoch 2, Step 110/1750 (6.29%), Loss: 0.7785\n",
      "Epoch 2, Step 115/1750 (6.57%), Loss: 0.4343\n",
      "Epoch 2, Step 120/1750 (6.86%), Loss: 0.4578\n",
      "Epoch 2, Step 125/1750 (7.14%), Loss: 0.1877\n",
      "Epoch 2, Step 130/1750 (7.43%), Loss: 0.1038\n",
      "Epoch 2, Step 135/1750 (7.71%), Loss: 0.2307\n",
      "Epoch 2, Step 140/1750 (8.00%), Loss: 0.0439\n",
      "Epoch 2, Step 145/1750 (8.29%), Loss: 0.0644\n",
      "Epoch 2, Step 150/1750 (8.57%), Loss: 0.0262\n",
      "Epoch 2, Step 155/1750 (8.86%), Loss: 0.0177\n",
      "Epoch 2, Step 160/1750 (9.14%), Loss: 0.0465\n",
      "Epoch 2, Step 165/1750 (9.43%), Loss: 0.0576\n",
      "Epoch 2, Step 170/1750 (9.71%), Loss: 0.0038\n",
      "Epoch 2, Step 175/1750 (10.00%), Loss: 0.3372\n",
      "Epoch 2, Step 180/1750 (10.29%), Loss: 0.3758\n",
      "Epoch 2, Step 185/1750 (10.57%), Loss: 0.0935\n",
      "Epoch 2, Step 190/1750 (10.86%), Loss: 0.0578\n",
      "Epoch 2, Step 195/1750 (11.14%), Loss: 0.0499\n",
      "Epoch 2, Step 200/1750 (11.43%), Loss: 0.2863\n",
      "Epoch 2, Step 205/1750 (11.71%), Loss: 0.1747\n",
      "Epoch 2, Step 210/1750 (12.00%), Loss: 0.1927\n",
      "Epoch 2, Step 215/1750 (12.29%), Loss: 0.0212\n",
      "Epoch 2, Step 220/1750 (12.57%), Loss: 0.0764\n",
      "Epoch 2, Step 225/1750 (12.86%), Loss: 0.2009\n",
      "Epoch 2, Step 230/1750 (13.14%), Loss: 0.0289\n",
      "Epoch 2, Step 235/1750 (13.43%), Loss: 0.1149\n",
      "Epoch 2, Step 240/1750 (13.71%), Loss: 0.0698\n",
      "Epoch 2, Step 245/1750 (14.00%), Loss: 0.0062\n",
      "Epoch 2, Step 250/1750 (14.29%), Loss: 0.0189\n",
      "Epoch 2, Step 255/1750 (14.57%), Loss: 0.0822\n",
      "Epoch 2, Step 260/1750 (14.86%), Loss: 0.0489\n",
      "Epoch 2, Step 265/1750 (15.14%), Loss: 0.0314\n",
      "Epoch 2, Step 270/1750 (15.43%), Loss: 0.0406\n",
      "Epoch 2, Step 275/1750 (15.71%), Loss: 0.2149\n",
      "Epoch 2, Step 280/1750 (16.00%), Loss: 0.0078\n",
      "Epoch 2, Step 285/1750 (16.29%), Loss: 0.0713\n",
      "Epoch 2, Step 290/1750 (16.57%), Loss: 0.0481\n",
      "Epoch 2, Step 295/1750 (16.86%), Loss: 0.0922\n",
      "Epoch 2, Step 300/1750 (17.14%), Loss: 0.1310\n",
      "Epoch 2, Step 305/1750 (17.43%), Loss: 0.2804\n",
      "Epoch 2, Step 310/1750 (17.71%), Loss: 0.0339\n",
      "Epoch 2, Step 315/1750 (18.00%), Loss: 0.1216\n",
      "Epoch 2, Step 320/1750 (18.29%), Loss: 0.0730\n",
      "Epoch 2, Step 325/1750 (18.57%), Loss: 0.0218\n",
      "Epoch 2, Step 330/1750 (18.86%), Loss: 0.1750\n",
      "Epoch 2, Step 335/1750 (19.14%), Loss: 0.0921\n",
      "Epoch 2, Step 340/1750 (19.43%), Loss: 0.0587\n",
      "Epoch 2, Step 345/1750 (19.71%), Loss: 0.1332\n",
      "Epoch 2, Step 350/1750 (20.00%), Loss: 0.0547\n",
      "Epoch 2, Step 355/1750 (20.29%), Loss: 0.1039\n",
      "Epoch 2, Step 360/1750 (20.57%), Loss: 0.0463\n",
      "Epoch 2, Step 365/1750 (20.86%), Loss: 0.1336\n",
      "Epoch 2, Step 370/1750 (21.14%), Loss: 0.4141\n",
      "Epoch 2, Step 375/1750 (21.43%), Loss: 0.0328\n",
      "Epoch 2, Step 380/1750 (21.71%), Loss: 0.4969\n",
      "Epoch 2, Step 385/1750 (22.00%), Loss: 0.2156\n",
      "Epoch 2, Step 390/1750 (22.29%), Loss: 0.1793\n",
      "Epoch 2, Step 395/1750 (22.57%), Loss: 0.3386\n",
      "Epoch 2, Step 400/1750 (22.86%), Loss: 0.0082\n",
      "Epoch 2, Step 405/1750 (23.14%), Loss: 0.0246\n",
      "Epoch 2, Step 410/1750 (23.43%), Loss: 0.1038\n",
      "Epoch 2, Step 415/1750 (23.71%), Loss: 0.2869\n",
      "Epoch 2, Step 420/1750 (24.00%), Loss: 0.0232\n",
      "Epoch 2, Step 425/1750 (24.29%), Loss: 0.1013\n",
      "Epoch 2, Step 430/1750 (24.57%), Loss: 0.0197\n",
      "Epoch 2, Step 435/1750 (24.86%), Loss: 0.2221\n",
      "Epoch 2, Step 440/1750 (25.14%), Loss: 0.0712\n",
      "Epoch 2, Step 445/1750 (25.43%), Loss: 0.2485\n",
      "Epoch 2, Step 450/1750 (25.71%), Loss: 0.0570\n",
      "Epoch 2, Step 455/1750 (26.00%), Loss: 0.2152\n",
      "Epoch 2, Step 460/1750 (26.29%), Loss: 0.0177\n",
      "Epoch 2, Step 465/1750 (26.57%), Loss: 0.0786\n",
      "Epoch 2, Step 470/1750 (26.86%), Loss: 0.2866\n",
      "Epoch 2, Step 475/1750 (27.14%), Loss: 0.0225\n",
      "Epoch 2, Step 480/1750 (27.43%), Loss: 0.0340\n",
      "Epoch 2, Step 485/1750 (27.71%), Loss: 0.1301\n",
      "Epoch 2, Step 490/1750 (28.00%), Loss: 0.0192\n",
      "Epoch 2, Step 495/1750 (28.29%), Loss: 0.7572\n",
      "Epoch 2, Step 500/1750 (28.57%), Loss: 0.2340\n",
      "Epoch 2, Step 505/1750 (28.86%), Loss: 0.0464\n",
      "Epoch 2, Step 510/1750 (29.14%), Loss: 0.0959\n",
      "Epoch 2, Step 515/1750 (29.43%), Loss: 0.2207\n",
      "Epoch 2, Step 520/1750 (29.71%), Loss: 0.3190\n",
      "Epoch 2, Step 525/1750 (30.00%), Loss: 0.4877\n",
      "Epoch 2, Step 530/1750 (30.29%), Loss: 0.0397\n",
      "Epoch 2, Step 535/1750 (30.57%), Loss: 0.1479\n",
      "Epoch 2, Step 540/1750 (30.86%), Loss: 0.0438\n",
      "Epoch 2, Step 545/1750 (31.14%), Loss: 0.1268\n",
      "Epoch 2, Step 550/1750 (31.43%), Loss: 0.3363\n",
      "Epoch 2, Step 555/1750 (31.71%), Loss: 0.0821\n",
      "Epoch 2, Step 560/1750 (32.00%), Loss: 0.2331\n",
      "Epoch 2, Step 565/1750 (32.29%), Loss: 0.0187\n",
      "Epoch 2, Step 570/1750 (32.57%), Loss: 0.0534\n",
      "Epoch 2, Step 575/1750 (32.86%), Loss: 0.2023\n",
      "Epoch 2, Step 580/1750 (33.14%), Loss: 0.0469\n",
      "Epoch 2, Step 585/1750 (33.43%), Loss: 0.1601\n",
      "Epoch 2, Step 590/1750 (33.71%), Loss: 0.0197\n",
      "Epoch 2, Step 595/1750 (34.00%), Loss: 0.1156\n",
      "Epoch 2, Step 600/1750 (34.29%), Loss: 0.1205\n",
      "Epoch 2, Step 605/1750 (34.57%), Loss: 0.0534\n",
      "Epoch 2, Step 610/1750 (34.86%), Loss: 0.1642\n",
      "Epoch 2, Step 615/1750 (35.14%), Loss: 0.0811\n",
      "Epoch 2, Step 620/1750 (35.43%), Loss: 0.0441\n",
      "Epoch 2, Step 625/1750 (35.71%), Loss: 0.0104\n",
      "Epoch 2, Step 630/1750 (36.00%), Loss: 0.0267\n",
      "Epoch 2, Step 635/1750 (36.29%), Loss: 0.1121\n",
      "Epoch 2, Step 640/1750 (36.57%), Loss: 0.1966\n",
      "Epoch 2, Step 645/1750 (36.86%), Loss: 0.0083\n",
      "Epoch 2, Step 650/1750 (37.14%), Loss: 0.0580\n",
      "Epoch 2, Step 655/1750 (37.43%), Loss: 0.1446\n",
      "Epoch 2, Step 660/1750 (37.71%), Loss: 0.1456\n",
      "Epoch 2, Step 665/1750 (38.00%), Loss: 0.1407\n",
      "Epoch 2, Step 670/1750 (38.29%), Loss: 0.0397\n",
      "Epoch 2, Step 675/1750 (38.57%), Loss: 0.1918\n",
      "Epoch 2, Step 680/1750 (38.86%), Loss: 0.1475\n",
      "Epoch 2, Step 685/1750 (39.14%), Loss: 0.0699\n",
      "Epoch 2, Step 690/1750 (39.43%), Loss: 0.1216\n",
      "Epoch 2, Step 695/1750 (39.71%), Loss: 0.0841\n",
      "Epoch 2, Step 700/1750 (40.00%), Loss: 0.2786\n",
      "Epoch 2, Step 705/1750 (40.29%), Loss: 0.1316\n",
      "Epoch 2, Step 710/1750 (40.57%), Loss: 0.0471\n",
      "Epoch 2, Step 715/1750 (40.86%), Loss: 0.0434\n",
      "Epoch 2, Step 720/1750 (41.14%), Loss: 0.0481\n",
      "Epoch 2, Step 725/1750 (41.43%), Loss: 0.2116\n",
      "Epoch 2, Step 730/1750 (41.71%), Loss: 0.2094\n",
      "Epoch 2, Step 735/1750 (42.00%), Loss: 0.2813\n",
      "Epoch 2, Step 740/1750 (42.29%), Loss: 0.2636\n",
      "Epoch 2, Step 745/1750 (42.57%), Loss: 0.0325\n",
      "Epoch 2, Step 750/1750 (42.86%), Loss: 0.0276\n",
      "Epoch 2, Step 755/1750 (43.14%), Loss: 0.0742\n",
      "Epoch 2, Step 760/1750 (43.43%), Loss: 0.3381\n",
      "Epoch 2, Step 765/1750 (43.71%), Loss: 0.0653\n",
      "Epoch 2, Step 770/1750 (44.00%), Loss: 0.0398\n",
      "Epoch 2, Step 775/1750 (44.29%), Loss: 0.0216\n",
      "Epoch 2, Step 780/1750 (44.57%), Loss: 0.2758\n",
      "Epoch 2, Step 785/1750 (44.86%), Loss: 0.3657\n",
      "Epoch 2, Step 790/1750 (45.14%), Loss: 0.0604\n",
      "Epoch 2, Step 795/1750 (45.43%), Loss: 0.0123\n",
      "Epoch 2, Step 800/1750 (45.71%), Loss: 0.0085\n",
      "Epoch 2, Step 805/1750 (46.00%), Loss: 0.0783\n",
      "Epoch 2, Step 810/1750 (46.29%), Loss: 0.0336\n",
      "Epoch 2, Step 815/1750 (46.57%), Loss: 0.0418\n",
      "Epoch 2, Step 820/1750 (46.86%), Loss: 0.1858\n",
      "Epoch 2, Step 825/1750 (47.14%), Loss: 0.2819\n",
      "Epoch 2, Step 830/1750 (47.43%), Loss: 0.0994\n",
      "Epoch 2, Step 835/1750 (47.71%), Loss: 0.2984\n",
      "Epoch 2, Step 840/1750 (48.00%), Loss: 0.0418\n",
      "Epoch 2, Step 845/1750 (48.29%), Loss: 0.0907\n",
      "Epoch 2, Step 850/1750 (48.57%), Loss: 0.1335\n",
      "Epoch 2, Step 855/1750 (48.86%), Loss: 0.0447\n",
      "Epoch 2, Step 860/1750 (49.14%), Loss: 0.1329\n",
      "Epoch 2, Step 865/1750 (49.43%), Loss: 0.1621\n",
      "Epoch 2, Step 870/1750 (49.71%), Loss: 0.0088\n",
      "Epoch 2, Step 875/1750 (50.00%), Loss: 0.1532\n",
      "Epoch 2, Step 880/1750 (50.29%), Loss: 0.3827\n",
      "Epoch 2, Step 885/1750 (50.57%), Loss: 0.0616\n",
      "Epoch 2, Step 890/1750 (50.86%), Loss: 0.2236\n",
      "Epoch 2, Step 895/1750 (51.14%), Loss: 0.3782\n",
      "Epoch 2, Step 900/1750 (51.43%), Loss: 0.4330\n",
      "Epoch 2, Step 905/1750 (51.71%), Loss: 0.1138\n",
      "Epoch 2, Step 910/1750 (52.00%), Loss: 0.0250\n",
      "Epoch 2, Step 915/1750 (52.29%), Loss: 0.0186\n",
      "Epoch 2, Step 920/1750 (52.57%), Loss: 0.0434\n",
      "Epoch 2, Step 925/1750 (52.86%), Loss: 0.3740\n",
      "Epoch 2, Step 930/1750 (53.14%), Loss: 0.1383\n",
      "Epoch 2, Step 935/1750 (53.43%), Loss: 0.1682\n",
      "Epoch 2, Step 940/1750 (53.71%), Loss: 0.1061\n",
      "Epoch 2, Step 945/1750 (54.00%), Loss: 0.1692\n",
      "Epoch 2, Step 950/1750 (54.29%), Loss: 0.1656\n",
      "Epoch 2, Step 955/1750 (54.57%), Loss: 0.1172\n",
      "Epoch 2, Step 960/1750 (54.86%), Loss: 0.0421\n",
      "Epoch 2, Step 965/1750 (55.14%), Loss: 0.0398\n",
      "Epoch 2, Step 970/1750 (55.43%), Loss: 0.0809\n",
      "Epoch 2, Step 975/1750 (55.71%), Loss: 0.1246\n",
      "Epoch 2, Step 980/1750 (56.00%), Loss: 0.4784\n",
      "Epoch 2, Step 985/1750 (56.29%), Loss: 0.1100\n",
      "Epoch 2, Step 990/1750 (56.57%), Loss: 0.0055\n",
      "Epoch 2, Step 995/1750 (56.86%), Loss: 0.0624\n",
      "Epoch 2, Step 1000/1750 (57.14%), Loss: 0.0840\n",
      "Epoch 2, Step 1005/1750 (57.43%), Loss: 0.0280\n",
      "Epoch 2, Step 1010/1750 (57.71%), Loss: 0.0436\n",
      "Epoch 2, Step 1015/1750 (58.00%), Loss: 0.0726\n",
      "Epoch 2, Step 1020/1750 (58.29%), Loss: 0.1430\n",
      "Epoch 2, Step 1025/1750 (58.57%), Loss: 0.0132\n",
      "Epoch 2, Step 1030/1750 (58.86%), Loss: 0.0650\n",
      "Epoch 2, Step 1035/1750 (59.14%), Loss: 0.0325\n",
      "Epoch 2, Step 1040/1750 (59.43%), Loss: 0.1341\n",
      "Epoch 2, Step 1045/1750 (59.71%), Loss: 0.0087\n",
      "Epoch 2, Step 1050/1750 (60.00%), Loss: 0.1578\n",
      "Epoch 2, Step 1055/1750 (60.29%), Loss: 0.5540\n",
      "Epoch 2, Step 1060/1750 (60.57%), Loss: 0.3110\n",
      "Epoch 2, Step 1065/1750 (60.86%), Loss: 0.0064\n",
      "Epoch 2, Step 1070/1750 (61.14%), Loss: 0.0544\n",
      "Epoch 2, Step 1075/1750 (61.43%), Loss: 0.0482\n",
      "Epoch 2, Step 1080/1750 (61.71%), Loss: 0.4059\n",
      "Epoch 2, Step 1085/1750 (62.00%), Loss: 0.0210\n",
      "Epoch 2, Step 1090/1750 (62.29%), Loss: 0.0487\n",
      "Epoch 2, Step 1095/1750 (62.57%), Loss: 0.1468\n",
      "Epoch 2, Step 1100/1750 (62.86%), Loss: 0.0939\n",
      "Epoch 2, Step 1105/1750 (63.14%), Loss: 0.4037\n",
      "Epoch 2, Step 1110/1750 (63.43%), Loss: 0.4341\n",
      "Epoch 2, Step 1115/1750 (63.71%), Loss: 0.0128\n",
      "Epoch 2, Step 1120/1750 (64.00%), Loss: 0.2277\n",
      "Epoch 2, Step 1125/1750 (64.29%), Loss: 0.0132\n",
      "Epoch 2, Step 1130/1750 (64.57%), Loss: 0.2254\n",
      "Epoch 2, Step 1135/1750 (64.86%), Loss: 0.0165\n",
      "Epoch 2, Step 1140/1750 (65.14%), Loss: 0.0254\n",
      "Epoch 2, Step 1145/1750 (65.43%), Loss: 0.3697\n",
      "Epoch 2, Step 1150/1750 (65.71%), Loss: 0.0647\n",
      "Epoch 2, Step 1155/1750 (66.00%), Loss: 0.1441\n",
      "Epoch 2, Step 1160/1750 (66.29%), Loss: 0.0705\n",
      "Epoch 2, Step 1165/1750 (66.57%), Loss: 0.2217\n",
      "Epoch 2, Step 1170/1750 (66.86%), Loss: 0.0898\n",
      "Epoch 2, Step 1175/1750 (67.14%), Loss: 0.0678\n",
      "Epoch 2, Step 1180/1750 (67.43%), Loss: 0.1528\n",
      "Epoch 2, Step 1185/1750 (67.71%), Loss: 0.0162\n",
      "Epoch 2, Step 1190/1750 (68.00%), Loss: 0.0260\n",
      "Epoch 2, Step 1195/1750 (68.29%), Loss: 0.0290\n",
      "Epoch 2, Step 1200/1750 (68.57%), Loss: 0.0341\n",
      "Epoch 2, Step 1205/1750 (68.86%), Loss: 0.1419\n",
      "Epoch 2, Step 1210/1750 (69.14%), Loss: 0.0295\n",
      "Epoch 2, Step 1215/1750 (69.43%), Loss: 0.1234\n",
      "Epoch 2, Step 1220/1750 (69.71%), Loss: 0.0380\n",
      "Epoch 2, Step 1225/1750 (70.00%), Loss: 0.0974\n",
      "Epoch 2, Step 1230/1750 (70.29%), Loss: 0.0451\n",
      "Epoch 2, Step 1235/1750 (70.57%), Loss: 0.2621\n",
      "Epoch 2, Step 1240/1750 (70.86%), Loss: 0.0154\n",
      "Epoch 2, Step 1245/1750 (71.14%), Loss: 0.0856\n",
      "Epoch 2, Step 1250/1750 (71.43%), Loss: 0.2625\n",
      "Epoch 2, Step 1255/1750 (71.71%), Loss: 0.0996\n",
      "Epoch 2, Step 1260/1750 (72.00%), Loss: 0.0425\n",
      "Epoch 2, Step 1265/1750 (72.29%), Loss: 0.0101\n",
      "Epoch 2, Step 1270/1750 (72.57%), Loss: 0.0753\n",
      "Epoch 2, Step 1275/1750 (72.86%), Loss: 0.0581\n",
      "Epoch 2, Step 1280/1750 (73.14%), Loss: 0.3243\n",
      "Epoch 2, Step 1285/1750 (73.43%), Loss: 0.0533\n",
      "Epoch 2, Step 1290/1750 (73.71%), Loss: 0.0251\n",
      "Epoch 2, Step 1295/1750 (74.00%), Loss: 0.0394\n",
      "Epoch 2, Step 1300/1750 (74.29%), Loss: 0.0227\n",
      "Epoch 2, Step 1305/1750 (74.57%), Loss: 0.0222\n",
      "Epoch 2, Step 1310/1750 (74.86%), Loss: 0.2184\n",
      "Epoch 2, Step 1315/1750 (75.14%), Loss: 0.0333\n",
      "Epoch 2, Step 1320/1750 (75.43%), Loss: 0.0690\n",
      "Epoch 2, Step 1325/1750 (75.71%), Loss: 0.0178\n",
      "Epoch 2, Step 1330/1750 (76.00%), Loss: 0.3661\n",
      "Epoch 2, Step 1335/1750 (76.29%), Loss: 0.2178\n",
      "Epoch 2, Step 1340/1750 (76.57%), Loss: 0.1227\n",
      "Epoch 2, Step 1345/1750 (76.86%), Loss: 0.0745\n",
      "Epoch 2, Step 1350/1750 (77.14%), Loss: 0.1783\n",
      "Epoch 2, Step 1355/1750 (77.43%), Loss: 0.1471\n",
      "Epoch 2, Step 1360/1750 (77.71%), Loss: 0.1274\n",
      "Epoch 2, Step 1365/1750 (78.00%), Loss: 0.1722\n",
      "Epoch 2, Step 1370/1750 (78.29%), Loss: 0.1680\n",
      "Epoch 2, Step 1375/1750 (78.57%), Loss: 0.0870\n",
      "Epoch 2, Step 1380/1750 (78.86%), Loss: 0.8250\n",
      "Epoch 2, Step 1385/1750 (79.14%), Loss: 0.0192\n",
      "Epoch 2, Step 1390/1750 (79.43%), Loss: 0.3852\n",
      "Epoch 2, Step 1395/1750 (79.71%), Loss: 0.0888\n",
      "Epoch 2, Step 1400/1750 (80.00%), Loss: 0.4000\n",
      "Epoch 2, Step 1405/1750 (80.29%), Loss: 0.0907\n",
      "Epoch 2, Step 1410/1750 (80.57%), Loss: 0.0286\n",
      "Epoch 2, Step 1415/1750 (80.86%), Loss: 0.3984\n",
      "Epoch 2, Step 1420/1750 (81.14%), Loss: 0.3050\n",
      "Epoch 2, Step 1425/1750 (81.43%), Loss: 0.0213\n",
      "Epoch 2, Step 1430/1750 (81.71%), Loss: 0.0197\n",
      "Epoch 2, Step 1435/1750 (82.00%), Loss: 0.0511\n",
      "Epoch 2, Step 1440/1750 (82.29%), Loss: 0.0525\n",
      "Epoch 2, Step 1445/1750 (82.57%), Loss: 0.2134\n",
      "Epoch 2, Step 1450/1750 (82.86%), Loss: 0.1417\n",
      "Epoch 2, Step 1455/1750 (83.14%), Loss: 0.1064\n",
      "Epoch 2, Step 1460/1750 (83.43%), Loss: 0.0372\n",
      "Epoch 2, Step 1465/1750 (83.71%), Loss: 0.0380\n",
      "Epoch 2, Step 1470/1750 (84.00%), Loss: 0.0814\n",
      "Epoch 2, Step 1475/1750 (84.29%), Loss: 0.1154\n",
      "Epoch 2, Step 1480/1750 (84.57%), Loss: 0.0946\n",
      "Epoch 2, Step 1485/1750 (84.86%), Loss: 0.1673\n",
      "Epoch 2, Step 1490/1750 (85.14%), Loss: 0.2766\n",
      "Epoch 2, Step 1495/1750 (85.43%), Loss: 0.0169\n",
      "Epoch 2, Step 1500/1750 (85.71%), Loss: 0.0187\n",
      "Epoch 2, Step 1505/1750 (86.00%), Loss: 0.4877\n",
      "Epoch 2, Step 1510/1750 (86.29%), Loss: 0.0079\n",
      "Epoch 2, Step 1515/1750 (86.57%), Loss: 0.4186\n",
      "Epoch 2, Step 1520/1750 (86.86%), Loss: 0.2164\n",
      "Epoch 2, Step 1525/1750 (87.14%), Loss: 0.2174\n",
      "Epoch 2, Step 1530/1750 (87.43%), Loss: 0.0292\n",
      "Epoch 2, Step 1535/1750 (87.71%), Loss: 0.0125\n",
      "Epoch 2, Step 1540/1750 (88.00%), Loss: 0.1127\n",
      "Epoch 2, Step 1545/1750 (88.29%), Loss: 0.0465\n",
      "Epoch 2, Step 1550/1750 (88.57%), Loss: 0.1137\n",
      "Epoch 2, Step 1555/1750 (88.86%), Loss: 0.1418\n",
      "Epoch 2, Step 1560/1750 (89.14%), Loss: 0.1865\n",
      "Epoch 2, Step 1565/1750 (89.43%), Loss: 0.1853\n",
      "Epoch 2, Step 1570/1750 (89.71%), Loss: 0.0501\n",
      "Epoch 2, Step 1575/1750 (90.00%), Loss: 0.0704\n",
      "Epoch 2, Step 1580/1750 (90.29%), Loss: 0.0404\n",
      "Epoch 2, Step 1585/1750 (90.57%), Loss: 0.0134\n",
      "Epoch 2, Step 1590/1750 (90.86%), Loss: 0.2714\n",
      "Epoch 2, Step 1595/1750 (91.14%), Loss: 0.0091\n",
      "Epoch 2, Step 1600/1750 (91.43%), Loss: 0.0541\n",
      "Epoch 2, Step 1605/1750 (91.71%), Loss: 0.2605\n",
      "Epoch 2, Step 1610/1750 (92.00%), Loss: 0.1000\n",
      "Epoch 2, Step 1615/1750 (92.29%), Loss: 0.1258\n",
      "Epoch 2, Step 1620/1750 (92.57%), Loss: 0.0741\n",
      "Epoch 2, Step 1625/1750 (92.86%), Loss: 0.0218\n",
      "Epoch 2, Step 1630/1750 (93.14%), Loss: 0.0170\n",
      "Epoch 2, Step 1635/1750 (93.43%), Loss: 0.0724\n",
      "Epoch 2, Step 1640/1750 (93.71%), Loss: 0.0132\n",
      "Epoch 2, Step 1645/1750 (94.00%), Loss: 0.0708\n",
      "Epoch 2, Step 1650/1750 (94.29%), Loss: 0.1513\n",
      "Epoch 2, Step 1655/1750 (94.57%), Loss: 0.0756\n",
      "Epoch 2, Step 1660/1750 (94.86%), Loss: 0.1614\n",
      "Epoch 2, Step 1665/1750 (95.14%), Loss: 0.0822\n",
      "Epoch 2, Step 1670/1750 (95.43%), Loss: 0.0215\n",
      "Epoch 2, Step 1675/1750 (95.71%), Loss: 0.0400\n",
      "Epoch 2, Step 1680/1750 (96.00%), Loss: 0.1985\n",
      "Epoch 2, Step 1685/1750 (96.29%), Loss: 0.1364\n",
      "Epoch 2, Step 1690/1750 (96.57%), Loss: 0.0267\n",
      "Epoch 2, Step 1695/1750 (96.86%), Loss: 0.0597\n",
      "Epoch 2, Step 1700/1750 (97.14%), Loss: 0.0305\n",
      "Epoch 2, Step 1705/1750 (97.43%), Loss: 0.0270\n",
      "Epoch 2, Step 1710/1750 (97.71%), Loss: 0.2888\n",
      "Epoch 2, Step 1715/1750 (98.00%), Loss: 0.0927\n",
      "Epoch 2, Step 1720/1750 (98.29%), Loss: 0.1676\n",
      "Epoch 2, Step 1725/1750 (98.57%), Loss: 0.0365\n",
      "Epoch 2, Step 1730/1750 (98.86%), Loss: 0.1162\n",
      "Epoch 2, Step 1735/1750 (99.14%), Loss: 0.1115\n",
      "Epoch 2, Step 1740/1750 (99.43%), Loss: 0.1484\n",
      "Epoch 2, Step 1745/1750 (99.71%), Loss: 0.0734\n",
      "Epoch 2, Step 1750/1750 (100.00%), Loss: 0.0175\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.20\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.43\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.06\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.38\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.39\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.04\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.37\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.34\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.28\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.23\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.01\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.04\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.01\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.20\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 0.57\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.10\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.05\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.40\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.06\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.22\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.45\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.24\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.05\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.20\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.01\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.14\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 0.45\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.20\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.12\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.03\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.01\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.03\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.23\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.01\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.37\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 0.73\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.06\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.16\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.06\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.22\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.57\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.09\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.27\n",
      "End of Epoch 2/10 - Average Train Loss: 0.13, Average Validation Loss: 0.21\n",
      "Starting Epoch 3/10\n",
      "Epoch 3, Step 5/1750 (0.29%), Loss: 0.0449\n",
      "Epoch 3, Step 10/1750 (0.57%), Loss: 0.0198\n",
      "Epoch 3, Step 15/1750 (0.86%), Loss: 0.0509\n",
      "Epoch 3, Step 20/1750 (1.14%), Loss: 0.0152\n",
      "Epoch 3, Step 25/1750 (1.43%), Loss: 0.0286\n",
      "Epoch 3, Step 30/1750 (1.71%), Loss: 0.1015\n",
      "Epoch 3, Step 35/1750 (2.00%), Loss: 0.0370\n",
      "Epoch 3, Step 40/1750 (2.29%), Loss: 0.0227\n",
      "Epoch 3, Step 45/1750 (2.57%), Loss: 0.0906\n",
      "Epoch 3, Step 50/1750 (2.86%), Loss: 0.0034\n",
      "Epoch 3, Step 55/1750 (3.14%), Loss: 0.0183\n",
      "Epoch 3, Step 60/1750 (3.43%), Loss: 0.0493\n",
      "Epoch 3, Step 65/1750 (3.71%), Loss: 0.0065\n",
      "Epoch 3, Step 70/1750 (4.00%), Loss: 0.0054\n",
      "Epoch 3, Step 75/1750 (4.29%), Loss: 0.0190\n",
      "Epoch 3, Step 80/1750 (4.57%), Loss: 0.0132\n",
      "Epoch 3, Step 85/1750 (4.86%), Loss: 0.0066\n",
      "Epoch 3, Step 90/1750 (5.14%), Loss: 0.0036\n",
      "Epoch 3, Step 95/1750 (5.43%), Loss: 0.0115\n",
      "Epoch 3, Step 100/1750 (5.71%), Loss: 0.0434\n",
      "Epoch 3, Step 105/1750 (6.00%), Loss: 0.0080\n",
      "Epoch 3, Step 110/1750 (6.29%), Loss: 0.0871\n",
      "Epoch 3, Step 115/1750 (6.57%), Loss: 0.0033\n",
      "Epoch 3, Step 120/1750 (6.86%), Loss: 0.0029\n",
      "Epoch 3, Step 125/1750 (7.14%), Loss: 0.0023\n",
      "Epoch 3, Step 130/1750 (7.43%), Loss: 0.0019\n",
      "Epoch 3, Step 135/1750 (7.71%), Loss: 0.0023\n",
      "Epoch 3, Step 140/1750 (8.00%), Loss: 0.0018\n",
      "Epoch 3, Step 145/1750 (8.29%), Loss: 0.0316\n",
      "Epoch 3, Step 150/1750 (8.57%), Loss: 0.0014\n",
      "Epoch 3, Step 155/1750 (8.86%), Loss: 0.5418\n",
      "Epoch 3, Step 160/1750 (9.14%), Loss: 0.0074\n",
      "Epoch 3, Step 165/1750 (9.43%), Loss: 0.0810\n",
      "Epoch 3, Step 170/1750 (9.71%), Loss: 0.0051\n",
      "Epoch 3, Step 175/1750 (10.00%), Loss: 0.0283\n",
      "Epoch 3, Step 180/1750 (10.29%), Loss: 0.0136\n",
      "Epoch 3, Step 185/1750 (10.57%), Loss: 0.1847\n",
      "Epoch 3, Step 190/1750 (10.86%), Loss: 0.3495\n",
      "Epoch 3, Step 195/1750 (11.14%), Loss: 0.0791\n",
      "Epoch 3, Step 200/1750 (11.43%), Loss: 0.0069\n",
      "Epoch 3, Step 205/1750 (11.71%), Loss: 0.0053\n",
      "Epoch 3, Step 210/1750 (12.00%), Loss: 0.0144\n",
      "Epoch 3, Step 215/1750 (12.29%), Loss: 0.0027\n",
      "Epoch 3, Step 220/1750 (12.57%), Loss: 0.0048\n",
      "Epoch 3, Step 225/1750 (12.86%), Loss: 0.0160\n",
      "Epoch 3, Step 230/1750 (13.14%), Loss: 0.0049\n",
      "Epoch 3, Step 235/1750 (13.43%), Loss: 0.0100\n",
      "Epoch 3, Step 240/1750 (13.71%), Loss: 0.0229\n",
      "Epoch 3, Step 245/1750 (14.00%), Loss: 0.0036\n",
      "Epoch 3, Step 250/1750 (14.29%), Loss: 0.0062\n",
      "Epoch 3, Step 255/1750 (14.57%), Loss: 0.2145\n",
      "Epoch 3, Step 260/1750 (14.86%), Loss: 0.0393\n",
      "Epoch 3, Step 265/1750 (15.14%), Loss: 0.0134\n",
      "Epoch 3, Step 270/1750 (15.43%), Loss: 0.0396\n",
      "Epoch 3, Step 275/1750 (15.71%), Loss: 0.0094\n",
      "Epoch 3, Step 280/1750 (16.00%), Loss: 0.0663\n",
      "Epoch 3, Step 285/1750 (16.29%), Loss: 0.0451\n",
      "Epoch 3, Step 290/1750 (16.57%), Loss: 0.0081\n",
      "Epoch 3, Step 295/1750 (16.86%), Loss: 0.0365\n",
      "Epoch 3, Step 300/1750 (17.14%), Loss: 0.0354\n",
      "Epoch 3, Step 305/1750 (17.43%), Loss: 0.0056\n",
      "Epoch 3, Step 310/1750 (17.71%), Loss: 0.0127\n",
      "Epoch 3, Step 315/1750 (18.00%), Loss: 0.0134\n",
      "Epoch 3, Step 320/1750 (18.29%), Loss: 0.0081\n",
      "Epoch 3, Step 325/1750 (18.57%), Loss: 0.0189\n",
      "Epoch 3, Step 330/1750 (18.86%), Loss: 0.0070\n",
      "Epoch 3, Step 335/1750 (19.14%), Loss: 0.0167\n",
      "Epoch 3, Step 340/1750 (19.43%), Loss: 0.0037\n",
      "Epoch 3, Step 345/1750 (19.71%), Loss: 0.0412\n",
      "Epoch 3, Step 350/1750 (20.00%), Loss: 0.0187\n",
      "Epoch 3, Step 355/1750 (20.29%), Loss: 0.0281\n",
      "Epoch 3, Step 360/1750 (20.57%), Loss: 0.0908\n",
      "Epoch 3, Step 365/1750 (20.86%), Loss: 0.0187\n",
      "Epoch 3, Step 370/1750 (21.14%), Loss: 0.0159\n",
      "Epoch 3, Step 375/1750 (21.43%), Loss: 0.0139\n",
      "Epoch 3, Step 380/1750 (21.71%), Loss: 0.0066\n",
      "Epoch 3, Step 385/1750 (22.00%), Loss: 0.0113\n",
      "Epoch 3, Step 390/1750 (22.29%), Loss: 0.0324\n",
      "Epoch 3, Step 395/1750 (22.57%), Loss: 0.0053\n",
      "Epoch 3, Step 400/1750 (22.86%), Loss: 0.0132\n",
      "Epoch 3, Step 405/1750 (23.14%), Loss: 0.0451\n",
      "Epoch 3, Step 410/1750 (23.43%), Loss: 0.0282\n",
      "Epoch 3, Step 415/1750 (23.71%), Loss: 0.0059\n",
      "Epoch 3, Step 420/1750 (24.00%), Loss: 0.0043\n",
      "Epoch 3, Step 425/1750 (24.29%), Loss: 0.0024\n",
      "Epoch 3, Step 430/1750 (24.57%), Loss: 0.1917\n",
      "Epoch 3, Step 435/1750 (24.86%), Loss: 0.6452\n",
      "Epoch 3, Step 440/1750 (25.14%), Loss: 0.1643\n",
      "Epoch 3, Step 445/1750 (25.43%), Loss: 0.0114\n",
      "Epoch 3, Step 450/1750 (25.71%), Loss: 0.0046\n",
      "Epoch 3, Step 455/1750 (26.00%), Loss: 0.0666\n",
      "Epoch 3, Step 460/1750 (26.29%), Loss: 0.0362\n",
      "Epoch 3, Step 465/1750 (26.57%), Loss: 0.0112\n",
      "Epoch 3, Step 470/1750 (26.86%), Loss: 0.1043\n",
      "Epoch 3, Step 475/1750 (27.14%), Loss: 0.0096\n",
      "Epoch 3, Step 480/1750 (27.43%), Loss: 0.0108\n",
      "Epoch 3, Step 485/1750 (27.71%), Loss: 0.0125\n",
      "Epoch 3, Step 490/1750 (28.00%), Loss: 0.0044\n",
      "Epoch 3, Step 495/1750 (28.29%), Loss: 0.0439\n",
      "Epoch 3, Step 500/1750 (28.57%), Loss: 0.0054\n",
      "Epoch 3, Step 505/1750 (28.86%), Loss: 0.0082\n",
      "Epoch 3, Step 510/1750 (29.14%), Loss: 0.0035\n",
      "Epoch 3, Step 515/1750 (29.43%), Loss: 0.2484\n",
      "Epoch 3, Step 520/1750 (29.71%), Loss: 0.0292\n",
      "Epoch 3, Step 525/1750 (30.00%), Loss: 0.1767\n",
      "Epoch 3, Step 530/1750 (30.29%), Loss: 0.1033\n",
      "Epoch 3, Step 535/1750 (30.57%), Loss: 0.0184\n",
      "Epoch 3, Step 540/1750 (30.86%), Loss: 0.0184\n",
      "Epoch 3, Step 545/1750 (31.14%), Loss: 0.0079\n",
      "Epoch 3, Step 550/1750 (31.43%), Loss: 0.0029\n",
      "Epoch 3, Step 555/1750 (31.71%), Loss: 0.0035\n",
      "Epoch 3, Step 560/1750 (32.00%), Loss: 0.0102\n",
      "Epoch 3, Step 565/1750 (32.29%), Loss: 0.2200\n",
      "Epoch 3, Step 570/1750 (32.57%), Loss: 0.0888\n",
      "Epoch 3, Step 575/1750 (32.86%), Loss: 0.0093\n",
      "Epoch 3, Step 580/1750 (33.14%), Loss: 0.0513\n",
      "Epoch 3, Step 585/1750 (33.43%), Loss: 0.0149\n",
      "Epoch 3, Step 590/1750 (33.71%), Loss: 0.0055\n",
      "Epoch 3, Step 595/1750 (34.00%), Loss: 0.0727\n",
      "Epoch 3, Step 600/1750 (34.29%), Loss: 0.0026\n",
      "Epoch 3, Step 605/1750 (34.57%), Loss: 0.0046\n",
      "Epoch 3, Step 610/1750 (34.86%), Loss: 0.0202\n",
      "Epoch 3, Step 615/1750 (35.14%), Loss: 0.0087\n",
      "Epoch 3, Step 620/1750 (35.43%), Loss: 0.0935\n",
      "Epoch 3, Step 625/1750 (35.71%), Loss: 0.1490\n",
      "Epoch 3, Step 630/1750 (36.00%), Loss: 0.0178\n",
      "Epoch 3, Step 635/1750 (36.29%), Loss: 0.0277\n",
      "Epoch 3, Step 640/1750 (36.57%), Loss: 0.0241\n",
      "Epoch 3, Step 645/1750 (36.86%), Loss: 0.0083\n",
      "Epoch 3, Step 650/1750 (37.14%), Loss: 0.0099\n",
      "Epoch 3, Step 655/1750 (37.43%), Loss: 0.0162\n",
      "Epoch 3, Step 660/1750 (37.71%), Loss: 0.0168\n",
      "Epoch 3, Step 665/1750 (38.00%), Loss: 0.0409\n",
      "Epoch 3, Step 670/1750 (38.29%), Loss: 0.2787\n",
      "Epoch 3, Step 675/1750 (38.57%), Loss: 0.0273\n",
      "Epoch 3, Step 680/1750 (38.86%), Loss: 0.0055\n",
      "Epoch 3, Step 685/1750 (39.14%), Loss: 0.2697\n",
      "Epoch 3, Step 690/1750 (39.43%), Loss: 0.0162\n",
      "Epoch 3, Step 695/1750 (39.71%), Loss: 0.3419\n",
      "Epoch 3, Step 700/1750 (40.00%), Loss: 0.0139\n",
      "Epoch 3, Step 705/1750 (40.29%), Loss: 0.1425\n",
      "Epoch 3, Step 710/1750 (40.57%), Loss: 0.0059\n",
      "Epoch 3, Step 715/1750 (40.86%), Loss: 0.1227\n",
      "Epoch 3, Step 720/1750 (41.14%), Loss: 0.0061\n",
      "Epoch 3, Step 725/1750 (41.43%), Loss: 0.0084\n",
      "Epoch 3, Step 730/1750 (41.71%), Loss: 0.0235\n",
      "Epoch 3, Step 735/1750 (42.00%), Loss: 0.0015\n",
      "Epoch 3, Step 740/1750 (42.29%), Loss: 0.0114\n",
      "Epoch 3, Step 745/1750 (42.57%), Loss: 0.0036\n",
      "Epoch 3, Step 750/1750 (42.86%), Loss: 0.0057\n",
      "Epoch 3, Step 755/1750 (43.14%), Loss: 0.0016\n",
      "Epoch 3, Step 760/1750 (43.43%), Loss: 0.0938\n",
      "Epoch 3, Step 765/1750 (43.71%), Loss: 0.3862\n",
      "Epoch 3, Step 770/1750 (44.00%), Loss: 0.0063\n",
      "Epoch 3, Step 775/1750 (44.29%), Loss: 0.0449\n",
      "Epoch 3, Step 780/1750 (44.57%), Loss: 0.0738\n",
      "Epoch 3, Step 785/1750 (44.86%), Loss: 0.0357\n",
      "Epoch 3, Step 790/1750 (45.14%), Loss: 0.0087\n",
      "Epoch 3, Step 795/1750 (45.43%), Loss: 0.0177\n",
      "Epoch 3, Step 800/1750 (45.71%), Loss: 0.0751\n",
      "Epoch 3, Step 805/1750 (46.00%), Loss: 0.0101\n",
      "Epoch 3, Step 810/1750 (46.29%), Loss: 0.2051\n",
      "Epoch 3, Step 815/1750 (46.57%), Loss: 0.0289\n",
      "Epoch 3, Step 820/1750 (46.86%), Loss: 0.0142\n",
      "Epoch 3, Step 825/1750 (47.14%), Loss: 0.0044\n",
      "Epoch 3, Step 830/1750 (47.43%), Loss: 0.0555\n",
      "Epoch 3, Step 835/1750 (47.71%), Loss: 0.0089\n",
      "Epoch 3, Step 840/1750 (48.00%), Loss: 0.0121\n",
      "Epoch 3, Step 845/1750 (48.29%), Loss: 0.0467\n",
      "Epoch 3, Step 850/1750 (48.57%), Loss: 0.0752\n",
      "Epoch 3, Step 855/1750 (48.86%), Loss: 0.0107\n",
      "Epoch 3, Step 860/1750 (49.14%), Loss: 0.0133\n",
      "Epoch 3, Step 865/1750 (49.43%), Loss: 0.0944\n",
      "Epoch 3, Step 870/1750 (49.71%), Loss: 0.0068\n",
      "Epoch 3, Step 875/1750 (50.00%), Loss: 0.0600\n",
      "Epoch 3, Step 880/1750 (50.29%), Loss: 0.1013\n",
      "Epoch 3, Step 885/1750 (50.57%), Loss: 0.0060\n",
      "Epoch 3, Step 890/1750 (50.86%), Loss: 0.0025\n",
      "Epoch 3, Step 895/1750 (51.14%), Loss: 0.0058\n",
      "Epoch 3, Step 900/1750 (51.43%), Loss: 0.0204\n",
      "Epoch 3, Step 905/1750 (51.71%), Loss: 0.0486\n",
      "Epoch 3, Step 910/1750 (52.00%), Loss: 0.0546\n",
      "Epoch 3, Step 915/1750 (52.29%), Loss: 0.0017\n",
      "Epoch 3, Step 920/1750 (52.57%), Loss: 0.3659\n",
      "Epoch 3, Step 925/1750 (52.86%), Loss: 0.0893\n",
      "Epoch 3, Step 930/1750 (53.14%), Loss: 0.0124\n",
      "Epoch 3, Step 935/1750 (53.43%), Loss: 0.2211\n",
      "Epoch 3, Step 940/1750 (53.71%), Loss: 0.3270\n",
      "Epoch 3, Step 945/1750 (54.00%), Loss: 0.0249\n",
      "Epoch 3, Step 950/1750 (54.29%), Loss: 0.0672\n",
      "Epoch 3, Step 955/1750 (54.57%), Loss: 0.0084\n",
      "Epoch 3, Step 960/1750 (54.86%), Loss: 0.0660\n",
      "Epoch 3, Step 965/1750 (55.14%), Loss: 0.0582\n",
      "Epoch 3, Step 970/1750 (55.43%), Loss: 0.1005\n",
      "Epoch 3, Step 975/1750 (55.71%), Loss: 0.0283\n",
      "Epoch 3, Step 980/1750 (56.00%), Loss: 0.0151\n",
      "Epoch 3, Step 985/1750 (56.29%), Loss: 0.0073\n",
      "Epoch 3, Step 990/1750 (56.57%), Loss: 0.0328\n",
      "Epoch 3, Step 995/1750 (56.86%), Loss: 0.1104\n",
      "Epoch 3, Step 1000/1750 (57.14%), Loss: 0.0230\n",
      "Epoch 3, Step 1005/1750 (57.43%), Loss: 0.0049\n",
      "Epoch 3, Step 1010/1750 (57.71%), Loss: 0.0052\n",
      "Epoch 3, Step 1015/1750 (58.00%), Loss: 0.0095\n",
      "Epoch 3, Step 1020/1750 (58.29%), Loss: 0.0064\n",
      "Epoch 3, Step 1025/1750 (58.57%), Loss: 0.0050\n",
      "Epoch 3, Step 1030/1750 (58.86%), Loss: 0.0016\n",
      "Epoch 3, Step 1035/1750 (59.14%), Loss: 0.0047\n",
      "Epoch 3, Step 1040/1750 (59.43%), Loss: 0.0019\n",
      "Epoch 3, Step 1045/1750 (59.71%), Loss: 0.0059\n",
      "Epoch 3, Step 1050/1750 (60.00%), Loss: 0.1477\n",
      "Epoch 3, Step 1055/1750 (60.29%), Loss: 0.0062\n",
      "Epoch 3, Step 1060/1750 (60.57%), Loss: 0.0078\n",
      "Epoch 3, Step 1065/1750 (60.86%), Loss: 0.0158\n",
      "Epoch 3, Step 1070/1750 (61.14%), Loss: 0.0347\n",
      "Epoch 3, Step 1075/1750 (61.43%), Loss: 0.0027\n",
      "Epoch 3, Step 1080/1750 (61.71%), Loss: 0.0074\n",
      "Epoch 3, Step 1085/1750 (62.00%), Loss: 0.0078\n",
      "Epoch 3, Step 1090/1750 (62.29%), Loss: 0.1584\n",
      "Epoch 3, Step 1095/1750 (62.57%), Loss: 0.0092\n",
      "Epoch 3, Step 1100/1750 (62.86%), Loss: 0.1103\n",
      "Epoch 3, Step 1105/1750 (63.14%), Loss: 0.0118\n",
      "Epoch 3, Step 1110/1750 (63.43%), Loss: 0.4207\n",
      "Epoch 3, Step 1115/1750 (63.71%), Loss: 0.0300\n",
      "Epoch 3, Step 1120/1750 (64.00%), Loss: 0.0154\n",
      "Epoch 3, Step 1125/1750 (64.29%), Loss: 0.0251\n",
      "Epoch 3, Step 1130/1750 (64.57%), Loss: 0.0045\n",
      "Epoch 3, Step 1135/1750 (64.86%), Loss: 0.0062\n",
      "Epoch 3, Step 1140/1750 (65.14%), Loss: 0.0029\n",
      "Epoch 3, Step 1145/1750 (65.43%), Loss: 0.1614\n",
      "Epoch 3, Step 1150/1750 (65.71%), Loss: 0.6789\n",
      "Epoch 3, Step 1155/1750 (66.00%), Loss: 0.0093\n",
      "Epoch 3, Step 1160/1750 (66.29%), Loss: 0.0053\n",
      "Epoch 3, Step 1165/1750 (66.57%), Loss: 0.0262\n",
      "Epoch 3, Step 1170/1750 (66.86%), Loss: 0.0638\n",
      "Epoch 3, Step 1175/1750 (67.14%), Loss: 0.0032\n",
      "Epoch 3, Step 1180/1750 (67.43%), Loss: 0.1769\n",
      "Epoch 3, Step 1185/1750 (67.71%), Loss: 0.0222\n",
      "Epoch 3, Step 1190/1750 (68.00%), Loss: 0.0082\n",
      "Epoch 3, Step 1195/1750 (68.29%), Loss: 0.0076\n",
      "Epoch 3, Step 1200/1750 (68.57%), Loss: 0.1738\n",
      "Epoch 3, Step 1205/1750 (68.86%), Loss: 0.0549\n",
      "Epoch 3, Step 1210/1750 (69.14%), Loss: 0.0147\n",
      "Epoch 3, Step 1215/1750 (69.43%), Loss: 0.0094\n",
      "Epoch 3, Step 1220/1750 (69.71%), Loss: 0.0740\n",
      "Epoch 3, Step 1225/1750 (70.00%), Loss: 0.0371\n",
      "Epoch 3, Step 1230/1750 (70.29%), Loss: 0.0273\n",
      "Epoch 3, Step 1235/1750 (70.57%), Loss: 0.0040\n",
      "Epoch 3, Step 1240/1750 (70.86%), Loss: 0.0381\n",
      "Epoch 3, Step 1245/1750 (71.14%), Loss: 0.0130\n",
      "Epoch 3, Step 1250/1750 (71.43%), Loss: 0.0130\n",
      "Epoch 3, Step 1255/1750 (71.71%), Loss: 0.0381\n",
      "Epoch 3, Step 1260/1750 (72.00%), Loss: 0.0050\n",
      "Epoch 3, Step 1265/1750 (72.29%), Loss: 0.0755\n",
      "Epoch 3, Step 1270/1750 (72.57%), Loss: 0.0050\n",
      "Epoch 3, Step 1275/1750 (72.86%), Loss: 0.0025\n",
      "Epoch 3, Step 1280/1750 (73.14%), Loss: 0.1540\n",
      "Epoch 3, Step 1285/1750 (73.43%), Loss: 0.0166\n",
      "Epoch 3, Step 1290/1750 (73.71%), Loss: 0.0277\n",
      "Epoch 3, Step 1295/1750 (74.00%), Loss: 0.4421\n",
      "Epoch 3, Step 1300/1750 (74.29%), Loss: 0.0139\n",
      "Epoch 3, Step 1305/1750 (74.57%), Loss: 0.0313\n",
      "Epoch 3, Step 1310/1750 (74.86%), Loss: 0.0037\n",
      "Epoch 3, Step 1315/1750 (75.14%), Loss: 0.2203\n",
      "Epoch 3, Step 1320/1750 (75.43%), Loss: 0.3721\n",
      "Epoch 3, Step 1325/1750 (75.71%), Loss: 0.0533\n",
      "Epoch 3, Step 1330/1750 (76.00%), Loss: 0.1135\n",
      "Epoch 3, Step 1335/1750 (76.29%), Loss: 0.0503\n",
      "Epoch 3, Step 1340/1750 (76.57%), Loss: 0.2889\n",
      "Epoch 3, Step 1345/1750 (76.86%), Loss: 0.1189\n",
      "Epoch 3, Step 1350/1750 (77.14%), Loss: 0.0078\n",
      "Epoch 3, Step 1355/1750 (77.43%), Loss: 0.0191\n",
      "Epoch 3, Step 1360/1750 (77.71%), Loss: 0.0154\n",
      "Epoch 3, Step 1365/1750 (78.00%), Loss: 0.0348\n",
      "Epoch 3, Step 1370/1750 (78.29%), Loss: 0.0438\n",
      "Epoch 3, Step 1375/1750 (78.57%), Loss: 0.0206\n",
      "Epoch 3, Step 1380/1750 (78.86%), Loss: 0.0800\n",
      "Epoch 3, Step 1385/1750 (79.14%), Loss: 0.0879\n",
      "Epoch 3, Step 1390/1750 (79.43%), Loss: 0.0038\n",
      "Epoch 3, Step 1395/1750 (79.71%), Loss: 0.1180\n",
      "Epoch 3, Step 1400/1750 (80.00%), Loss: 0.0051\n",
      "Epoch 3, Step 1405/1750 (80.29%), Loss: 0.0024\n",
      "Epoch 3, Step 1410/1750 (80.57%), Loss: 0.0027\n",
      "Epoch 3, Step 1415/1750 (80.86%), Loss: 0.0021\n",
      "Epoch 3, Step 1420/1750 (81.14%), Loss: 0.0037\n",
      "Epoch 3, Step 1425/1750 (81.43%), Loss: 0.0045\n",
      "Epoch 3, Step 1430/1750 (81.71%), Loss: 0.0027\n",
      "Epoch 3, Step 1435/1750 (82.00%), Loss: 0.1811\n",
      "Epoch 3, Step 1440/1750 (82.29%), Loss: 0.2814\n",
      "Epoch 3, Step 1445/1750 (82.57%), Loss: 0.2085\n",
      "Epoch 3, Step 1450/1750 (82.86%), Loss: 0.0145\n",
      "Epoch 3, Step 1455/1750 (83.14%), Loss: 0.0074\n",
      "Epoch 3, Step 1460/1750 (83.43%), Loss: 0.0538\n",
      "Epoch 3, Step 1465/1750 (83.71%), Loss: 0.0037\n",
      "Epoch 3, Step 1470/1750 (84.00%), Loss: 0.0066\n",
      "Epoch 3, Step 1475/1750 (84.29%), Loss: 0.1466\n",
      "Epoch 3, Step 1480/1750 (84.57%), Loss: 0.1497\n",
      "Epoch 3, Step 1485/1750 (84.86%), Loss: 0.0659\n",
      "Epoch 3, Step 1490/1750 (85.14%), Loss: 0.5315\n",
      "Epoch 3, Step 1495/1750 (85.43%), Loss: 0.0610\n",
      "Epoch 3, Step 1500/1750 (85.71%), Loss: 0.1348\n",
      "Epoch 3, Step 1505/1750 (86.00%), Loss: 0.0465\n",
      "Epoch 3, Step 1510/1750 (86.29%), Loss: 0.0601\n",
      "Epoch 3, Step 1515/1750 (86.57%), Loss: 0.0585\n",
      "Epoch 3, Step 1520/1750 (86.86%), Loss: 0.0087\n",
      "Epoch 3, Step 1525/1750 (87.14%), Loss: 0.0075\n",
      "Epoch 3, Step 1530/1750 (87.43%), Loss: 0.0308\n",
      "Epoch 3, Step 1535/1750 (87.71%), Loss: 0.2314\n",
      "Epoch 3, Step 1540/1750 (88.00%), Loss: 0.0435\n",
      "Epoch 3, Step 1545/1750 (88.29%), Loss: 0.0075\n",
      "Epoch 3, Step 1550/1750 (88.57%), Loss: 0.3732\n",
      "Epoch 3, Step 1555/1750 (88.86%), Loss: 0.0100\n",
      "Epoch 3, Step 1560/1750 (89.14%), Loss: 0.0740\n",
      "Epoch 3, Step 1565/1750 (89.43%), Loss: 0.0411\n",
      "Epoch 3, Step 1570/1750 (89.71%), Loss: 0.0029\n",
      "Epoch 3, Step 1575/1750 (90.00%), Loss: 0.1266\n",
      "Epoch 3, Step 1580/1750 (90.29%), Loss: 0.0067\n",
      "Epoch 3, Step 1585/1750 (90.57%), Loss: 0.1327\n",
      "Epoch 3, Step 1590/1750 (90.86%), Loss: 0.0355\n",
      "Epoch 3, Step 1595/1750 (91.14%), Loss: 0.0347\n",
      "Epoch 3, Step 1600/1750 (91.43%), Loss: 0.0122\n",
      "Epoch 3, Step 1605/1750 (91.71%), Loss: 0.0738\n",
      "Epoch 3, Step 1610/1750 (92.00%), Loss: 0.1380\n",
      "Epoch 3, Step 1615/1750 (92.29%), Loss: 0.1482\n",
      "Epoch 3, Step 1620/1750 (92.57%), Loss: 0.0174\n",
      "Epoch 3, Step 1625/1750 (92.86%), Loss: 0.3141\n",
      "Epoch 3, Step 1630/1750 (93.14%), Loss: 0.0062\n",
      "Epoch 3, Step 1635/1750 (93.43%), Loss: 0.0056\n",
      "Epoch 3, Step 1640/1750 (93.71%), Loss: 0.1899\n",
      "Epoch 3, Step 1645/1750 (94.00%), Loss: 0.0165\n",
      "Epoch 3, Step 1650/1750 (94.29%), Loss: 0.0037\n",
      "Epoch 3, Step 1655/1750 (94.57%), Loss: 0.0271\n",
      "Epoch 3, Step 1660/1750 (94.86%), Loss: 0.0052\n",
      "Epoch 3, Step 1665/1750 (95.14%), Loss: 0.0114\n",
      "Epoch 3, Step 1670/1750 (95.43%), Loss: 0.0120\n",
      "Epoch 3, Step 1675/1750 (95.71%), Loss: 0.0044\n",
      "Epoch 3, Step 1680/1750 (96.00%), Loss: 0.0058\n",
      "Epoch 3, Step 1685/1750 (96.29%), Loss: 0.0137\n",
      "Epoch 3, Step 1690/1750 (96.57%), Loss: 0.0410\n",
      "Epoch 3, Step 1695/1750 (96.86%), Loss: 0.0042\n",
      "Epoch 3, Step 1700/1750 (97.14%), Loss: 0.0069\n",
      "Epoch 3, Step 1705/1750 (97.43%), Loss: 0.0045\n",
      "Epoch 3, Step 1710/1750 (97.71%), Loss: 0.0350\n",
      "Epoch 3, Step 1715/1750 (98.00%), Loss: 0.0108\n",
      "Epoch 3, Step 1720/1750 (98.29%), Loss: 0.0134\n",
      "Epoch 3, Step 1725/1750 (98.57%), Loss: 0.0168\n",
      "Epoch 3, Step 1730/1750 (98.86%), Loss: 0.0214\n",
      "Epoch 3, Step 1735/1750 (99.14%), Loss: 0.0222\n",
      "Epoch 3, Step 1740/1750 (99.43%), Loss: 0.0062\n",
      "Epoch 3, Step 1745/1750 (99.71%), Loss: 0.2547\n",
      "Epoch 3, Step 1750/1750 (100.00%), Loss: 0.1284\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.27\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.03\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.02\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.22\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.29\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.30\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.37\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.23\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.32\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.26\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.01\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.03\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.01\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.13\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 0.85\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.04\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.04\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.29\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.13\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.23\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.79\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.32\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.02\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.32\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.03\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.23\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 0.69\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.39\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.31\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.09\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.12\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.01\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.32\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.08\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.40\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.07\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.11\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.08\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.09\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.18\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.40\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.16\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.31\n",
      "End of Epoch 3/10 - Average Train Loss: 0.06, Average Validation Loss: 0.23\n",
      "Starting Epoch 4/10\n",
      "Epoch 4, Step 5/1750 (0.29%), Loss: 0.0186\n",
      "Epoch 4, Step 10/1750 (0.57%), Loss: 0.0417\n",
      "Epoch 4, Step 15/1750 (0.86%), Loss: 0.0081\n",
      "Epoch 4, Step 20/1750 (1.14%), Loss: 0.0074\n",
      "Epoch 4, Step 25/1750 (1.43%), Loss: 0.0376\n",
      "Epoch 4, Step 30/1750 (1.71%), Loss: 0.0220\n",
      "Epoch 4, Step 35/1750 (2.00%), Loss: 0.0011\n",
      "Epoch 4, Step 40/1750 (2.29%), Loss: 0.0074\n",
      "Epoch 4, Step 45/1750 (2.57%), Loss: 0.0017\n",
      "Epoch 4, Step 50/1750 (2.86%), Loss: 0.0071\n",
      "Epoch 4, Step 55/1750 (3.14%), Loss: 0.0389\n",
      "Epoch 4, Step 60/1750 (3.43%), Loss: 0.0023\n",
      "Epoch 4, Step 65/1750 (3.71%), Loss: 0.4205\n",
      "Epoch 4, Step 70/1750 (4.00%), Loss: 0.0012\n",
      "Epoch 4, Step 75/1750 (4.29%), Loss: 0.0135\n",
      "Epoch 4, Step 80/1750 (4.57%), Loss: 0.0023\n",
      "Epoch 4, Step 85/1750 (4.86%), Loss: 0.0060\n",
      "Epoch 4, Step 90/1750 (5.14%), Loss: 0.0071\n",
      "Epoch 4, Step 95/1750 (5.43%), Loss: 0.0034\n",
      "Epoch 4, Step 100/1750 (5.71%), Loss: 0.0032\n",
      "Epoch 4, Step 105/1750 (6.00%), Loss: 0.0063\n",
      "Epoch 4, Step 110/1750 (6.29%), Loss: 0.0127\n",
      "Epoch 4, Step 115/1750 (6.57%), Loss: 0.0013\n",
      "Epoch 4, Step 120/1750 (6.86%), Loss: 0.0201\n",
      "Epoch 4, Step 125/1750 (7.14%), Loss: 0.0036\n",
      "Epoch 4, Step 130/1750 (7.43%), Loss: 0.0801\n",
      "Epoch 4, Step 135/1750 (7.71%), Loss: 0.0040\n",
      "Epoch 4, Step 140/1750 (8.00%), Loss: 0.0015\n",
      "Epoch 4, Step 145/1750 (8.29%), Loss: 0.0034\n",
      "Epoch 4, Step 150/1750 (8.57%), Loss: 0.0072\n",
      "Epoch 4, Step 155/1750 (8.86%), Loss: 0.0008\n",
      "Epoch 4, Step 160/1750 (9.14%), Loss: 0.0007\n",
      "Epoch 4, Step 165/1750 (9.43%), Loss: 0.2537\n",
      "Epoch 4, Step 170/1750 (9.71%), Loss: 0.0009\n",
      "Epoch 4, Step 175/1750 (10.00%), Loss: 0.0018\n",
      "Epoch 4, Step 180/1750 (10.29%), Loss: 0.0041\n",
      "Epoch 4, Step 185/1750 (10.57%), Loss: 0.0099\n",
      "Epoch 4, Step 190/1750 (10.86%), Loss: 0.0584\n",
      "Epoch 4, Step 195/1750 (11.14%), Loss: 0.0074\n",
      "Epoch 4, Step 200/1750 (11.43%), Loss: 0.0044\n",
      "Epoch 4, Step 205/1750 (11.71%), Loss: 0.0036\n",
      "Epoch 4, Step 210/1750 (12.00%), Loss: 0.0022\n",
      "Epoch 4, Step 215/1750 (12.29%), Loss: 0.0030\n",
      "Epoch 4, Step 220/1750 (12.57%), Loss: 0.0044\n",
      "Epoch 4, Step 225/1750 (12.86%), Loss: 0.0024\n",
      "Epoch 4, Step 230/1750 (13.14%), Loss: 0.0031\n",
      "Epoch 4, Step 235/1750 (13.43%), Loss: 0.0017\n",
      "Epoch 4, Step 240/1750 (13.71%), Loss: 0.0017\n",
      "Epoch 4, Step 245/1750 (14.00%), Loss: 0.0008\n",
      "Epoch 4, Step 250/1750 (14.29%), Loss: 0.0679\n",
      "Epoch 4, Step 255/1750 (14.57%), Loss: 0.0123\n",
      "Epoch 4, Step 260/1750 (14.86%), Loss: 0.0024\n",
      "Epoch 4, Step 265/1750 (15.14%), Loss: 0.0020\n",
      "Epoch 4, Step 270/1750 (15.43%), Loss: 0.0029\n",
      "Epoch 4, Step 275/1750 (15.71%), Loss: 0.0212\n",
      "Epoch 4, Step 280/1750 (16.00%), Loss: 0.0325\n",
      "Epoch 4, Step 285/1750 (16.29%), Loss: 0.0037\n",
      "Epoch 4, Step 290/1750 (16.57%), Loss: 0.0011\n",
      "Epoch 4, Step 295/1750 (16.86%), Loss: 0.0146\n",
      "Epoch 4, Step 300/1750 (17.14%), Loss: 0.0013\n",
      "Epoch 4, Step 305/1750 (17.43%), Loss: 0.0035\n",
      "Epoch 4, Step 310/1750 (17.71%), Loss: 0.0245\n",
      "Epoch 4, Step 315/1750 (18.00%), Loss: 0.0610\n",
      "Epoch 4, Step 320/1750 (18.29%), Loss: 0.0068\n",
      "Epoch 4, Step 325/1750 (18.57%), Loss: 0.0006\n",
      "Epoch 4, Step 330/1750 (18.86%), Loss: 0.0008\n",
      "Epoch 4, Step 335/1750 (19.14%), Loss: 0.0011\n",
      "Epoch 4, Step 340/1750 (19.43%), Loss: 0.0006\n",
      "Epoch 4, Step 345/1750 (19.71%), Loss: 0.0017\n",
      "Epoch 4, Step 350/1750 (20.00%), Loss: 0.0022\n",
      "Epoch 4, Step 355/1750 (20.29%), Loss: 0.0048\n",
      "Epoch 4, Step 360/1750 (20.57%), Loss: 0.0013\n",
      "Epoch 4, Step 365/1750 (20.86%), Loss: 0.0004\n",
      "Epoch 4, Step 370/1750 (21.14%), Loss: 0.0115\n",
      "Epoch 4, Step 375/1750 (21.43%), Loss: 0.0031\n",
      "Epoch 4, Step 380/1750 (21.71%), Loss: 0.0004\n",
      "Epoch 4, Step 385/1750 (22.00%), Loss: 0.0025\n",
      "Epoch 4, Step 390/1750 (22.29%), Loss: 0.0017\n",
      "Epoch 4, Step 395/1750 (22.57%), Loss: 0.0021\n",
      "Epoch 4, Step 400/1750 (22.86%), Loss: 0.2986\n",
      "Epoch 4, Step 405/1750 (23.14%), Loss: 0.4132\n",
      "Epoch 4, Step 410/1750 (23.43%), Loss: 0.0108\n",
      "Epoch 4, Step 415/1750 (23.71%), Loss: 0.0010\n",
      "Epoch 4, Step 420/1750 (24.00%), Loss: 0.1295\n",
      "Epoch 4, Step 425/1750 (24.29%), Loss: 0.0019\n",
      "Epoch 4, Step 430/1750 (24.57%), Loss: 0.0016\n",
      "Epoch 4, Step 435/1750 (24.86%), Loss: 0.0012\n",
      "Epoch 4, Step 440/1750 (25.14%), Loss: 0.0013\n",
      "Epoch 4, Step 445/1750 (25.43%), Loss: 0.0019\n",
      "Epoch 4, Step 450/1750 (25.71%), Loss: 0.0206\n",
      "Epoch 4, Step 455/1750 (26.00%), Loss: 0.0013\n",
      "Epoch 4, Step 460/1750 (26.29%), Loss: 0.0009\n",
      "Epoch 4, Step 465/1750 (26.57%), Loss: 0.0111\n",
      "Epoch 4, Step 470/1750 (26.86%), Loss: 0.0169\n",
      "Epoch 4, Step 475/1750 (27.14%), Loss: 0.0022\n",
      "Epoch 4, Step 480/1750 (27.43%), Loss: 0.0123\n",
      "Epoch 4, Step 485/1750 (27.71%), Loss: 0.1944\n",
      "Epoch 4, Step 490/1750 (28.00%), Loss: 0.0074\n",
      "Epoch 4, Step 495/1750 (28.29%), Loss: 0.0022\n",
      "Epoch 4, Step 500/1750 (28.57%), Loss: 0.0100\n",
      "Epoch 4, Step 505/1750 (28.86%), Loss: 0.0012\n",
      "Epoch 4, Step 510/1750 (29.14%), Loss: 0.0280\n",
      "Epoch 4, Step 515/1750 (29.43%), Loss: 0.0008\n",
      "Epoch 4, Step 520/1750 (29.71%), Loss: 0.0939\n",
      "Epoch 4, Step 525/1750 (30.00%), Loss: 0.0026\n",
      "Epoch 4, Step 530/1750 (30.29%), Loss: 0.5600\n",
      "Epoch 4, Step 535/1750 (30.57%), Loss: 0.0046\n",
      "Epoch 4, Step 540/1750 (30.86%), Loss: 0.0178\n",
      "Epoch 4, Step 545/1750 (31.14%), Loss: 0.0042\n",
      "Epoch 4, Step 550/1750 (31.43%), Loss: 0.0063\n",
      "Epoch 4, Step 555/1750 (31.71%), Loss: 0.0031\n",
      "Epoch 4, Step 560/1750 (32.00%), Loss: 0.0194\n",
      "Epoch 4, Step 565/1750 (32.29%), Loss: 0.0024\n",
      "Epoch 4, Step 570/1750 (32.57%), Loss: 0.1651\n",
      "Epoch 4, Step 575/1750 (32.86%), Loss: 0.4509\n",
      "Epoch 4, Step 580/1750 (33.14%), Loss: 0.0044\n",
      "Epoch 4, Step 585/1750 (33.43%), Loss: 0.0140\n",
      "Epoch 4, Step 590/1750 (33.71%), Loss: 0.0007\n",
      "Epoch 4, Step 595/1750 (34.00%), Loss: 0.0344\n",
      "Epoch 4, Step 600/1750 (34.29%), Loss: 0.0008\n",
      "Epoch 4, Step 605/1750 (34.57%), Loss: 0.0011\n",
      "Epoch 4, Step 610/1750 (34.86%), Loss: 0.0222\n",
      "Epoch 4, Step 615/1750 (35.14%), Loss: 0.0085\n",
      "Epoch 4, Step 620/1750 (35.43%), Loss: 0.0019\n",
      "Epoch 4, Step 625/1750 (35.71%), Loss: 0.0007\n",
      "Epoch 4, Step 630/1750 (36.00%), Loss: 0.0010\n",
      "Epoch 4, Step 635/1750 (36.29%), Loss: 0.2860\n",
      "Epoch 4, Step 640/1750 (36.57%), Loss: 0.0007\n",
      "Epoch 4, Step 645/1750 (36.86%), Loss: 0.0010\n",
      "Epoch 4, Step 650/1750 (37.14%), Loss: 0.0012\n",
      "Epoch 4, Step 655/1750 (37.43%), Loss: 0.0041\n",
      "Epoch 4, Step 660/1750 (37.71%), Loss: 0.0010\n",
      "Epoch 4, Step 665/1750 (38.00%), Loss: 0.0016\n",
      "Epoch 4, Step 670/1750 (38.29%), Loss: 0.0012\n",
      "Epoch 4, Step 675/1750 (38.57%), Loss: 0.0089\n",
      "Epoch 4, Step 680/1750 (38.86%), Loss: 0.0506\n",
      "Epoch 4, Step 685/1750 (39.14%), Loss: 0.0053\n",
      "Epoch 4, Step 690/1750 (39.43%), Loss: 0.0063\n",
      "Epoch 4, Step 695/1750 (39.71%), Loss: 0.0019\n",
      "Epoch 4, Step 700/1750 (40.00%), Loss: 0.0035\n",
      "Epoch 4, Step 705/1750 (40.29%), Loss: 0.0142\n",
      "Epoch 4, Step 710/1750 (40.57%), Loss: 0.0058\n",
      "Epoch 4, Step 715/1750 (40.86%), Loss: 0.0040\n",
      "Epoch 4, Step 720/1750 (41.14%), Loss: 0.0069\n",
      "Epoch 4, Step 725/1750 (41.43%), Loss: 0.0044\n",
      "Epoch 4, Step 730/1750 (41.71%), Loss: 0.4317\n",
      "Epoch 4, Step 735/1750 (42.00%), Loss: 0.0017\n",
      "Epoch 4, Step 740/1750 (42.29%), Loss: 0.0067\n",
      "Epoch 4, Step 745/1750 (42.57%), Loss: 0.0062\n",
      "Epoch 4, Step 750/1750 (42.86%), Loss: 0.1682\n",
      "Epoch 4, Step 755/1750 (43.14%), Loss: 0.0055\n",
      "Epoch 4, Step 760/1750 (43.43%), Loss: 0.1425\n",
      "Epoch 4, Step 765/1750 (43.71%), Loss: 0.0036\n",
      "Epoch 4, Step 770/1750 (44.00%), Loss: 0.4368\n",
      "Epoch 4, Step 775/1750 (44.29%), Loss: 0.0015\n",
      "Epoch 4, Step 780/1750 (44.57%), Loss: 0.0030\n",
      "Epoch 4, Step 785/1750 (44.86%), Loss: 0.0095\n",
      "Epoch 4, Step 790/1750 (45.14%), Loss: 0.0036\n",
      "Epoch 4, Step 795/1750 (45.43%), Loss: 0.1074\n",
      "Epoch 4, Step 800/1750 (45.71%), Loss: 0.1977\n",
      "Epoch 4, Step 805/1750 (46.00%), Loss: 0.0027\n",
      "Epoch 4, Step 810/1750 (46.29%), Loss: 0.0065\n",
      "Epoch 4, Step 815/1750 (46.57%), Loss: 0.0921\n",
      "Epoch 4, Step 820/1750 (46.86%), Loss: 0.0074\n",
      "Epoch 4, Step 825/1750 (47.14%), Loss: 0.0054\n",
      "Epoch 4, Step 830/1750 (47.43%), Loss: 0.0150\n",
      "Epoch 4, Step 835/1750 (47.71%), Loss: 0.0029\n",
      "Epoch 4, Step 840/1750 (48.00%), Loss: 0.0076\n",
      "Epoch 4, Step 845/1750 (48.29%), Loss: 0.0192\n",
      "Epoch 4, Step 850/1750 (48.57%), Loss: 0.0026\n",
      "Epoch 4, Step 855/1750 (48.86%), Loss: 0.0107\n",
      "Epoch 4, Step 860/1750 (49.14%), Loss: 0.0019\n",
      "Epoch 4, Step 865/1750 (49.43%), Loss: 0.0011\n",
      "Epoch 4, Step 870/1750 (49.71%), Loss: 0.0225\n",
      "Epoch 4, Step 875/1750 (50.00%), Loss: 0.0640\n",
      "Epoch 4, Step 880/1750 (50.29%), Loss: 0.0107\n",
      "Epoch 4, Step 885/1750 (50.57%), Loss: 0.0014\n",
      "Epoch 4, Step 890/1750 (50.86%), Loss: 0.0009\n",
      "Epoch 4, Step 895/1750 (51.14%), Loss: 0.0030\n",
      "Epoch 4, Step 900/1750 (51.43%), Loss: 0.0262\n",
      "Epoch 4, Step 905/1750 (51.71%), Loss: 0.2301\n",
      "Epoch 4, Step 910/1750 (52.00%), Loss: 0.0012\n",
      "Epoch 4, Step 915/1750 (52.29%), Loss: 0.0049\n",
      "Epoch 4, Step 920/1750 (52.57%), Loss: 0.0029\n",
      "Epoch 4, Step 925/1750 (52.86%), Loss: 0.0782\n",
      "Epoch 4, Step 930/1750 (53.14%), Loss: 0.0018\n",
      "Epoch 4, Step 935/1750 (53.43%), Loss: 0.0052\n",
      "Epoch 4, Step 940/1750 (53.71%), Loss: 0.0019\n",
      "Epoch 4, Step 945/1750 (54.00%), Loss: 0.0122\n",
      "Epoch 4, Step 950/1750 (54.29%), Loss: 0.0137\n",
      "Epoch 4, Step 955/1750 (54.57%), Loss: 0.0010\n",
      "Epoch 4, Step 960/1750 (54.86%), Loss: 0.0160\n",
      "Epoch 4, Step 965/1750 (55.14%), Loss: 0.0015\n",
      "Epoch 4, Step 970/1750 (55.43%), Loss: 0.0089\n",
      "Epoch 4, Step 975/1750 (55.71%), Loss: 0.0464\n",
      "Epoch 4, Step 980/1750 (56.00%), Loss: 0.0033\n",
      "Epoch 4, Step 985/1750 (56.29%), Loss: 0.0022\n",
      "Epoch 4, Step 990/1750 (56.57%), Loss: 0.0035\n",
      "Epoch 4, Step 995/1750 (56.86%), Loss: 0.0007\n",
      "Epoch 4, Step 1000/1750 (57.14%), Loss: 0.0105\n",
      "Epoch 4, Step 1005/1750 (57.43%), Loss: 0.0058\n",
      "Epoch 4, Step 1010/1750 (57.71%), Loss: 0.0010\n",
      "Epoch 4, Step 1015/1750 (58.00%), Loss: 0.0028\n",
      "Epoch 4, Step 1020/1750 (58.29%), Loss: 0.0268\n",
      "Epoch 4, Step 1025/1750 (58.57%), Loss: 0.0533\n",
      "Epoch 4, Step 1030/1750 (58.86%), Loss: 0.0132\n",
      "Epoch 4, Step 1035/1750 (59.14%), Loss: 0.0097\n",
      "Epoch 4, Step 1040/1750 (59.43%), Loss: 0.0078\n",
      "Epoch 4, Step 1045/1750 (59.71%), Loss: 0.0139\n",
      "Epoch 4, Step 1050/1750 (60.00%), Loss: 0.0038\n",
      "Epoch 4, Step 1055/1750 (60.29%), Loss: 0.0057\n",
      "Epoch 4, Step 1060/1750 (60.57%), Loss: 0.0010\n",
      "Epoch 4, Step 1065/1750 (60.86%), Loss: 0.0011\n",
      "Epoch 4, Step 1070/1750 (61.14%), Loss: 0.0017\n",
      "Epoch 4, Step 1075/1750 (61.43%), Loss: 0.0009\n",
      "Epoch 4, Step 1080/1750 (61.71%), Loss: 0.0018\n",
      "Epoch 4, Step 1085/1750 (62.00%), Loss: 0.0009\n",
      "Epoch 4, Step 1090/1750 (62.29%), Loss: 0.0014\n",
      "Epoch 4, Step 1095/1750 (62.57%), Loss: 0.0043\n",
      "Epoch 4, Step 1100/1750 (62.86%), Loss: 0.0148\n",
      "Epoch 4, Step 1105/1750 (63.14%), Loss: 0.0049\n",
      "Epoch 4, Step 1110/1750 (63.43%), Loss: 0.0123\n",
      "Epoch 4, Step 1115/1750 (63.71%), Loss: 0.0166\n",
      "Epoch 4, Step 1120/1750 (64.00%), Loss: 0.4415\n",
      "Epoch 4, Step 1125/1750 (64.29%), Loss: 0.0292\n",
      "Epoch 4, Step 1130/1750 (64.57%), Loss: 0.0159\n",
      "Epoch 4, Step 1135/1750 (64.86%), Loss: 0.0045\n",
      "Epoch 4, Step 1140/1750 (65.14%), Loss: 0.0065\n",
      "Epoch 4, Step 1145/1750 (65.43%), Loss: 0.0327\n",
      "Epoch 4, Step 1150/1750 (65.71%), Loss: 0.0017\n",
      "Epoch 4, Step 1155/1750 (66.00%), Loss: 0.0043\n",
      "Epoch 4, Step 1160/1750 (66.29%), Loss: 0.0037\n",
      "Epoch 4, Step 1165/1750 (66.57%), Loss: 0.0016\n",
      "Epoch 4, Step 1170/1750 (66.86%), Loss: 0.0018\n",
      "Epoch 4, Step 1175/1750 (67.14%), Loss: 0.0065\n",
      "Epoch 4, Step 1180/1750 (67.43%), Loss: 0.0582\n",
      "Epoch 4, Step 1185/1750 (67.71%), Loss: 0.0021\n",
      "Epoch 4, Step 1190/1750 (68.00%), Loss: 0.0010\n",
      "Epoch 4, Step 1195/1750 (68.29%), Loss: 0.0008\n",
      "Epoch 4, Step 1200/1750 (68.57%), Loss: 0.0018\n",
      "Epoch 4, Step 1205/1750 (68.86%), Loss: 0.0006\n",
      "Epoch 4, Step 1210/1750 (69.14%), Loss: 0.0067\n",
      "Epoch 4, Step 1215/1750 (69.43%), Loss: 0.4630\n",
      "Epoch 4, Step 1220/1750 (69.71%), Loss: 0.0035\n",
      "Epoch 4, Step 1225/1750 (70.00%), Loss: 0.0041\n",
      "Epoch 4, Step 1230/1750 (70.29%), Loss: 0.0114\n",
      "Epoch 4, Step 1235/1750 (70.57%), Loss: 0.0063\n",
      "Epoch 4, Step 1240/1750 (70.86%), Loss: 0.0028\n",
      "Epoch 4, Step 1245/1750 (71.14%), Loss: 0.0020\n",
      "Epoch 4, Step 1250/1750 (71.43%), Loss: 0.1110\n",
      "Epoch 4, Step 1255/1750 (71.71%), Loss: 0.0016\n",
      "Epoch 4, Step 1260/1750 (72.00%), Loss: 0.0034\n",
      "Epoch 4, Step 1265/1750 (72.29%), Loss: 0.0093\n",
      "Epoch 4, Step 1270/1750 (72.57%), Loss: 0.0435\n",
      "Epoch 4, Step 1275/1750 (72.86%), Loss: 0.0146\n",
      "Epoch 4, Step 1280/1750 (73.14%), Loss: 0.0167\n",
      "Epoch 4, Step 1285/1750 (73.43%), Loss: 0.0035\n",
      "Epoch 4, Step 1290/1750 (73.71%), Loss: 0.0016\n",
      "Epoch 4, Step 1295/1750 (74.00%), Loss: 0.0029\n",
      "Epoch 4, Step 1300/1750 (74.29%), Loss: 0.0013\n",
      "Epoch 4, Step 1305/1750 (74.57%), Loss: 0.0193\n",
      "Epoch 4, Step 1310/1750 (74.86%), Loss: 0.0013\n",
      "Epoch 4, Step 1315/1750 (75.14%), Loss: 0.2259\n",
      "Epoch 4, Step 1320/1750 (75.43%), Loss: 0.2031\n",
      "Epoch 4, Step 1325/1750 (75.71%), Loss: 0.0264\n",
      "Epoch 4, Step 1330/1750 (76.00%), Loss: 0.0144\n",
      "Epoch 4, Step 1335/1750 (76.29%), Loss: 0.0497\n",
      "Epoch 4, Step 1340/1750 (76.57%), Loss: 0.0255\n",
      "Epoch 4, Step 1345/1750 (76.86%), Loss: 0.0045\n",
      "Epoch 4, Step 1350/1750 (77.14%), Loss: 0.0630\n",
      "Epoch 4, Step 1355/1750 (77.43%), Loss: 0.0079\n",
      "Epoch 4, Step 1360/1750 (77.71%), Loss: 0.0306\n",
      "Epoch 4, Step 1365/1750 (78.00%), Loss: 0.0067\n",
      "Epoch 4, Step 1370/1750 (78.29%), Loss: 0.0026\n",
      "Epoch 4, Step 1375/1750 (78.57%), Loss: 0.5667\n",
      "Epoch 4, Step 1380/1750 (78.86%), Loss: 0.0472\n",
      "Epoch 4, Step 1385/1750 (79.14%), Loss: 0.0210\n",
      "Epoch 4, Step 1390/1750 (79.43%), Loss: 0.2250\n",
      "Epoch 4, Step 1395/1750 (79.71%), Loss: 0.0244\n",
      "Epoch 4, Step 1400/1750 (80.00%), Loss: 0.1088\n",
      "Epoch 4, Step 1405/1750 (80.29%), Loss: 0.0343\n",
      "Epoch 4, Step 1410/1750 (80.57%), Loss: 0.0019\n",
      "Epoch 4, Step 1415/1750 (80.86%), Loss: 0.0033\n",
      "Epoch 4, Step 1420/1750 (81.14%), Loss: 0.0025\n",
      "Epoch 4, Step 1425/1750 (81.43%), Loss: 0.0330\n",
      "Epoch 4, Step 1430/1750 (81.71%), Loss: 0.0090\n",
      "Epoch 4, Step 1435/1750 (82.00%), Loss: 0.0018\n",
      "Epoch 4, Step 1440/1750 (82.29%), Loss: 0.0023\n",
      "Epoch 4, Step 1445/1750 (82.57%), Loss: 0.0028\n",
      "Epoch 4, Step 1450/1750 (82.86%), Loss: 0.0019\n",
      "Epoch 4, Step 1455/1750 (83.14%), Loss: 0.0010\n",
      "Epoch 4, Step 1460/1750 (83.43%), Loss: 0.0021\n",
      "Epoch 4, Step 1465/1750 (83.71%), Loss: 0.0010\n",
      "Epoch 4, Step 1470/1750 (84.00%), Loss: 0.0008\n",
      "Epoch 4, Step 1475/1750 (84.29%), Loss: 0.0048\n",
      "Epoch 4, Step 1480/1750 (84.57%), Loss: 0.0435\n",
      "Epoch 4, Step 1485/1750 (84.86%), Loss: 0.0013\n",
      "Epoch 4, Step 1490/1750 (85.14%), Loss: 0.0017\n",
      "Epoch 4, Step 1495/1750 (85.43%), Loss: 0.0659\n",
      "Epoch 4, Step 1500/1750 (85.71%), Loss: 0.0435\n",
      "Epoch 4, Step 1505/1750 (86.00%), Loss: 0.0015\n",
      "Epoch 4, Step 1510/1750 (86.29%), Loss: 0.0197\n",
      "Epoch 4, Step 1515/1750 (86.57%), Loss: 0.0213\n",
      "Epoch 4, Step 1520/1750 (86.86%), Loss: 0.0084\n",
      "Epoch 4, Step 1525/1750 (87.14%), Loss: 0.0320\n",
      "Epoch 4, Step 1530/1750 (87.43%), Loss: 0.0029\n",
      "Epoch 4, Step 1535/1750 (87.71%), Loss: 0.0142\n",
      "Epoch 4, Step 1540/1750 (88.00%), Loss: 0.2139\n",
      "Epoch 4, Step 1545/1750 (88.29%), Loss: 0.0027\n",
      "Epoch 4, Step 1550/1750 (88.57%), Loss: 0.0042\n",
      "Epoch 4, Step 1555/1750 (88.86%), Loss: 0.0011\n",
      "Epoch 4, Step 1560/1750 (89.14%), Loss: 0.0178\n",
      "Epoch 4, Step 1565/1750 (89.43%), Loss: 0.0805\n",
      "Epoch 4, Step 1570/1750 (89.71%), Loss: 0.0012\n",
      "Epoch 4, Step 1575/1750 (90.00%), Loss: 0.0856\n",
      "Epoch 4, Step 1580/1750 (90.29%), Loss: 0.0075\n",
      "Epoch 4, Step 1585/1750 (90.57%), Loss: 0.0460\n",
      "Epoch 4, Step 1590/1750 (90.86%), Loss: 0.0076\n",
      "Epoch 4, Step 1595/1750 (91.14%), Loss: 0.0078\n",
      "Epoch 4, Step 1600/1750 (91.43%), Loss: 0.0025\n",
      "Epoch 4, Step 1605/1750 (91.71%), Loss: 0.0502\n",
      "Epoch 4, Step 1610/1750 (92.00%), Loss: 0.0005\n",
      "Epoch 4, Step 1615/1750 (92.29%), Loss: 0.0006\n",
      "Epoch 4, Step 1620/1750 (92.57%), Loss: 0.0010\n",
      "Epoch 4, Step 1625/1750 (92.86%), Loss: 0.0013\n",
      "Epoch 4, Step 1630/1750 (93.14%), Loss: 0.0005\n",
      "Epoch 4, Step 1635/1750 (93.43%), Loss: 0.0389\n",
      "Epoch 4, Step 1640/1750 (93.71%), Loss: 0.0007\n",
      "Epoch 4, Step 1645/1750 (94.00%), Loss: 0.0127\n",
      "Epoch 4, Step 1650/1750 (94.29%), Loss: 0.0022\n",
      "Epoch 4, Step 1655/1750 (94.57%), Loss: 0.0102\n",
      "Epoch 4, Step 1660/1750 (94.86%), Loss: 0.0165\n",
      "Epoch 4, Step 1665/1750 (95.14%), Loss: 0.0033\n",
      "Epoch 4, Step 1670/1750 (95.43%), Loss: 0.0043\n",
      "Epoch 4, Step 1675/1750 (95.71%), Loss: 0.0016\n",
      "Epoch 4, Step 1680/1750 (96.00%), Loss: 0.0031\n",
      "Epoch 4, Step 1685/1750 (96.29%), Loss: 0.0010\n",
      "Epoch 4, Step 1690/1750 (96.57%), Loss: 0.0013\n",
      "Epoch 4, Step 1695/1750 (96.86%), Loss: 0.0083\n",
      "Epoch 4, Step 1700/1750 (97.14%), Loss: 0.0168\n",
      "Epoch 4, Step 1705/1750 (97.43%), Loss: 0.0043\n",
      "Epoch 4, Step 1710/1750 (97.71%), Loss: 0.0046\n",
      "Epoch 4, Step 1715/1750 (98.00%), Loss: 0.0081\n",
      "Epoch 4, Step 1720/1750 (98.29%), Loss: 0.0161\n",
      "Epoch 4, Step 1725/1750 (98.57%), Loss: 0.0520\n",
      "Epoch 4, Step 1730/1750 (98.86%), Loss: 0.0463\n",
      "Epoch 4, Step 1735/1750 (99.14%), Loss: 0.0021\n",
      "Epoch 4, Step 1740/1750 (99.43%), Loss: 0.0009\n",
      "Epoch 4, Step 1745/1750 (99.71%), Loss: 0.0014\n",
      "Epoch 4, Step 1750/1750 (100.00%), Loss: 0.0029\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.40\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.26\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.01\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.51\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.69\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.33\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.50\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.43\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.78\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.43\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.24\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.01\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.01\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 1.57\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.15\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.04\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.44\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.24\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.30\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.87\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.46\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.03\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.43\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.00\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.38\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 1.01\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.23\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.41\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.04\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.00\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.12\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.70\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.02\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.53\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 0.94\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.28\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.12\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.11\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.01\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.47\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.01\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.40\n",
      "End of Epoch 4/10 - Average Train Loss: 0.03, Average Validation Loss: 0.30\n",
      "Starting Epoch 5/10\n",
      "Epoch 5, Step 5/1750 (0.29%), Loss: 0.0018\n",
      "Epoch 5, Step 10/1750 (0.57%), Loss: 0.0073\n",
      "Epoch 5, Step 15/1750 (0.86%), Loss: 0.0166\n",
      "Epoch 5, Step 20/1750 (1.14%), Loss: 0.0016\n",
      "Epoch 5, Step 25/1750 (1.43%), Loss: 0.0007\n",
      "Epoch 5, Step 30/1750 (1.71%), Loss: 0.0020\n",
      "Epoch 5, Step 35/1750 (2.00%), Loss: 0.0143\n",
      "Epoch 5, Step 40/1750 (2.29%), Loss: 0.0019\n",
      "Epoch 5, Step 45/1750 (2.57%), Loss: 0.0015\n",
      "Epoch 5, Step 50/1750 (2.86%), Loss: 0.0362\n",
      "Epoch 5, Step 55/1750 (3.14%), Loss: 0.0011\n",
      "Epoch 5, Step 60/1750 (3.43%), Loss: 0.0514\n",
      "Epoch 5, Step 65/1750 (3.71%), Loss: 0.0016\n",
      "Epoch 5, Step 70/1750 (4.00%), Loss: 0.0010\n",
      "Epoch 5, Step 75/1750 (4.29%), Loss: 0.0015\n",
      "Epoch 5, Step 80/1750 (4.57%), Loss: 0.0089\n",
      "Epoch 5, Step 85/1750 (4.86%), Loss: 0.0088\n",
      "Epoch 5, Step 90/1750 (5.14%), Loss: 0.0015\n",
      "Epoch 5, Step 95/1750 (5.43%), Loss: 0.0025\n",
      "Epoch 5, Step 100/1750 (5.71%), Loss: 0.0011\n",
      "Epoch 5, Step 105/1750 (6.00%), Loss: 0.0020\n",
      "Epoch 5, Step 110/1750 (6.29%), Loss: 0.0189\n",
      "Epoch 5, Step 115/1750 (6.57%), Loss: 0.0045\n",
      "Epoch 5, Step 120/1750 (6.86%), Loss: 0.0007\n",
      "Epoch 5, Step 125/1750 (7.14%), Loss: 0.0098\n",
      "Epoch 5, Step 130/1750 (7.43%), Loss: 0.0023\n",
      "Epoch 5, Step 135/1750 (7.71%), Loss: 0.2174\n",
      "Epoch 5, Step 140/1750 (8.00%), Loss: 0.0015\n",
      "Epoch 5, Step 145/1750 (8.29%), Loss: 0.0014\n",
      "Epoch 5, Step 150/1750 (8.57%), Loss: 0.0006\n",
      "Epoch 5, Step 155/1750 (8.86%), Loss: 0.0031\n",
      "Epoch 5, Step 160/1750 (9.14%), Loss: 0.0010\n",
      "Epoch 5, Step 165/1750 (9.43%), Loss: 0.0009\n",
      "Epoch 5, Step 170/1750 (9.71%), Loss: 0.0026\n",
      "Epoch 5, Step 175/1750 (10.00%), Loss: 0.0328\n",
      "Epoch 5, Step 180/1750 (10.29%), Loss: 0.0015\n",
      "Epoch 5, Step 185/1750 (10.57%), Loss: 0.0049\n",
      "Epoch 5, Step 190/1750 (10.86%), Loss: 0.0078\n",
      "Epoch 5, Step 195/1750 (11.14%), Loss: 0.0004\n",
      "Epoch 5, Step 200/1750 (11.43%), Loss: 0.0056\n",
      "Epoch 5, Step 205/1750 (11.71%), Loss: 0.0099\n",
      "Epoch 5, Step 210/1750 (12.00%), Loss: 0.0016\n",
      "Epoch 5, Step 215/1750 (12.29%), Loss: 0.0010\n",
      "Epoch 5, Step 220/1750 (12.57%), Loss: 0.0491\n",
      "Epoch 5, Step 225/1750 (12.86%), Loss: 0.0009\n",
      "Epoch 5, Step 230/1750 (13.14%), Loss: 0.0016\n",
      "Epoch 5, Step 235/1750 (13.43%), Loss: 0.0021\n",
      "Epoch 5, Step 240/1750 (13.71%), Loss: 0.0008\n",
      "Epoch 5, Step 245/1750 (14.00%), Loss: 0.0028\n",
      "Epoch 5, Step 250/1750 (14.29%), Loss: 0.0783\n",
      "Epoch 5, Step 255/1750 (14.57%), Loss: 0.0014\n",
      "Epoch 5, Step 260/1750 (14.86%), Loss: 0.0009\n",
      "Epoch 5, Step 265/1750 (15.14%), Loss: 0.0010\n",
      "Epoch 5, Step 270/1750 (15.43%), Loss: 0.1335\n",
      "Epoch 5, Step 275/1750 (15.71%), Loss: 0.2083\n",
      "Epoch 5, Step 280/1750 (16.00%), Loss: 0.0013\n",
      "Epoch 5, Step 285/1750 (16.29%), Loss: 0.1800\n",
      "Epoch 5, Step 290/1750 (16.57%), Loss: 0.0072\n",
      "Epoch 5, Step 295/1750 (16.86%), Loss: 0.0017\n",
      "Epoch 5, Step 300/1750 (17.14%), Loss: 0.0022\n",
      "Epoch 5, Step 305/1750 (17.43%), Loss: 0.0016\n",
      "Epoch 5, Step 310/1750 (17.71%), Loss: 0.0067\n",
      "Epoch 5, Step 315/1750 (18.00%), Loss: 0.0388\n",
      "Epoch 5, Step 320/1750 (18.29%), Loss: 0.0014\n",
      "Epoch 5, Step 325/1750 (18.57%), Loss: 0.0038\n",
      "Epoch 5, Step 330/1750 (18.86%), Loss: 0.0012\n",
      "Epoch 5, Step 335/1750 (19.14%), Loss: 0.0040\n",
      "Epoch 5, Step 340/1750 (19.43%), Loss: 0.0006\n",
      "Epoch 5, Step 345/1750 (19.71%), Loss: 0.0008\n",
      "Epoch 5, Step 350/1750 (20.00%), Loss: 0.0716\n",
      "Epoch 5, Step 355/1750 (20.29%), Loss: 0.0010\n",
      "Epoch 5, Step 360/1750 (20.57%), Loss: 0.0020\n",
      "Epoch 5, Step 365/1750 (20.86%), Loss: 0.0006\n",
      "Epoch 5, Step 370/1750 (21.14%), Loss: 0.3193\n",
      "Epoch 5, Step 375/1750 (21.43%), Loss: 0.1775\n",
      "Epoch 5, Step 380/1750 (21.71%), Loss: 0.0014\n",
      "Epoch 5, Step 385/1750 (22.00%), Loss: 0.0019\n",
      "Epoch 5, Step 390/1750 (22.29%), Loss: 0.0014\n",
      "Epoch 5, Step 395/1750 (22.57%), Loss: 0.0055\n",
      "Epoch 5, Step 400/1750 (22.86%), Loss: 0.0168\n",
      "Epoch 5, Step 405/1750 (23.14%), Loss: 0.0008\n",
      "Epoch 5, Step 410/1750 (23.43%), Loss: 0.0035\n",
      "Epoch 5, Step 415/1750 (23.71%), Loss: 0.0021\n",
      "Epoch 5, Step 420/1750 (24.00%), Loss: 0.0027\n",
      "Epoch 5, Step 425/1750 (24.29%), Loss: 0.0003\n",
      "Epoch 5, Step 430/1750 (24.57%), Loss: 0.0788\n",
      "Epoch 5, Step 435/1750 (24.86%), Loss: 0.0009\n",
      "Epoch 5, Step 440/1750 (25.14%), Loss: 0.0011\n",
      "Epoch 5, Step 445/1750 (25.43%), Loss: 0.0004\n",
      "Epoch 5, Step 450/1750 (25.71%), Loss: 0.4890\n",
      "Epoch 5, Step 455/1750 (26.00%), Loss: 0.0011\n",
      "Epoch 5, Step 460/1750 (26.29%), Loss: 0.0021\n",
      "Epoch 5, Step 465/1750 (26.57%), Loss: 0.0008\n",
      "Epoch 5, Step 470/1750 (26.86%), Loss: 0.0011\n",
      "Epoch 5, Step 475/1750 (27.14%), Loss: 0.0043\n",
      "Epoch 5, Step 480/1750 (27.43%), Loss: 0.0011\n",
      "Epoch 5, Step 485/1750 (27.71%), Loss: 0.1139\n",
      "Epoch 5, Step 490/1750 (28.00%), Loss: 0.0074\n",
      "Epoch 5, Step 495/1750 (28.29%), Loss: 0.0011\n",
      "Epoch 5, Step 500/1750 (28.57%), Loss: 0.2269\n",
      "Epoch 5, Step 505/1750 (28.86%), Loss: 0.0009\n",
      "Epoch 5, Step 510/1750 (29.14%), Loss: 0.0011\n",
      "Epoch 5, Step 515/1750 (29.43%), Loss: 0.4216\n",
      "Epoch 5, Step 520/1750 (29.71%), Loss: 0.0033\n",
      "Epoch 5, Step 525/1750 (30.00%), Loss: 0.0027\n",
      "Epoch 5, Step 530/1750 (30.29%), Loss: 0.0014\n",
      "Epoch 5, Step 535/1750 (30.57%), Loss: 0.0572\n",
      "Epoch 5, Step 540/1750 (30.86%), Loss: 0.0046\n",
      "Epoch 5, Step 545/1750 (31.14%), Loss: 0.0016\n",
      "Epoch 5, Step 550/1750 (31.43%), Loss: 0.0034\n",
      "Epoch 5, Step 555/1750 (31.71%), Loss: 0.0032\n",
      "Epoch 5, Step 560/1750 (32.00%), Loss: 0.0006\n",
      "Epoch 5, Step 565/1750 (32.29%), Loss: 0.0038\n",
      "Epoch 5, Step 570/1750 (32.57%), Loss: 0.0281\n",
      "Epoch 5, Step 575/1750 (32.86%), Loss: 0.0005\n",
      "Epoch 5, Step 580/1750 (33.14%), Loss: 0.0007\n",
      "Epoch 5, Step 585/1750 (33.43%), Loss: 0.0105\n",
      "Epoch 5, Step 590/1750 (33.71%), Loss: 0.0015\n",
      "Epoch 5, Step 595/1750 (34.00%), Loss: 0.0018\n",
      "Epoch 5, Step 600/1750 (34.29%), Loss: 0.2423\n",
      "Epoch 5, Step 605/1750 (34.57%), Loss: 0.0155\n",
      "Epoch 5, Step 610/1750 (34.86%), Loss: 0.0017\n",
      "Epoch 5, Step 615/1750 (35.14%), Loss: 0.0011\n",
      "Epoch 5, Step 620/1750 (35.43%), Loss: 0.0016\n",
      "Epoch 5, Step 625/1750 (35.71%), Loss: 0.0010\n",
      "Epoch 5, Step 630/1750 (36.00%), Loss: 0.0020\n",
      "Epoch 5, Step 635/1750 (36.29%), Loss: 0.0010\n",
      "Epoch 5, Step 640/1750 (36.57%), Loss: 0.0004\n",
      "Epoch 5, Step 645/1750 (36.86%), Loss: 0.0003\n",
      "Epoch 5, Step 650/1750 (37.14%), Loss: 0.0328\n",
      "Epoch 5, Step 655/1750 (37.43%), Loss: 0.0003\n",
      "Epoch 5, Step 660/1750 (37.71%), Loss: 0.0005\n",
      "Epoch 5, Step 665/1750 (38.00%), Loss: 0.0004\n",
      "Epoch 5, Step 670/1750 (38.29%), Loss: 0.0023\n",
      "Epoch 5, Step 675/1750 (38.57%), Loss: 0.0006\n",
      "Epoch 5, Step 680/1750 (38.86%), Loss: 0.0008\n",
      "Epoch 5, Step 685/1750 (39.14%), Loss: 0.0008\n",
      "Epoch 5, Step 690/1750 (39.43%), Loss: 0.0443\n",
      "Epoch 5, Step 695/1750 (39.71%), Loss: 0.0465\n",
      "Epoch 5, Step 700/1750 (40.00%), Loss: 0.1113\n",
      "Epoch 5, Step 705/1750 (40.29%), Loss: 0.0320\n",
      "Epoch 5, Step 710/1750 (40.57%), Loss: 0.0018\n",
      "Epoch 5, Step 715/1750 (40.86%), Loss: 0.0017\n",
      "Epoch 5, Step 720/1750 (41.14%), Loss: 0.0354\n",
      "Epoch 5, Step 725/1750 (41.43%), Loss: 0.0015\n",
      "Epoch 5, Step 730/1750 (41.71%), Loss: 0.0147\n",
      "Epoch 5, Step 735/1750 (42.00%), Loss: 0.0008\n",
      "Epoch 5, Step 740/1750 (42.29%), Loss: 0.0027\n",
      "Epoch 5, Step 745/1750 (42.57%), Loss: 0.0019\n",
      "Epoch 5, Step 750/1750 (42.86%), Loss: 0.0013\n",
      "Epoch 5, Step 755/1750 (43.14%), Loss: 0.0009\n",
      "Epoch 5, Step 760/1750 (43.43%), Loss: 0.0017\n",
      "Epoch 5, Step 765/1750 (43.71%), Loss: 0.0777\n",
      "Epoch 5, Step 770/1750 (44.00%), Loss: 0.0006\n",
      "Epoch 5, Step 775/1750 (44.29%), Loss: 0.0007\n",
      "Epoch 5, Step 780/1750 (44.57%), Loss: 0.0161\n",
      "Epoch 5, Step 785/1750 (44.86%), Loss: 0.0113\n",
      "Epoch 5, Step 790/1750 (45.14%), Loss: 0.0040\n",
      "Epoch 5, Step 795/1750 (45.43%), Loss: 0.0043\n",
      "Epoch 5, Step 800/1750 (45.71%), Loss: 0.0005\n",
      "Epoch 5, Step 805/1750 (46.00%), Loss: 0.0514\n",
      "Epoch 5, Step 810/1750 (46.29%), Loss: 0.0016\n",
      "Epoch 5, Step 815/1750 (46.57%), Loss: 0.0006\n",
      "Epoch 5, Step 820/1750 (46.86%), Loss: 0.0008\n",
      "Epoch 5, Step 825/1750 (47.14%), Loss: 0.0008\n",
      "Epoch 5, Step 830/1750 (47.43%), Loss: 0.0453\n",
      "Epoch 5, Step 835/1750 (47.71%), Loss: 0.0041\n",
      "Epoch 5, Step 840/1750 (48.00%), Loss: 0.0035\n",
      "Epoch 5, Step 845/1750 (48.29%), Loss: 0.0015\n",
      "Epoch 5, Step 850/1750 (48.57%), Loss: 0.0004\n",
      "Epoch 5, Step 855/1750 (48.86%), Loss: 0.0008\n",
      "Epoch 5, Step 860/1750 (49.14%), Loss: 0.0103\n",
      "Epoch 5, Step 865/1750 (49.43%), Loss: 0.0052\n",
      "Epoch 5, Step 870/1750 (49.71%), Loss: 0.0004\n",
      "Epoch 5, Step 875/1750 (50.00%), Loss: 0.0015\n",
      "Epoch 5, Step 880/1750 (50.29%), Loss: 0.0008\n",
      "Epoch 5, Step 885/1750 (50.57%), Loss: 0.0018\n",
      "Epoch 5, Step 890/1750 (50.86%), Loss: 0.0010\n",
      "Epoch 5, Step 895/1750 (51.14%), Loss: 0.0004\n",
      "Epoch 5, Step 900/1750 (51.43%), Loss: 0.0004\n",
      "Epoch 5, Step 905/1750 (51.71%), Loss: 0.0012\n",
      "Epoch 5, Step 910/1750 (52.00%), Loss: 0.0023\n",
      "Epoch 5, Step 915/1750 (52.29%), Loss: 0.0058\n",
      "Epoch 5, Step 920/1750 (52.57%), Loss: 0.0502\n",
      "Epoch 5, Step 925/1750 (52.86%), Loss: 0.0006\n",
      "Epoch 5, Step 930/1750 (53.14%), Loss: 0.0020\n",
      "Epoch 5, Step 935/1750 (53.43%), Loss: 0.0027\n",
      "Epoch 5, Step 940/1750 (53.71%), Loss: 0.0004\n",
      "Epoch 5, Step 945/1750 (54.00%), Loss: 0.0004\n",
      "Epoch 5, Step 950/1750 (54.29%), Loss: 0.0013\n",
      "Epoch 5, Step 955/1750 (54.57%), Loss: 0.0017\n",
      "Epoch 5, Step 960/1750 (54.86%), Loss: 0.0003\n",
      "Epoch 5, Step 965/1750 (55.14%), Loss: 0.0004\n",
      "Epoch 5, Step 970/1750 (55.43%), Loss: 0.0165\n",
      "Epoch 5, Step 975/1750 (55.71%), Loss: 0.0002\n",
      "Epoch 5, Step 980/1750 (56.00%), Loss: 0.0003\n",
      "Epoch 5, Step 985/1750 (56.29%), Loss: 0.0003\n",
      "Epoch 5, Step 990/1750 (56.57%), Loss: 0.0002\n",
      "Epoch 5, Step 995/1750 (56.86%), Loss: 0.0002\n",
      "Epoch 5, Step 1000/1750 (57.14%), Loss: 0.0002\n",
      "Epoch 5, Step 1005/1750 (57.43%), Loss: 0.0022\n",
      "Epoch 5, Step 1010/1750 (57.71%), Loss: 0.0002\n",
      "Epoch 5, Step 1015/1750 (58.00%), Loss: 0.0002\n",
      "Epoch 5, Step 1020/1750 (58.29%), Loss: 0.0003\n",
      "Epoch 5, Step 1025/1750 (58.57%), Loss: 0.0134\n",
      "Epoch 5, Step 1030/1750 (58.86%), Loss: 0.0003\n",
      "Epoch 5, Step 1035/1750 (59.14%), Loss: 0.1992\n",
      "Epoch 5, Step 1040/1750 (59.43%), Loss: 0.0005\n",
      "Epoch 5, Step 1045/1750 (59.71%), Loss: 0.0170\n",
      "Epoch 5, Step 1050/1750 (60.00%), Loss: 0.0003\n",
      "Epoch 5, Step 1055/1750 (60.29%), Loss: 0.0004\n",
      "Epoch 5, Step 1060/1750 (60.57%), Loss: 0.0006\n",
      "Epoch 5, Step 1065/1750 (60.86%), Loss: 0.5410\n",
      "Epoch 5, Step 1070/1750 (61.14%), Loss: 0.0008\n",
      "Epoch 5, Step 1075/1750 (61.43%), Loss: 0.0013\n",
      "Epoch 5, Step 1080/1750 (61.71%), Loss: 0.0144\n",
      "Epoch 5, Step 1085/1750 (62.00%), Loss: 0.0050\n",
      "Epoch 5, Step 1090/1750 (62.29%), Loss: 0.0006\n",
      "Epoch 5, Step 1095/1750 (62.57%), Loss: 0.0007\n",
      "Epoch 5, Step 1100/1750 (62.86%), Loss: 0.0008\n",
      "Epoch 5, Step 1105/1750 (63.14%), Loss: 0.0010\n",
      "Epoch 5, Step 1110/1750 (63.43%), Loss: 0.0219\n",
      "Epoch 5, Step 1115/1750 (63.71%), Loss: 0.1456\n",
      "Epoch 5, Step 1120/1750 (64.00%), Loss: 0.0008\n",
      "Epoch 5, Step 1125/1750 (64.29%), Loss: 0.0294\n",
      "Epoch 5, Step 1130/1750 (64.57%), Loss: 0.0025\n",
      "Epoch 5, Step 1135/1750 (64.86%), Loss: 0.0128\n",
      "Epoch 5, Step 1140/1750 (65.14%), Loss: 0.2119\n",
      "Epoch 5, Step 1145/1750 (65.43%), Loss: 0.0014\n",
      "Epoch 5, Step 1150/1750 (65.71%), Loss: 0.1194\n",
      "Epoch 5, Step 1155/1750 (66.00%), Loss: 0.1660\n",
      "Epoch 5, Step 1160/1750 (66.29%), Loss: 0.0071\n",
      "Epoch 5, Step 1165/1750 (66.57%), Loss: 0.0105\n",
      "Epoch 5, Step 1170/1750 (66.86%), Loss: 0.0690\n",
      "Epoch 5, Step 1175/1750 (67.14%), Loss: 0.1638\n",
      "Epoch 5, Step 1180/1750 (67.43%), Loss: 0.0092\n",
      "Epoch 5, Step 1185/1750 (67.71%), Loss: 0.0020\n",
      "Epoch 5, Step 1190/1750 (68.00%), Loss: 0.0029\n",
      "Epoch 5, Step 1195/1750 (68.29%), Loss: 0.0009\n",
      "Epoch 5, Step 1200/1750 (68.57%), Loss: 0.0071\n",
      "Epoch 5, Step 1205/1750 (68.86%), Loss: 0.0281\n",
      "Epoch 5, Step 1210/1750 (69.14%), Loss: 0.0007\n",
      "Epoch 5, Step 1215/1750 (69.43%), Loss: 0.0054\n",
      "Epoch 5, Step 1220/1750 (69.71%), Loss: 0.0007\n",
      "Epoch 5, Step 1225/1750 (70.00%), Loss: 0.0015\n",
      "Epoch 5, Step 1230/1750 (70.29%), Loss: 0.0007\n",
      "Epoch 5, Step 1235/1750 (70.57%), Loss: 0.0038\n",
      "Epoch 5, Step 1240/1750 (70.86%), Loss: 0.0177\n",
      "Epoch 5, Step 1245/1750 (71.14%), Loss: 0.0004\n",
      "Epoch 5, Step 1250/1750 (71.43%), Loss: 0.0322\n",
      "Epoch 5, Step 1255/1750 (71.71%), Loss: 0.4878\n",
      "Epoch 5, Step 1260/1750 (72.00%), Loss: 0.0037\n",
      "Epoch 5, Step 1265/1750 (72.29%), Loss: 0.0008\n",
      "Epoch 5, Step 1270/1750 (72.57%), Loss: 0.0028\n",
      "Epoch 5, Step 1275/1750 (72.86%), Loss: 0.0015\n",
      "Epoch 5, Step 1280/1750 (73.14%), Loss: 0.0014\n",
      "Epoch 5, Step 1285/1750 (73.43%), Loss: 0.0017\n",
      "Epoch 5, Step 1290/1750 (73.71%), Loss: 0.0058\n",
      "Epoch 5, Step 1295/1750 (74.00%), Loss: 0.0025\n",
      "Epoch 5, Step 1300/1750 (74.29%), Loss: 0.0097\n",
      "Epoch 5, Step 1305/1750 (74.57%), Loss: 0.0021\n",
      "Epoch 5, Step 1310/1750 (74.86%), Loss: 0.0017\n",
      "Epoch 5, Step 1315/1750 (75.14%), Loss: 0.0072\n",
      "Epoch 5, Step 1320/1750 (75.43%), Loss: 0.0065\n",
      "Epoch 5, Step 1325/1750 (75.71%), Loss: 0.0029\n",
      "Epoch 5, Step 1330/1750 (76.00%), Loss: 0.1135\n",
      "Epoch 5, Step 1335/1750 (76.29%), Loss: 0.0166\n",
      "Epoch 5, Step 1340/1750 (76.57%), Loss: 0.0014\n",
      "Epoch 5, Step 1345/1750 (76.86%), Loss: 0.0018\n",
      "Epoch 5, Step 1350/1750 (77.14%), Loss: 0.0020\n",
      "Epoch 5, Step 1355/1750 (77.43%), Loss: 0.0009\n",
      "Epoch 5, Step 1360/1750 (77.71%), Loss: 0.0013\n",
      "Epoch 5, Step 1365/1750 (78.00%), Loss: 0.0011\n",
      "Epoch 5, Step 1370/1750 (78.29%), Loss: 0.0009\n",
      "Epoch 5, Step 1375/1750 (78.57%), Loss: 0.0009\n",
      "Epoch 5, Step 1380/1750 (78.86%), Loss: 0.0013\n",
      "Epoch 5, Step 1385/1750 (79.14%), Loss: 0.0009\n",
      "Epoch 5, Step 1390/1750 (79.43%), Loss: 0.0008\n",
      "Epoch 5, Step 1395/1750 (79.71%), Loss: 0.0007\n",
      "Epoch 5, Step 1400/1750 (80.00%), Loss: 0.0005\n",
      "Epoch 5, Step 1405/1750 (80.29%), Loss: 0.0009\n",
      "Epoch 5, Step 1410/1750 (80.57%), Loss: 0.0009\n",
      "Epoch 5, Step 1415/1750 (80.86%), Loss: 0.0009\n",
      "Epoch 5, Step 1420/1750 (81.14%), Loss: 0.0005\n",
      "Epoch 5, Step 1425/1750 (81.43%), Loss: 0.0536\n",
      "Epoch 5, Step 1430/1750 (81.71%), Loss: 0.0008\n",
      "Epoch 5, Step 1435/1750 (82.00%), Loss: 0.0014\n",
      "Epoch 5, Step 1440/1750 (82.29%), Loss: 0.0167\n",
      "Epoch 5, Step 1445/1750 (82.57%), Loss: 0.0327\n",
      "Epoch 5, Step 1450/1750 (82.86%), Loss: 0.0007\n",
      "Epoch 5, Step 1455/1750 (83.14%), Loss: 0.0020\n",
      "Epoch 5, Step 1460/1750 (83.43%), Loss: 0.0021\n",
      "Epoch 5, Step 1465/1750 (83.71%), Loss: 0.0009\n",
      "Epoch 5, Step 1470/1750 (84.00%), Loss: 0.0015\n",
      "Epoch 5, Step 1475/1750 (84.29%), Loss: 0.0007\n",
      "Epoch 5, Step 1480/1750 (84.57%), Loss: 0.0006\n",
      "Epoch 5, Step 1485/1750 (84.86%), Loss: 0.0008\n",
      "Epoch 5, Step 1490/1750 (85.14%), Loss: 0.0007\n",
      "Epoch 5, Step 1495/1750 (85.43%), Loss: 0.0772\n",
      "Epoch 5, Step 1500/1750 (85.71%), Loss: 0.0006\n",
      "Epoch 5, Step 1505/1750 (86.00%), Loss: 0.0005\n",
      "Epoch 5, Step 1510/1750 (86.29%), Loss: 0.0017\n",
      "Epoch 5, Step 1515/1750 (86.57%), Loss: 0.0005\n",
      "Epoch 5, Step 1520/1750 (86.86%), Loss: 0.0009\n",
      "Epoch 5, Step 1525/1750 (87.14%), Loss: 0.0090\n",
      "Epoch 5, Step 1530/1750 (87.43%), Loss: 0.0378\n",
      "Epoch 5, Step 1535/1750 (87.71%), Loss: 0.0011\n",
      "Epoch 5, Step 1540/1750 (88.00%), Loss: 0.0021\n",
      "Epoch 5, Step 1545/1750 (88.29%), Loss: 0.0015\n",
      "Epoch 5, Step 1550/1750 (88.57%), Loss: 0.0197\n",
      "Epoch 5, Step 1555/1750 (88.86%), Loss: 0.0044\n",
      "Epoch 5, Step 1560/1750 (89.14%), Loss: 0.1643\n",
      "Epoch 5, Step 1565/1750 (89.43%), Loss: 0.0124\n",
      "Epoch 5, Step 1570/1750 (89.71%), Loss: 0.0023\n",
      "Epoch 5, Step 1575/1750 (90.00%), Loss: 0.0033\n",
      "Epoch 5, Step 1580/1750 (90.29%), Loss: 0.0036\n",
      "Epoch 5, Step 1585/1750 (90.57%), Loss: 0.0105\n",
      "Epoch 5, Step 1590/1750 (90.86%), Loss: 0.0023\n",
      "Epoch 5, Step 1595/1750 (91.14%), Loss: 0.0011\n",
      "Epoch 5, Step 1600/1750 (91.43%), Loss: 0.0017\n",
      "Epoch 5, Step 1605/1750 (91.71%), Loss: 0.2627\n",
      "Epoch 5, Step 1610/1750 (92.00%), Loss: 0.0049\n",
      "Epoch 5, Step 1615/1750 (92.29%), Loss: 0.0085\n",
      "Epoch 5, Step 1620/1750 (92.57%), Loss: 0.0518\n",
      "Epoch 5, Step 1625/1750 (92.86%), Loss: 0.0089\n",
      "Epoch 5, Step 1630/1750 (93.14%), Loss: 0.0124\n",
      "Epoch 5, Step 1635/1750 (93.43%), Loss: 0.0016\n",
      "Epoch 5, Step 1640/1750 (93.71%), Loss: 0.0011\n",
      "Epoch 5, Step 1645/1750 (94.00%), Loss: 0.0007\n",
      "Epoch 5, Step 1650/1750 (94.29%), Loss: 0.0007\n",
      "Epoch 5, Step 1655/1750 (94.57%), Loss: 0.0039\n",
      "Epoch 5, Step 1660/1750 (94.86%), Loss: 0.0009\n",
      "Epoch 5, Step 1665/1750 (95.14%), Loss: 0.0303\n",
      "Epoch 5, Step 1670/1750 (95.43%), Loss: 0.0013\n",
      "Epoch 5, Step 1675/1750 (95.71%), Loss: 0.0005\n",
      "Epoch 5, Step 1680/1750 (96.00%), Loss: 0.0007\n",
      "Epoch 5, Step 1685/1750 (96.29%), Loss: 0.0006\n",
      "Epoch 5, Step 1690/1750 (96.57%), Loss: 0.0213\n",
      "Epoch 5, Step 1695/1750 (96.86%), Loss: 0.0008\n",
      "Epoch 5, Step 1700/1750 (97.14%), Loss: 0.0082\n",
      "Epoch 5, Step 1705/1750 (97.43%), Loss: 0.0866\n",
      "Epoch 5, Step 1710/1750 (97.71%), Loss: 0.0006\n",
      "Epoch 5, Step 1715/1750 (98.00%), Loss: 0.0014\n",
      "Epoch 5, Step 1720/1750 (98.29%), Loss: 0.0110\n",
      "Epoch 5, Step 1725/1750 (98.57%), Loss: 0.0316\n",
      "Epoch 5, Step 1730/1750 (98.86%), Loss: 0.0024\n",
      "Epoch 5, Step 1735/1750 (99.14%), Loss: 0.0057\n",
      "Epoch 5, Step 1740/1750 (99.43%), Loss: 0.3887\n",
      "Epoch 5, Step 1745/1750 (99.71%), Loss: 0.0034\n",
      "Epoch 5, Step 1750/1750 (100.00%), Loss: 0.0032\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.15\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.03\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.03\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.50\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.59\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.12\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.45\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.72\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.26\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.42\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.00\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.01\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.08\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 0.83\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.16\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.01\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.36\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.00\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.22\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.60\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.31\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.00\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.25\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.02\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.19\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 0.58\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.22\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.32\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.12\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.01\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.04\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.47\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.06\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.37\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.28\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.27\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.29\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.09\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.21\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.26\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.26\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.24\n",
      "End of Epoch 5/10 - Average Train Loss: 0.02, Average Validation Loss: 0.26\n",
      "Starting Epoch 6/10\n",
      "Epoch 6, Step 5/1750 (0.29%), Loss: 0.1046\n",
      "Epoch 6, Step 10/1750 (0.57%), Loss: 0.0018\n",
      "Epoch 6, Step 15/1750 (0.86%), Loss: 0.0066\n",
      "Epoch 6, Step 20/1750 (1.14%), Loss: 0.0034\n",
      "Epoch 6, Step 25/1750 (1.43%), Loss: 0.0026\n",
      "Epoch 6, Step 30/1750 (1.71%), Loss: 0.0034\n",
      "Epoch 6, Step 35/1750 (2.00%), Loss: 0.0045\n",
      "Epoch 6, Step 40/1750 (2.29%), Loss: 0.0030\n",
      "Epoch 6, Step 45/1750 (2.57%), Loss: 0.0136\n",
      "Epoch 6, Step 50/1750 (2.86%), Loss: 0.1703\n",
      "Epoch 6, Step 55/1750 (3.14%), Loss: 0.0098\n",
      "Epoch 6, Step 60/1750 (3.43%), Loss: 0.0045\n",
      "Epoch 6, Step 65/1750 (3.71%), Loss: 0.0052\n",
      "Epoch 6, Step 70/1750 (4.00%), Loss: 0.0385\n",
      "Epoch 6, Step 75/1750 (4.29%), Loss: 0.0149\n",
      "Epoch 6, Step 80/1750 (4.57%), Loss: 0.0012\n",
      "Epoch 6, Step 85/1750 (4.86%), Loss: 0.0022\n",
      "Epoch 6, Step 90/1750 (5.14%), Loss: 0.0022\n",
      "Epoch 6, Step 95/1750 (5.43%), Loss: 0.0037\n",
      "Epoch 6, Step 100/1750 (5.71%), Loss: 0.0008\n",
      "Epoch 6, Step 105/1750 (6.00%), Loss: 0.0015\n",
      "Epoch 6, Step 110/1750 (6.29%), Loss: 0.0012\n",
      "Epoch 6, Step 115/1750 (6.57%), Loss: 0.0015\n",
      "Epoch 6, Step 120/1750 (6.86%), Loss: 0.0008\n",
      "Epoch 6, Step 125/1750 (7.14%), Loss: 0.0097\n",
      "Epoch 6, Step 130/1750 (7.43%), Loss: 0.0009\n",
      "Epoch 6, Step 135/1750 (7.71%), Loss: 0.0011\n",
      "Epoch 6, Step 140/1750 (8.00%), Loss: 0.0008\n",
      "Epoch 6, Step 145/1750 (8.29%), Loss: 0.0009\n",
      "Epoch 6, Step 150/1750 (8.57%), Loss: 0.0009\n",
      "Epoch 6, Step 155/1750 (8.86%), Loss: 0.0007\n",
      "Epoch 6, Step 160/1750 (9.14%), Loss: 0.0008\n",
      "Epoch 6, Step 165/1750 (9.43%), Loss: 0.0008\n",
      "Epoch 6, Step 170/1750 (9.71%), Loss: 0.0007\n",
      "Epoch 6, Step 175/1750 (10.00%), Loss: 0.0082\n",
      "Epoch 6, Step 180/1750 (10.29%), Loss: 0.0007\n",
      "Epoch 6, Step 185/1750 (10.57%), Loss: 0.0010\n",
      "Epoch 6, Step 190/1750 (10.86%), Loss: 0.0101\n",
      "Epoch 6, Step 195/1750 (11.14%), Loss: 0.1160\n",
      "Epoch 6, Step 200/1750 (11.43%), Loss: 0.0010\n",
      "Epoch 6, Step 205/1750 (11.71%), Loss: 0.0004\n",
      "Epoch 6, Step 210/1750 (12.00%), Loss: 0.0011\n",
      "Epoch 6, Step 215/1750 (12.29%), Loss: 0.0042\n",
      "Epoch 6, Step 220/1750 (12.57%), Loss: 0.0015\n",
      "Epoch 6, Step 225/1750 (12.86%), Loss: 0.0009\n",
      "Epoch 6, Step 230/1750 (13.14%), Loss: 0.0010\n",
      "Epoch 6, Step 235/1750 (13.43%), Loss: 0.0006\n",
      "Epoch 6, Step 240/1750 (13.71%), Loss: 0.0007\n",
      "Epoch 6, Step 245/1750 (14.00%), Loss: 0.0007\n",
      "Epoch 6, Step 250/1750 (14.29%), Loss: 0.0005\n",
      "Epoch 6, Step 255/1750 (14.57%), Loss: 0.0044\n",
      "Epoch 6, Step 260/1750 (14.86%), Loss: 0.0007\n",
      "Epoch 6, Step 265/1750 (15.14%), Loss: 0.0005\n",
      "Epoch 6, Step 270/1750 (15.43%), Loss: 0.0007\n",
      "Epoch 6, Step 275/1750 (15.71%), Loss: 0.0005\n",
      "Epoch 6, Step 280/1750 (16.00%), Loss: 0.0009\n",
      "Epoch 6, Step 285/1750 (16.29%), Loss: 0.0440\n",
      "Epoch 6, Step 290/1750 (16.57%), Loss: 0.0004\n",
      "Epoch 6, Step 295/1750 (16.86%), Loss: 0.0405\n",
      "Epoch 6, Step 300/1750 (17.14%), Loss: 0.0009\n",
      "Epoch 6, Step 305/1750 (17.43%), Loss: 0.0004\n",
      "Epoch 6, Step 310/1750 (17.71%), Loss: 0.0004\n",
      "Epoch 6, Step 315/1750 (18.00%), Loss: 0.0007\n",
      "Epoch 6, Step 320/1750 (18.29%), Loss: 0.0005\n",
      "Epoch 6, Step 325/1750 (18.57%), Loss: 0.0234\n",
      "Epoch 6, Step 330/1750 (18.86%), Loss: 0.0024\n",
      "Epoch 6, Step 335/1750 (19.14%), Loss: 0.0008\n",
      "Epoch 6, Step 340/1750 (19.43%), Loss: 0.0005\n",
      "Epoch 6, Step 345/1750 (19.71%), Loss: 0.0067\n",
      "Epoch 6, Step 350/1750 (20.00%), Loss: 0.0003\n",
      "Epoch 6, Step 355/1750 (20.29%), Loss: 0.0007\n",
      "Epoch 6, Step 360/1750 (20.57%), Loss: 0.0033\n",
      "Epoch 6, Step 365/1750 (20.86%), Loss: 0.0004\n",
      "Epoch 6, Step 370/1750 (21.14%), Loss: 0.0003\n",
      "Epoch 6, Step 375/1750 (21.43%), Loss: 0.0004\n",
      "Epoch 6, Step 380/1750 (21.71%), Loss: 0.0007\n",
      "Epoch 6, Step 385/1750 (22.00%), Loss: 0.0003\n",
      "Epoch 6, Step 390/1750 (22.29%), Loss: 0.0004\n",
      "Epoch 6, Step 395/1750 (22.57%), Loss: 0.0002\n",
      "Epoch 6, Step 400/1750 (22.86%), Loss: 0.0004\n",
      "Epoch 6, Step 405/1750 (23.14%), Loss: 0.0003\n",
      "Epoch 6, Step 410/1750 (23.43%), Loss: 0.0013\n",
      "Epoch 6, Step 415/1750 (23.71%), Loss: 0.0004\n",
      "Epoch 6, Step 420/1750 (24.00%), Loss: 0.0005\n",
      "Epoch 6, Step 425/1750 (24.29%), Loss: 0.0002\n",
      "Epoch 6, Step 430/1750 (24.57%), Loss: 0.0002\n",
      "Epoch 6, Step 435/1750 (24.86%), Loss: 0.0003\n",
      "Epoch 6, Step 440/1750 (25.14%), Loss: 0.0002\n",
      "Epoch 6, Step 445/1750 (25.43%), Loss: 0.0035\n",
      "Epoch 6, Step 450/1750 (25.71%), Loss: 0.0002\n",
      "Epoch 6, Step 455/1750 (26.00%), Loss: 0.0002\n",
      "Epoch 6, Step 460/1750 (26.29%), Loss: 0.0001\n",
      "Epoch 6, Step 465/1750 (26.57%), Loss: 0.0002\n",
      "Epoch 6, Step 470/1750 (26.86%), Loss: 0.0008\n",
      "Epoch 6, Step 475/1750 (27.14%), Loss: 0.0002\n",
      "Epoch 6, Step 480/1750 (27.43%), Loss: 0.0004\n",
      "Epoch 6, Step 485/1750 (27.71%), Loss: 0.0002\n",
      "Epoch 6, Step 490/1750 (28.00%), Loss: 0.0002\n",
      "Epoch 6, Step 495/1750 (28.29%), Loss: 0.0002\n",
      "Epoch 6, Step 500/1750 (28.57%), Loss: 0.0006\n",
      "Epoch 6, Step 505/1750 (28.86%), Loss: 0.0003\n",
      "Epoch 6, Step 510/1750 (29.14%), Loss: 0.0006\n",
      "Epoch 6, Step 515/1750 (29.43%), Loss: 0.0830\n",
      "Epoch 6, Step 520/1750 (29.71%), Loss: 0.0003\n",
      "Epoch 6, Step 525/1750 (30.00%), Loss: 0.2997\n",
      "Epoch 6, Step 530/1750 (30.29%), Loss: 0.0003\n",
      "Epoch 6, Step 535/1750 (30.57%), Loss: 0.0243\n",
      "Epoch 6, Step 540/1750 (30.86%), Loss: 0.0011\n",
      "Epoch 6, Step 545/1750 (31.14%), Loss: 0.0003\n",
      "Epoch 6, Step 550/1750 (31.43%), Loss: 0.0009\n",
      "Epoch 6, Step 555/1750 (31.71%), Loss: 0.0005\n",
      "Epoch 6, Step 560/1750 (32.00%), Loss: 0.0007\n",
      "Epoch 6, Step 565/1750 (32.29%), Loss: 0.0003\n",
      "Epoch 6, Step 570/1750 (32.57%), Loss: 0.0011\n",
      "Epoch 6, Step 575/1750 (32.86%), Loss: 0.0183\n",
      "Epoch 6, Step 580/1750 (33.14%), Loss: 0.0042\n",
      "Epoch 6, Step 585/1750 (33.43%), Loss: 0.0004\n",
      "Epoch 6, Step 590/1750 (33.71%), Loss: 0.0003\n",
      "Epoch 6, Step 595/1750 (34.00%), Loss: 0.0004\n",
      "Epoch 6, Step 600/1750 (34.29%), Loss: 0.0003\n",
      "Epoch 6, Step 605/1750 (34.57%), Loss: 0.0005\n",
      "Epoch 6, Step 610/1750 (34.86%), Loss: 0.0266\n",
      "Epoch 6, Step 615/1750 (35.14%), Loss: 0.0014\n",
      "Epoch 6, Step 620/1750 (35.43%), Loss: 0.0011\n",
      "Epoch 6, Step 625/1750 (35.71%), Loss: 0.1365\n",
      "Epoch 6, Step 630/1750 (36.00%), Loss: 0.0003\n",
      "Epoch 6, Step 635/1750 (36.29%), Loss: 0.0012\n",
      "Epoch 6, Step 640/1750 (36.57%), Loss: 0.0003\n",
      "Epoch 6, Step 645/1750 (36.86%), Loss: 0.0004\n",
      "Epoch 6, Step 650/1750 (37.14%), Loss: 0.0705\n",
      "Epoch 6, Step 655/1750 (37.43%), Loss: 0.0012\n",
      "Epoch 6, Step 660/1750 (37.71%), Loss: 0.0034\n",
      "Epoch 6, Step 665/1750 (38.00%), Loss: 0.0010\n",
      "Epoch 6, Step 670/1750 (38.29%), Loss: 0.0013\n",
      "Epoch 6, Step 675/1750 (38.57%), Loss: 0.0363\n",
      "Epoch 6, Step 680/1750 (38.86%), Loss: 0.1777\n",
      "Epoch 6, Step 685/1750 (39.14%), Loss: 0.0007\n",
      "Epoch 6, Step 690/1750 (39.43%), Loss: 0.0907\n",
      "Epoch 6, Step 695/1750 (39.71%), Loss: 0.0023\n",
      "Epoch 6, Step 700/1750 (40.00%), Loss: 0.2644\n",
      "Epoch 6, Step 705/1750 (40.29%), Loss: 0.0013\n",
      "Epoch 6, Step 710/1750 (40.57%), Loss: 0.0080\n",
      "Epoch 6, Step 715/1750 (40.86%), Loss: 0.0104\n",
      "Epoch 6, Step 720/1750 (41.14%), Loss: 0.0056\n",
      "Epoch 6, Step 725/1750 (41.43%), Loss: 0.0194\n",
      "Epoch 6, Step 730/1750 (41.71%), Loss: 0.0008\n",
      "Epoch 6, Step 735/1750 (42.00%), Loss: 0.0234\n",
      "Epoch 6, Step 740/1750 (42.29%), Loss: 0.0102\n",
      "Epoch 6, Step 745/1750 (42.57%), Loss: 0.0212\n",
      "Epoch 6, Step 750/1750 (42.86%), Loss: 0.0008\n",
      "Epoch 6, Step 755/1750 (43.14%), Loss: 0.0176\n",
      "Epoch 6, Step 760/1750 (43.43%), Loss: 0.0090\n",
      "Epoch 6, Step 765/1750 (43.71%), Loss: 0.0015\n",
      "Epoch 6, Step 770/1750 (44.00%), Loss: 0.0004\n",
      "Epoch 6, Step 775/1750 (44.29%), Loss: 0.0013\n",
      "Epoch 6, Step 780/1750 (44.57%), Loss: 0.0033\n",
      "Epoch 6, Step 785/1750 (44.86%), Loss: 0.0030\n",
      "Epoch 6, Step 790/1750 (45.14%), Loss: 0.0008\n",
      "Epoch 6, Step 795/1750 (45.43%), Loss: 0.0028\n",
      "Epoch 6, Step 800/1750 (45.71%), Loss: 0.0068\n",
      "Epoch 6, Step 805/1750 (46.00%), Loss: 0.0004\n",
      "Epoch 6, Step 810/1750 (46.29%), Loss: 0.0009\n",
      "Epoch 6, Step 815/1750 (46.57%), Loss: 0.0026\n",
      "Epoch 6, Step 820/1750 (46.86%), Loss: 0.0080\n",
      "Epoch 6, Step 825/1750 (47.14%), Loss: 0.0008\n",
      "Epoch 6, Step 830/1750 (47.43%), Loss: 0.0007\n",
      "Epoch 6, Step 835/1750 (47.71%), Loss: 0.0006\n",
      "Epoch 6, Step 840/1750 (48.00%), Loss: 0.0012\n",
      "Epoch 6, Step 845/1750 (48.29%), Loss: 0.0489\n",
      "Epoch 6, Step 850/1750 (48.57%), Loss: 0.0005\n",
      "Epoch 6, Step 855/1750 (48.86%), Loss: 0.0037\n",
      "Epoch 6, Step 860/1750 (49.14%), Loss: 0.0010\n",
      "Epoch 6, Step 865/1750 (49.43%), Loss: 0.0012\n",
      "Epoch 6, Step 870/1750 (49.71%), Loss: 0.0004\n",
      "Epoch 6, Step 875/1750 (50.00%), Loss: 0.0006\n",
      "Epoch 6, Step 880/1750 (50.29%), Loss: 0.0004\n",
      "Epoch 6, Step 885/1750 (50.57%), Loss: 0.1180\n",
      "Epoch 6, Step 890/1750 (50.86%), Loss: 0.0006\n",
      "Epoch 6, Step 895/1750 (51.14%), Loss: 0.0005\n",
      "Epoch 6, Step 900/1750 (51.43%), Loss: 0.0003\n",
      "Epoch 6, Step 905/1750 (51.71%), Loss: 0.0004\n",
      "Epoch 6, Step 910/1750 (52.00%), Loss: 0.0004\n",
      "Epoch 6, Step 915/1750 (52.29%), Loss: 0.0111\n",
      "Epoch 6, Step 920/1750 (52.57%), Loss: 0.0007\n",
      "Epoch 6, Step 925/1750 (52.86%), Loss: 0.0014\n",
      "Epoch 6, Step 930/1750 (53.14%), Loss: 0.0899\n",
      "Epoch 6, Step 935/1750 (53.43%), Loss: 0.0003\n",
      "Epoch 6, Step 940/1750 (53.71%), Loss: 0.0012\n",
      "Epoch 6, Step 945/1750 (54.00%), Loss: 0.2305\n",
      "Epoch 6, Step 950/1750 (54.29%), Loss: 0.0009\n",
      "Epoch 6, Step 955/1750 (54.57%), Loss: 0.0033\n",
      "Epoch 6, Step 960/1750 (54.86%), Loss: 0.0211\n",
      "Epoch 6, Step 965/1750 (55.14%), Loss: 0.0696\n",
      "Epoch 6, Step 970/1750 (55.43%), Loss: 0.0003\n",
      "Epoch 6, Step 975/1750 (55.71%), Loss: 0.0110\n",
      "Epoch 6, Step 980/1750 (56.00%), Loss: 0.0002\n",
      "Epoch 6, Step 985/1750 (56.29%), Loss: 0.0006\n",
      "Epoch 6, Step 990/1750 (56.57%), Loss: 0.0012\n",
      "Epoch 6, Step 995/1750 (56.86%), Loss: 0.0007\n",
      "Epoch 6, Step 1000/1750 (57.14%), Loss: 0.0003\n",
      "Epoch 6, Step 1005/1750 (57.43%), Loss: 0.0002\n",
      "Epoch 6, Step 1010/1750 (57.71%), Loss: 0.0003\n",
      "Epoch 6, Step 1015/1750 (58.00%), Loss: 0.0086\n",
      "Epoch 6, Step 1020/1750 (58.29%), Loss: 0.0008\n",
      "Epoch 6, Step 1025/1750 (58.57%), Loss: 0.0005\n",
      "Epoch 6, Step 1030/1750 (58.86%), Loss: 0.0033\n",
      "Epoch 6, Step 1035/1750 (59.14%), Loss: 0.0013\n",
      "Epoch 6, Step 1040/1750 (59.43%), Loss: 0.0014\n",
      "Epoch 6, Step 1045/1750 (59.71%), Loss: 0.0007\n",
      "Epoch 6, Step 1050/1750 (60.00%), Loss: 0.0016\n",
      "Epoch 6, Step 1055/1750 (60.29%), Loss: 0.0080\n",
      "Epoch 6, Step 1060/1750 (60.57%), Loss: 0.0011\n",
      "Epoch 6, Step 1065/1750 (60.86%), Loss: 0.0012\n",
      "Epoch 6, Step 1070/1750 (61.14%), Loss: 0.0017\n",
      "Epoch 6, Step 1075/1750 (61.43%), Loss: 0.0008\n",
      "Epoch 6, Step 1080/1750 (61.71%), Loss: 0.0020\n",
      "Epoch 6, Step 1085/1750 (62.00%), Loss: 0.0017\n",
      "Epoch 6, Step 1090/1750 (62.29%), Loss: 0.0013\n",
      "Epoch 6, Step 1095/1750 (62.57%), Loss: 0.0461\n",
      "Epoch 6, Step 1100/1750 (62.86%), Loss: 0.0078\n",
      "Epoch 6, Step 1105/1750 (63.14%), Loss: 0.0010\n",
      "Epoch 6, Step 1110/1750 (63.43%), Loss: 0.0010\n",
      "Epoch 6, Step 1115/1750 (63.71%), Loss: 0.0060\n",
      "Epoch 6, Step 1120/1750 (64.00%), Loss: 0.0048\n",
      "Epoch 6, Step 1125/1750 (64.29%), Loss: 0.0014\n",
      "Epoch 6, Step 1130/1750 (64.57%), Loss: 0.0012\n",
      "Epoch 6, Step 1135/1750 (64.86%), Loss: 0.0027\n",
      "Epoch 6, Step 1140/1750 (65.14%), Loss: 0.0055\n",
      "Epoch 6, Step 1145/1750 (65.43%), Loss: 0.0032\n",
      "Epoch 6, Step 1150/1750 (65.71%), Loss: 0.0027\n",
      "Epoch 6, Step 1155/1750 (66.00%), Loss: 0.0022\n",
      "Epoch 6, Step 1160/1750 (66.29%), Loss: 0.0016\n",
      "Epoch 6, Step 1165/1750 (66.57%), Loss: 0.0035\n",
      "Epoch 6, Step 1170/1750 (66.86%), Loss: 0.0010\n",
      "Epoch 6, Step 1175/1750 (67.14%), Loss: 0.0014\n",
      "Epoch 6, Step 1180/1750 (67.43%), Loss: 0.0015\n",
      "Epoch 6, Step 1185/1750 (67.71%), Loss: 0.0018\n",
      "Epoch 6, Step 1190/1750 (68.00%), Loss: 0.0535\n",
      "Epoch 6, Step 1195/1750 (68.29%), Loss: 0.0025\n",
      "Epoch 6, Step 1200/1750 (68.57%), Loss: 0.0013\n",
      "Epoch 6, Step 1205/1750 (68.86%), Loss: 0.0009\n",
      "Epoch 6, Step 1210/1750 (69.14%), Loss: 0.0009\n",
      "Epoch 6, Step 1215/1750 (69.43%), Loss: 0.0603\n",
      "Epoch 6, Step 1220/1750 (69.71%), Loss: 0.0009\n",
      "Epoch 6, Step 1225/1750 (70.00%), Loss: 0.0009\n",
      "Epoch 6, Step 1230/1750 (70.29%), Loss: 0.0305\n",
      "Epoch 6, Step 1235/1750 (70.57%), Loss: 0.0015\n",
      "Epoch 6, Step 1240/1750 (70.86%), Loss: 0.0008\n",
      "Epoch 6, Step 1245/1750 (71.14%), Loss: 0.0075\n",
      "Epoch 6, Step 1250/1750 (71.43%), Loss: 0.0005\n",
      "Epoch 6, Step 1255/1750 (71.71%), Loss: 0.0007\n",
      "Epoch 6, Step 1260/1750 (72.00%), Loss: 0.0007\n",
      "Epoch 6, Step 1265/1750 (72.29%), Loss: 0.0007\n",
      "Epoch 6, Step 1270/1750 (72.57%), Loss: 0.0008\n",
      "Epoch 6, Step 1275/1750 (72.86%), Loss: 0.0004\n",
      "Epoch 6, Step 1280/1750 (73.14%), Loss: 0.0005\n",
      "Epoch 6, Step 1285/1750 (73.43%), Loss: 0.0011\n",
      "Epoch 6, Step 1290/1750 (73.71%), Loss: 0.0003\n",
      "Epoch 6, Step 1295/1750 (74.00%), Loss: 0.0137\n",
      "Epoch 6, Step 1300/1750 (74.29%), Loss: 0.0006\n",
      "Epoch 6, Step 1305/1750 (74.57%), Loss: 0.0005\n",
      "Epoch 6, Step 1310/1750 (74.86%), Loss: 0.0010\n",
      "Epoch 6, Step 1315/1750 (75.14%), Loss: 0.0017\n",
      "Epoch 6, Step 1320/1750 (75.43%), Loss: 0.0018\n",
      "Epoch 6, Step 1325/1750 (75.71%), Loss: 0.0021\n",
      "Epoch 6, Step 1330/1750 (76.00%), Loss: 0.0018\n",
      "Epoch 6, Step 1335/1750 (76.29%), Loss: 0.0014\n",
      "Epoch 6, Step 1340/1750 (76.57%), Loss: 0.0013\n",
      "Epoch 6, Step 1345/1750 (76.86%), Loss: 0.0010\n",
      "Epoch 6, Step 1350/1750 (77.14%), Loss: 0.0014\n",
      "Epoch 6, Step 1355/1750 (77.43%), Loss: 0.0679\n",
      "Epoch 6, Step 1360/1750 (77.71%), Loss: 0.0011\n",
      "Epoch 6, Step 1365/1750 (78.00%), Loss: 0.0012\n",
      "Epoch 6, Step 1370/1750 (78.29%), Loss: 0.0007\n",
      "Epoch 6, Step 1375/1750 (78.57%), Loss: 0.0006\n",
      "Epoch 6, Step 1380/1750 (78.86%), Loss: 0.0009\n",
      "Epoch 6, Step 1385/1750 (79.14%), Loss: 0.0009\n",
      "Epoch 6, Step 1390/1750 (79.43%), Loss: 0.0007\n",
      "Epoch 6, Step 1395/1750 (79.71%), Loss: 0.0007\n",
      "Epoch 6, Step 1400/1750 (80.00%), Loss: 0.0009\n",
      "Epoch 6, Step 1405/1750 (80.29%), Loss: 0.0004\n",
      "Epoch 6, Step 1410/1750 (80.57%), Loss: 0.0008\n",
      "Epoch 6, Step 1415/1750 (80.86%), Loss: 0.0004\n",
      "Epoch 6, Step 1420/1750 (81.14%), Loss: 0.0004\n",
      "Epoch 6, Step 1425/1750 (81.43%), Loss: 0.0006\n",
      "Epoch 6, Step 1430/1750 (81.71%), Loss: 0.0008\n",
      "Epoch 6, Step 1435/1750 (82.00%), Loss: 0.0014\n",
      "Epoch 6, Step 1440/1750 (82.29%), Loss: 0.0006\n",
      "Epoch 6, Step 1445/1750 (82.57%), Loss: 0.0005\n",
      "Epoch 6, Step 1450/1750 (82.86%), Loss: 0.0003\n",
      "Epoch 6, Step 1455/1750 (83.14%), Loss: 0.0699\n",
      "Epoch 6, Step 1460/1750 (83.43%), Loss: 0.0021\n",
      "Epoch 6, Step 1465/1750 (83.71%), Loss: 0.0005\n",
      "Epoch 6, Step 1470/1750 (84.00%), Loss: 0.0004\n",
      "Epoch 6, Step 1475/1750 (84.29%), Loss: 0.0007\n",
      "Epoch 6, Step 1480/1750 (84.57%), Loss: 0.0004\n",
      "Epoch 6, Step 1485/1750 (84.86%), Loss: 0.1059\n",
      "Epoch 6, Step 1490/1750 (85.14%), Loss: 0.0029\n",
      "Epoch 6, Step 1495/1750 (85.43%), Loss: 0.0014\n",
      "Epoch 6, Step 1500/1750 (85.71%), Loss: 0.0226\n",
      "Epoch 6, Step 1505/1750 (86.00%), Loss: 0.0316\n",
      "Epoch 6, Step 1510/1750 (86.29%), Loss: 0.0857\n",
      "Epoch 6, Step 1515/1750 (86.57%), Loss: 0.1702\n",
      "Epoch 6, Step 1520/1750 (86.86%), Loss: 0.0026\n",
      "Epoch 6, Step 1525/1750 (87.14%), Loss: 0.0006\n",
      "Epoch 6, Step 1530/1750 (87.43%), Loss: 0.0006\n",
      "Epoch 6, Step 1535/1750 (87.71%), Loss: 0.0005\n",
      "Epoch 6, Step 1540/1750 (88.00%), Loss: 0.0016\n",
      "Epoch 6, Step 1545/1750 (88.29%), Loss: 0.0006\n",
      "Epoch 6, Step 1550/1750 (88.57%), Loss: 0.0004\n",
      "Epoch 6, Step 1555/1750 (88.86%), Loss: 0.0005\n",
      "Epoch 6, Step 1560/1750 (89.14%), Loss: 0.0006\n",
      "Epoch 6, Step 1565/1750 (89.43%), Loss: 0.0005\n",
      "Epoch 6, Step 1570/1750 (89.71%), Loss: 0.0278\n",
      "Epoch 6, Step 1575/1750 (90.00%), Loss: 0.0019\n",
      "Epoch 6, Step 1580/1750 (90.29%), Loss: 0.0013\n",
      "Epoch 6, Step 1585/1750 (90.57%), Loss: 0.0004\n",
      "Epoch 6, Step 1590/1750 (90.86%), Loss: 0.0009\n",
      "Epoch 6, Step 1595/1750 (91.14%), Loss: 0.0070\n",
      "Epoch 6, Step 1600/1750 (91.43%), Loss: 0.0075\n",
      "Epoch 6, Step 1605/1750 (91.71%), Loss: 0.0004\n",
      "Epoch 6, Step 1610/1750 (92.00%), Loss: 0.0007\n",
      "Epoch 6, Step 1615/1750 (92.29%), Loss: 0.0537\n",
      "Epoch 6, Step 1620/1750 (92.57%), Loss: 0.0053\n",
      "Epoch 6, Step 1625/1750 (92.86%), Loss: 0.0018\n",
      "Epoch 6, Step 1630/1750 (93.14%), Loss: 0.0164\n",
      "Epoch 6, Step 1635/1750 (93.43%), Loss: 0.0018\n",
      "Epoch 6, Step 1640/1750 (93.71%), Loss: 0.0006\n",
      "Epoch 6, Step 1645/1750 (94.00%), Loss: 0.0009\n",
      "Epoch 6, Step 1650/1750 (94.29%), Loss: 0.0082\n",
      "Epoch 6, Step 1655/1750 (94.57%), Loss: 0.0003\n",
      "Epoch 6, Step 1660/1750 (94.86%), Loss: 0.0004\n",
      "Epoch 6, Step 1665/1750 (95.14%), Loss: 0.0009\n",
      "Epoch 6, Step 1670/1750 (95.43%), Loss: 0.0005\n",
      "Epoch 6, Step 1675/1750 (95.71%), Loss: 0.0006\n",
      "Epoch 6, Step 1680/1750 (96.00%), Loss: 0.0004\n",
      "Epoch 6, Step 1685/1750 (96.29%), Loss: 0.0003\n",
      "Epoch 6, Step 1690/1750 (96.57%), Loss: 0.0004\n",
      "Epoch 6, Step 1695/1750 (96.86%), Loss: 0.0005\n",
      "Epoch 6, Step 1700/1750 (97.14%), Loss: 0.0003\n",
      "Epoch 6, Step 1705/1750 (97.43%), Loss: 0.0007\n",
      "Epoch 6, Step 1710/1750 (97.71%), Loss: 0.0007\n",
      "Epoch 6, Step 1715/1750 (98.00%), Loss: 0.0049\n",
      "Epoch 6, Step 1720/1750 (98.29%), Loss: 0.0003\n",
      "Epoch 6, Step 1725/1750 (98.57%), Loss: 0.0003\n",
      "Epoch 6, Step 1730/1750 (98.86%), Loss: 0.0003\n",
      "Epoch 6, Step 1735/1750 (99.14%), Loss: 0.0003\n",
      "Epoch 6, Step 1740/1750 (99.43%), Loss: 0.0004\n",
      "Epoch 6, Step 1745/1750 (99.71%), Loss: 0.0003\n",
      "Epoch 6, Step 1750/1750 (100.00%), Loss: 0.0003\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.29\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.00\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.00\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 1.11\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.96\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.43\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 1.12\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.71\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.26\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.68\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.00\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.00\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.12\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 0.95\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.31\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.00\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.47\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.00\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.74\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 1.28\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 1.01\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.00\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.55\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.00\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.47\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 0.71\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.45\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.65\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.00\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.00\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.48\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.40\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.02\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.48\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.74\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.15\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.56\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.61\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.56\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.33\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.99\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.48\n",
      "End of Epoch 6/10 - Average Train Loss: 0.01, Average Validation Loss: 0.43\n",
      "Starting Epoch 7/10\n",
      "Epoch 7, Step 5/1750 (0.29%), Loss: 0.0252\n",
      "Epoch 7, Step 10/1750 (0.57%), Loss: 0.0004\n",
      "Epoch 7, Step 15/1750 (0.86%), Loss: 0.0003\n",
      "Epoch 7, Step 20/1750 (1.14%), Loss: 0.0005\n",
      "Epoch 7, Step 25/1750 (1.43%), Loss: 0.0004\n",
      "Epoch 7, Step 30/1750 (1.71%), Loss: 0.0003\n",
      "Epoch 7, Step 35/1750 (2.00%), Loss: 0.0006\n",
      "Epoch 7, Step 40/1750 (2.29%), Loss: 0.0005\n",
      "Epoch 7, Step 45/1750 (2.57%), Loss: 0.0004\n",
      "Epoch 7, Step 50/1750 (2.86%), Loss: 0.0016\n",
      "Epoch 7, Step 55/1750 (3.14%), Loss: 0.0003\n",
      "Epoch 7, Step 60/1750 (3.43%), Loss: 0.0004\n",
      "Epoch 7, Step 65/1750 (3.71%), Loss: 0.0004\n",
      "Epoch 7, Step 70/1750 (4.00%), Loss: 0.0005\n",
      "Epoch 7, Step 75/1750 (4.29%), Loss: 0.0003\n",
      "Epoch 7, Step 80/1750 (4.57%), Loss: 0.0003\n",
      "Epoch 7, Step 85/1750 (4.86%), Loss: 0.0044\n",
      "Epoch 7, Step 90/1750 (5.14%), Loss: 0.0157\n",
      "Epoch 7, Step 95/1750 (5.43%), Loss: 0.0029\n",
      "Epoch 7, Step 100/1750 (5.71%), Loss: 0.0004\n",
      "Epoch 7, Step 105/1750 (6.00%), Loss: 0.0006\n",
      "Epoch 7, Step 110/1750 (6.29%), Loss: 0.0037\n",
      "Epoch 7, Step 115/1750 (6.57%), Loss: 0.0004\n",
      "Epoch 7, Step 120/1750 (6.86%), Loss: 0.0007\n",
      "Epoch 7, Step 125/1750 (7.14%), Loss: 0.0005\n",
      "Epoch 7, Step 130/1750 (7.43%), Loss: 0.0003\n",
      "Epoch 7, Step 135/1750 (7.71%), Loss: 0.0002\n",
      "Epoch 7, Step 140/1750 (8.00%), Loss: 0.0003\n",
      "Epoch 7, Step 145/1750 (8.29%), Loss: 0.0003\n",
      "Epoch 7, Step 150/1750 (8.57%), Loss: 0.4113\n",
      "Epoch 7, Step 155/1750 (8.86%), Loss: 0.0003\n",
      "Epoch 7, Step 160/1750 (9.14%), Loss: 0.0011\n",
      "Epoch 7, Step 165/1750 (9.43%), Loss: 0.0004\n",
      "Epoch 7, Step 170/1750 (9.71%), Loss: 0.0007\n",
      "Epoch 7, Step 175/1750 (10.00%), Loss: 0.0003\n",
      "Epoch 7, Step 180/1750 (10.29%), Loss: 0.0002\n",
      "Epoch 7, Step 185/1750 (10.57%), Loss: 0.0007\n",
      "Epoch 7, Step 190/1750 (10.86%), Loss: 0.0005\n",
      "Epoch 7, Step 195/1750 (11.14%), Loss: 0.0006\n",
      "Epoch 7, Step 200/1750 (11.43%), Loss: 0.0044\n",
      "Epoch 7, Step 205/1750 (11.71%), Loss: 0.0005\n",
      "Epoch 7, Step 210/1750 (12.00%), Loss: 0.0696\n",
      "Epoch 7, Step 215/1750 (12.29%), Loss: 0.0006\n",
      "Epoch 7, Step 220/1750 (12.57%), Loss: 0.0005\n",
      "Epoch 7, Step 225/1750 (12.86%), Loss: 0.0009\n",
      "Epoch 7, Step 230/1750 (13.14%), Loss: 0.0008\n",
      "Epoch 7, Step 235/1750 (13.43%), Loss: 0.0005\n",
      "Epoch 7, Step 240/1750 (13.71%), Loss: 0.0008\n",
      "Epoch 7, Step 245/1750 (14.00%), Loss: 0.0031\n",
      "Epoch 7, Step 250/1750 (14.29%), Loss: 0.0014\n",
      "Epoch 7, Step 255/1750 (14.57%), Loss: 0.0009\n",
      "Epoch 7, Step 260/1750 (14.86%), Loss: 0.0006\n",
      "Epoch 7, Step 265/1750 (15.14%), Loss: 0.0046\n",
      "Epoch 7, Step 270/1750 (15.43%), Loss: 0.0159\n",
      "Epoch 7, Step 275/1750 (15.71%), Loss: 0.0021\n",
      "Epoch 7, Step 280/1750 (16.00%), Loss: 0.0002\n",
      "Epoch 7, Step 285/1750 (16.29%), Loss: 0.0005\n",
      "Epoch 7, Step 290/1750 (16.57%), Loss: 0.0011\n",
      "Epoch 7, Step 295/1750 (16.86%), Loss: 0.0006\n",
      "Epoch 7, Step 300/1750 (17.14%), Loss: 0.0010\n",
      "Epoch 7, Step 305/1750 (17.43%), Loss: 0.0015\n",
      "Epoch 7, Step 310/1750 (17.71%), Loss: 0.0018\n",
      "Epoch 7, Step 315/1750 (18.00%), Loss: 0.0008\n",
      "Epoch 7, Step 320/1750 (18.29%), Loss: 0.0011\n",
      "Epoch 7, Step 325/1750 (18.57%), Loss: 0.0006\n",
      "Epoch 7, Step 330/1750 (18.86%), Loss: 0.0003\n",
      "Epoch 7, Step 335/1750 (19.14%), Loss: 0.0007\n",
      "Epoch 7, Step 340/1750 (19.43%), Loss: 0.0003\n",
      "Epoch 7, Step 345/1750 (19.71%), Loss: 0.1543\n",
      "Epoch 7, Step 350/1750 (20.00%), Loss: 0.0006\n",
      "Epoch 7, Step 355/1750 (20.29%), Loss: 0.0005\n",
      "Epoch 7, Step 360/1750 (20.57%), Loss: 0.1221\n",
      "Epoch 7, Step 365/1750 (20.86%), Loss: 0.0007\n",
      "Epoch 7, Step 370/1750 (21.14%), Loss: 0.0024\n",
      "Epoch 7, Step 375/1750 (21.43%), Loss: 0.0176\n",
      "Epoch 7, Step 380/1750 (21.71%), Loss: 0.0008\n",
      "Epoch 7, Step 385/1750 (22.00%), Loss: 0.0009\n",
      "Epoch 7, Step 390/1750 (22.29%), Loss: 0.0009\n",
      "Epoch 7, Step 395/1750 (22.57%), Loss: 0.0009\n",
      "Epoch 7, Step 400/1750 (22.86%), Loss: 0.0399\n",
      "Epoch 7, Step 405/1750 (23.14%), Loss: 0.0149\n",
      "Epoch 7, Step 410/1750 (23.43%), Loss: 0.0466\n",
      "Epoch 7, Step 415/1750 (23.71%), Loss: 0.0035\n",
      "Epoch 7, Step 420/1750 (24.00%), Loss: 0.0013\n",
      "Epoch 7, Step 425/1750 (24.29%), Loss: 0.0167\n",
      "Epoch 7, Step 430/1750 (24.57%), Loss: 0.0006\n",
      "Epoch 7, Step 435/1750 (24.86%), Loss: 0.2231\n",
      "Epoch 7, Step 440/1750 (25.14%), Loss: 0.0008\n",
      "Epoch 7, Step 445/1750 (25.43%), Loss: 0.0013\n",
      "Epoch 7, Step 450/1750 (25.71%), Loss: 0.0003\n",
      "Epoch 7, Step 455/1750 (26.00%), Loss: 0.0011\n",
      "Epoch 7, Step 460/1750 (26.29%), Loss: 0.0257\n",
      "Epoch 7, Step 465/1750 (26.57%), Loss: 0.0007\n",
      "Epoch 7, Step 470/1750 (26.86%), Loss: 0.0012\n",
      "Epoch 7, Step 475/1750 (27.14%), Loss: 0.0034\n",
      "Epoch 7, Step 480/1750 (27.43%), Loss: 0.0008\n",
      "Epoch 7, Step 485/1750 (27.71%), Loss: 0.0005\n",
      "Epoch 7, Step 490/1750 (28.00%), Loss: 0.0004\n",
      "Epoch 7, Step 495/1750 (28.29%), Loss: 0.0004\n",
      "Epoch 7, Step 500/1750 (28.57%), Loss: 0.0004\n",
      "Epoch 7, Step 505/1750 (28.86%), Loss: 0.0003\n",
      "Epoch 7, Step 510/1750 (29.14%), Loss: 0.0116\n",
      "Epoch 7, Step 515/1750 (29.43%), Loss: 0.0003\n",
      "Epoch 7, Step 520/1750 (29.71%), Loss: 0.0005\n",
      "Epoch 7, Step 525/1750 (30.00%), Loss: 0.0005\n",
      "Epoch 7, Step 530/1750 (30.29%), Loss: 0.0003\n",
      "Epoch 7, Step 535/1750 (30.57%), Loss: 0.0006\n",
      "Epoch 7, Step 540/1750 (30.86%), Loss: 0.0007\n",
      "Epoch 7, Step 545/1750 (31.14%), Loss: 0.0005\n",
      "Epoch 7, Step 550/1750 (31.43%), Loss: 0.0004\n",
      "Epoch 7, Step 555/1750 (31.71%), Loss: 0.0006\n",
      "Epoch 7, Step 560/1750 (32.00%), Loss: 0.0009\n",
      "Epoch 7, Step 565/1750 (32.29%), Loss: 0.0003\n",
      "Epoch 7, Step 570/1750 (32.57%), Loss: 0.0006\n",
      "Epoch 7, Step 575/1750 (32.86%), Loss: 0.0006\n",
      "Epoch 7, Step 580/1750 (33.14%), Loss: 0.0034\n",
      "Epoch 7, Step 585/1750 (33.43%), Loss: 0.0005\n",
      "Epoch 7, Step 590/1750 (33.71%), Loss: 0.0008\n",
      "Epoch 7, Step 595/1750 (34.00%), Loss: 0.0004\n",
      "Epoch 7, Step 600/1750 (34.29%), Loss: 0.0009\n",
      "Epoch 7, Step 605/1750 (34.57%), Loss: 0.0006\n",
      "Epoch 7, Step 610/1750 (34.86%), Loss: 0.0008\n",
      "Epoch 7, Step 615/1750 (35.14%), Loss: 0.0005\n",
      "Epoch 7, Step 620/1750 (35.43%), Loss: 0.0007\n",
      "Epoch 7, Step 625/1750 (35.71%), Loss: 0.0008\n",
      "Epoch 7, Step 630/1750 (36.00%), Loss: 0.0010\n",
      "Epoch 7, Step 635/1750 (36.29%), Loss: 0.0007\n",
      "Epoch 7, Step 640/1750 (36.57%), Loss: 0.0005\n",
      "Epoch 7, Step 645/1750 (36.86%), Loss: 0.0005\n",
      "Epoch 7, Step 650/1750 (37.14%), Loss: 0.0005\n",
      "Epoch 7, Step 655/1750 (37.43%), Loss: 0.0006\n",
      "Epoch 7, Step 660/1750 (37.71%), Loss: 0.0010\n",
      "Epoch 7, Step 665/1750 (38.00%), Loss: 0.0007\n",
      "Epoch 7, Step 670/1750 (38.29%), Loss: 0.0033\n",
      "Epoch 7, Step 675/1750 (38.57%), Loss: 0.0006\n",
      "Epoch 7, Step 680/1750 (38.86%), Loss: 0.0005\n",
      "Epoch 7, Step 685/1750 (39.14%), Loss: 0.0007\n",
      "Epoch 7, Step 690/1750 (39.43%), Loss: 0.0004\n",
      "Epoch 7, Step 695/1750 (39.71%), Loss: 0.0050\n",
      "Epoch 7, Step 700/1750 (40.00%), Loss: 0.0006\n",
      "Epoch 7, Step 705/1750 (40.29%), Loss: 0.0003\n",
      "Epoch 7, Step 710/1750 (40.57%), Loss: 0.0006\n",
      "Epoch 7, Step 715/1750 (40.86%), Loss: 0.0003\n",
      "Epoch 7, Step 720/1750 (41.14%), Loss: 0.0004\n",
      "Epoch 7, Step 725/1750 (41.43%), Loss: 0.0003\n",
      "Epoch 7, Step 730/1750 (41.71%), Loss: 0.0002\n",
      "Epoch 7, Step 735/1750 (42.00%), Loss: 0.0003\n",
      "Epoch 7, Step 740/1750 (42.29%), Loss: 0.0002\n",
      "Epoch 7, Step 745/1750 (42.57%), Loss: 0.0003\n",
      "Epoch 7, Step 750/1750 (42.86%), Loss: 0.0150\n",
      "Epoch 7, Step 755/1750 (43.14%), Loss: 0.0003\n",
      "Epoch 7, Step 760/1750 (43.43%), Loss: 0.0003\n",
      "Epoch 7, Step 765/1750 (43.71%), Loss: 0.0002\n",
      "Epoch 7, Step 770/1750 (44.00%), Loss: 0.0004\n",
      "Epoch 7, Step 775/1750 (44.29%), Loss: 0.0003\n",
      "Epoch 7, Step 780/1750 (44.57%), Loss: 0.0003\n",
      "Epoch 7, Step 785/1750 (44.86%), Loss: 0.0004\n",
      "Epoch 7, Step 790/1750 (45.14%), Loss: 0.0004\n",
      "Epoch 7, Step 795/1750 (45.43%), Loss: 0.0004\n",
      "Epoch 7, Step 800/1750 (45.71%), Loss: 0.0002\n",
      "Epoch 7, Step 805/1750 (46.00%), Loss: 0.0003\n",
      "Epoch 7, Step 810/1750 (46.29%), Loss: 0.0002\n",
      "Epoch 7, Step 815/1750 (46.57%), Loss: 0.0004\n",
      "Epoch 7, Step 820/1750 (46.86%), Loss: 0.0078\n",
      "Epoch 7, Step 825/1750 (47.14%), Loss: 0.0003\n",
      "Epoch 7, Step 830/1750 (47.43%), Loss: 0.0002\n",
      "Epoch 7, Step 835/1750 (47.71%), Loss: 0.0003\n",
      "Epoch 7, Step 840/1750 (48.00%), Loss: 0.0002\n",
      "Epoch 7, Step 845/1750 (48.29%), Loss: 0.0003\n",
      "Epoch 7, Step 850/1750 (48.57%), Loss: 0.0008\n",
      "Epoch 7, Step 855/1750 (48.86%), Loss: 0.0003\n",
      "Epoch 7, Step 860/1750 (49.14%), Loss: 0.0003\n",
      "Epoch 7, Step 865/1750 (49.43%), Loss: 0.0009\n",
      "Epoch 7, Step 870/1750 (49.71%), Loss: 0.0002\n",
      "Epoch 7, Step 875/1750 (50.00%), Loss: 0.0001\n",
      "Epoch 7, Step 880/1750 (50.29%), Loss: 0.0002\n",
      "Epoch 7, Step 885/1750 (50.57%), Loss: 0.0003\n",
      "Epoch 7, Step 890/1750 (50.86%), Loss: 0.0002\n",
      "Epoch 7, Step 895/1750 (51.14%), Loss: 0.0005\n",
      "Epoch 7, Step 900/1750 (51.43%), Loss: 0.0002\n",
      "Epoch 7, Step 905/1750 (51.71%), Loss: 0.0003\n",
      "Epoch 7, Step 910/1750 (52.00%), Loss: 0.0003\n",
      "Epoch 7, Step 915/1750 (52.29%), Loss: 0.0003\n",
      "Epoch 7, Step 920/1750 (52.57%), Loss: 0.0004\n",
      "Epoch 7, Step 925/1750 (52.86%), Loss: 0.0004\n",
      "Epoch 7, Step 930/1750 (53.14%), Loss: 0.0003\n",
      "Epoch 7, Step 935/1750 (53.43%), Loss: 0.0018\n",
      "Epoch 7, Step 940/1750 (53.71%), Loss: 0.0274\n",
      "Epoch 7, Step 945/1750 (54.00%), Loss: 0.0004\n",
      "Epoch 7, Step 950/1750 (54.29%), Loss: 0.0003\n",
      "Epoch 7, Step 955/1750 (54.57%), Loss: 0.0009\n",
      "Epoch 7, Step 960/1750 (54.86%), Loss: 0.0003\n",
      "Epoch 7, Step 965/1750 (55.14%), Loss: 0.0003\n",
      "Epoch 7, Step 970/1750 (55.43%), Loss: 0.0004\n",
      "Epoch 7, Step 975/1750 (55.71%), Loss: 0.0003\n",
      "Epoch 7, Step 980/1750 (56.00%), Loss: 0.0003\n",
      "Epoch 7, Step 985/1750 (56.29%), Loss: 0.0004\n",
      "Epoch 7, Step 990/1750 (56.57%), Loss: 0.0003\n",
      "Epoch 7, Step 995/1750 (56.86%), Loss: 0.0003\n",
      "Epoch 7, Step 1000/1750 (57.14%), Loss: 0.0003\n",
      "Epoch 7, Step 1005/1750 (57.43%), Loss: 0.0005\n",
      "Epoch 7, Step 1010/1750 (57.71%), Loss: 0.0004\n",
      "Epoch 7, Step 1015/1750 (58.00%), Loss: 0.0001\n",
      "Epoch 7, Step 1020/1750 (58.29%), Loss: 0.0011\n",
      "Epoch 7, Step 1025/1750 (58.57%), Loss: 0.1343\n",
      "Epoch 7, Step 1030/1750 (58.86%), Loss: 0.0002\n",
      "Epoch 7, Step 1035/1750 (59.14%), Loss: 0.0006\n",
      "Epoch 7, Step 1040/1750 (59.43%), Loss: 0.0125\n",
      "Epoch 7, Step 1045/1750 (59.71%), Loss: 0.0004\n",
      "Epoch 7, Step 1050/1750 (60.00%), Loss: 0.0004\n",
      "Epoch 7, Step 1055/1750 (60.29%), Loss: 0.0003\n",
      "Epoch 7, Step 1060/1750 (60.57%), Loss: 0.0003\n",
      "Epoch 7, Step 1065/1750 (60.86%), Loss: 0.0003\n",
      "Epoch 7, Step 1070/1750 (61.14%), Loss: 0.0005\n",
      "Epoch 7, Step 1075/1750 (61.43%), Loss: 0.0001\n",
      "Epoch 7, Step 1080/1750 (61.71%), Loss: 0.0002\n",
      "Epoch 7, Step 1085/1750 (62.00%), Loss: 0.0002\n",
      "Epoch 7, Step 1090/1750 (62.29%), Loss: 0.0002\n",
      "Epoch 7, Step 1095/1750 (62.57%), Loss: 0.5165\n",
      "Epoch 7, Step 1100/1750 (62.86%), Loss: 0.0003\n",
      "Epoch 7, Step 1105/1750 (63.14%), Loss: 0.0002\n",
      "Epoch 7, Step 1110/1750 (63.43%), Loss: 0.0004\n",
      "Epoch 7, Step 1115/1750 (63.71%), Loss: 0.0005\n",
      "Epoch 7, Step 1120/1750 (64.00%), Loss: 0.0004\n",
      "Epoch 7, Step 1125/1750 (64.29%), Loss: 0.0004\n",
      "Epoch 7, Step 1130/1750 (64.57%), Loss: 0.0005\n",
      "Epoch 7, Step 1135/1750 (64.86%), Loss: 0.0004\n",
      "Epoch 7, Step 1140/1750 (65.14%), Loss: 0.0093\n",
      "Epoch 7, Step 1145/1750 (65.43%), Loss: 0.0003\n",
      "Epoch 7, Step 1150/1750 (65.71%), Loss: 0.0344\n",
      "Epoch 7, Step 1155/1750 (66.00%), Loss: 0.0004\n",
      "Epoch 7, Step 1160/1750 (66.29%), Loss: 0.0364\n",
      "Epoch 7, Step 1165/1750 (66.57%), Loss: 0.0009\n",
      "Epoch 7, Step 1170/1750 (66.86%), Loss: 0.0007\n",
      "Epoch 7, Step 1175/1750 (67.14%), Loss: 0.0050\n",
      "Epoch 7, Step 1180/1750 (67.43%), Loss: 0.0003\n",
      "Epoch 7, Step 1185/1750 (67.71%), Loss: 0.0007\n",
      "Epoch 7, Step 1190/1750 (68.00%), Loss: 0.3943\n",
      "Epoch 7, Step 1195/1750 (68.29%), Loss: 0.1845\n",
      "Epoch 7, Step 1200/1750 (68.57%), Loss: 0.0022\n",
      "Epoch 7, Step 1205/1750 (68.86%), Loss: 0.0075\n",
      "Epoch 7, Step 1210/1750 (69.14%), Loss: 0.0053\n",
      "Epoch 7, Step 1215/1750 (69.43%), Loss: 0.0005\n",
      "Epoch 7, Step 1220/1750 (69.71%), Loss: 0.0020\n",
      "Epoch 7, Step 1225/1750 (70.00%), Loss: 0.0009\n",
      "Epoch 7, Step 1230/1750 (70.29%), Loss: 0.0007\n",
      "Epoch 7, Step 1235/1750 (70.57%), Loss: 0.0082\n",
      "Epoch 7, Step 1240/1750 (70.86%), Loss: 0.0110\n",
      "Epoch 7, Step 1245/1750 (71.14%), Loss: 0.0005\n",
      "Epoch 7, Step 1250/1750 (71.43%), Loss: 0.0005\n",
      "Epoch 7, Step 1255/1750 (71.71%), Loss: 0.0005\n",
      "Epoch 7, Step 1260/1750 (72.00%), Loss: 0.0010\n",
      "Epoch 7, Step 1265/1750 (72.29%), Loss: 0.0010\n",
      "Epoch 7, Step 1270/1750 (72.57%), Loss: 0.0006\n",
      "Epoch 7, Step 1275/1750 (72.86%), Loss: 0.0005\n",
      "Epoch 7, Step 1280/1750 (73.14%), Loss: 0.0006\n",
      "Epoch 7, Step 1285/1750 (73.43%), Loss: 0.0005\n",
      "Epoch 7, Step 1290/1750 (73.71%), Loss: 0.0003\n",
      "Epoch 7, Step 1295/1750 (74.00%), Loss: 0.0006\n",
      "Epoch 7, Step 1300/1750 (74.29%), Loss: 0.0007\n",
      "Epoch 7, Step 1305/1750 (74.57%), Loss: 0.0004\n",
      "Epoch 7, Step 1310/1750 (74.86%), Loss: 0.0007\n",
      "Epoch 7, Step 1315/1750 (75.14%), Loss: 0.0003\n",
      "Epoch 7, Step 1320/1750 (75.43%), Loss: 0.0005\n",
      "Epoch 7, Step 1325/1750 (75.71%), Loss: 0.0005\n",
      "Epoch 7, Step 1330/1750 (76.00%), Loss: 0.0003\n",
      "Epoch 7, Step 1335/1750 (76.29%), Loss: 0.0005\n",
      "Epoch 7, Step 1340/1750 (76.57%), Loss: 0.0003\n",
      "Epoch 7, Step 1345/1750 (76.86%), Loss: 0.0002\n",
      "Epoch 7, Step 1350/1750 (77.14%), Loss: 0.0003\n",
      "Epoch 7, Step 1355/1750 (77.43%), Loss: 0.0002\n",
      "Epoch 7, Step 1360/1750 (77.71%), Loss: 0.0003\n",
      "Epoch 7, Step 1365/1750 (78.00%), Loss: 0.0005\n",
      "Epoch 7, Step 1370/1750 (78.29%), Loss: 0.0003\n",
      "Epoch 7, Step 1375/1750 (78.57%), Loss: 0.0002\n",
      "Epoch 7, Step 1380/1750 (78.86%), Loss: 0.0003\n",
      "Epoch 7, Step 1385/1750 (79.14%), Loss: 0.0002\n",
      "Epoch 7, Step 1390/1750 (79.43%), Loss: 0.0004\n",
      "Epoch 7, Step 1395/1750 (79.71%), Loss: 0.0003\n",
      "Epoch 7, Step 1400/1750 (80.00%), Loss: 0.0004\n",
      "Epoch 7, Step 1405/1750 (80.29%), Loss: 0.0003\n",
      "Epoch 7, Step 1410/1750 (80.57%), Loss: 0.0008\n",
      "Epoch 7, Step 1415/1750 (80.86%), Loss: 0.0002\n",
      "Epoch 7, Step 1420/1750 (81.14%), Loss: 0.0003\n",
      "Epoch 7, Step 1425/1750 (81.43%), Loss: 0.0002\n",
      "Epoch 7, Step 1430/1750 (81.71%), Loss: 0.0051\n",
      "Epoch 7, Step 1435/1750 (82.00%), Loss: 0.0002\n",
      "Epoch 7, Step 1440/1750 (82.29%), Loss: 0.0014\n",
      "Epoch 7, Step 1445/1750 (82.57%), Loss: 0.0010\n",
      "Epoch 7, Step 1450/1750 (82.86%), Loss: 0.0261\n",
      "Epoch 7, Step 1455/1750 (83.14%), Loss: 0.0002\n",
      "Epoch 7, Step 1460/1750 (83.43%), Loss: 0.0125\n",
      "Epoch 7, Step 1465/1750 (83.71%), Loss: 0.0003\n",
      "Epoch 7, Step 1470/1750 (84.00%), Loss: 0.0002\n",
      "Epoch 7, Step 1475/1750 (84.29%), Loss: 0.0003\n",
      "Epoch 7, Step 1480/1750 (84.57%), Loss: 0.0005\n",
      "Epoch 7, Step 1485/1750 (84.86%), Loss: 0.0005\n",
      "Epoch 7, Step 1490/1750 (85.14%), Loss: 0.0004\n",
      "Epoch 7, Step 1495/1750 (85.43%), Loss: 0.0005\n",
      "Epoch 7, Step 1500/1750 (85.71%), Loss: 0.0007\n",
      "Epoch 7, Step 1505/1750 (86.00%), Loss: 0.0011\n",
      "Epoch 7, Step 1510/1750 (86.29%), Loss: 0.0004\n",
      "Epoch 7, Step 1515/1750 (86.57%), Loss: 0.0013\n",
      "Epoch 7, Step 1520/1750 (86.86%), Loss: 0.0017\n",
      "Epoch 7, Step 1525/1750 (87.14%), Loss: 0.0013\n",
      "Epoch 7, Step 1530/1750 (87.43%), Loss: 0.0011\n",
      "Epoch 7, Step 1535/1750 (87.71%), Loss: 0.0020\n",
      "Epoch 7, Step 1540/1750 (88.00%), Loss: 0.0019\n",
      "Epoch 7, Step 1545/1750 (88.29%), Loss: 0.0014\n",
      "Epoch 7, Step 1550/1750 (88.57%), Loss: 0.0015\n",
      "Epoch 7, Step 1555/1750 (88.86%), Loss: 0.0026\n",
      "Epoch 7, Step 1560/1750 (89.14%), Loss: 0.0027\n",
      "Epoch 7, Step 1565/1750 (89.43%), Loss: 0.0027\n",
      "Epoch 7, Step 1570/1750 (89.71%), Loss: 0.0020\n",
      "Epoch 7, Step 1575/1750 (90.00%), Loss: 0.0011\n",
      "Epoch 7, Step 1580/1750 (90.29%), Loss: 0.0024\n",
      "Epoch 7, Step 1585/1750 (90.57%), Loss: 0.0021\n",
      "Epoch 7, Step 1590/1750 (90.86%), Loss: 0.0020\n",
      "Epoch 7, Step 1595/1750 (91.14%), Loss: 0.0019\n",
      "Epoch 7, Step 1600/1750 (91.43%), Loss: 0.0229\n",
      "Epoch 7, Step 1605/1750 (91.71%), Loss: 0.0010\n",
      "Epoch 7, Step 1610/1750 (92.00%), Loss: 0.0014\n",
      "Epoch 7, Step 1615/1750 (92.29%), Loss: 0.0008\n",
      "Epoch 7, Step 1620/1750 (92.57%), Loss: 0.0009\n",
      "Epoch 7, Step 1625/1750 (92.86%), Loss: 0.0009\n",
      "Epoch 7, Step 1630/1750 (93.14%), Loss: 0.0009\n",
      "Epoch 7, Step 1635/1750 (93.43%), Loss: 0.0206\n",
      "Epoch 7, Step 1640/1750 (93.71%), Loss: 0.0004\n",
      "Epoch 7, Step 1645/1750 (94.00%), Loss: 0.0005\n",
      "Epoch 7, Step 1650/1750 (94.29%), Loss: 0.0006\n",
      "Epoch 7, Step 1655/1750 (94.57%), Loss: 0.0013\n",
      "Epoch 7, Step 1660/1750 (94.86%), Loss: 0.0005\n",
      "Epoch 7, Step 1665/1750 (95.14%), Loss: 0.0008\n",
      "Epoch 7, Step 1670/1750 (95.43%), Loss: 0.0006\n",
      "Epoch 7, Step 1675/1750 (95.71%), Loss: 0.0005\n",
      "Epoch 7, Step 1680/1750 (96.00%), Loss: 0.0008\n",
      "Epoch 7, Step 1685/1750 (96.29%), Loss: 0.0193\n",
      "Epoch 7, Step 1690/1750 (96.57%), Loss: 0.0005\n",
      "Epoch 7, Step 1695/1750 (96.86%), Loss: 0.0004\n",
      "Epoch 7, Step 1700/1750 (97.14%), Loss: 0.0003\n",
      "Epoch 7, Step 1705/1750 (97.43%), Loss: 0.0003\n",
      "Epoch 7, Step 1710/1750 (97.71%), Loss: 0.0003\n",
      "Epoch 7, Step 1715/1750 (98.00%), Loss: 0.0009\n",
      "Epoch 7, Step 1720/1750 (98.29%), Loss: 0.0004\n",
      "Epoch 7, Step 1725/1750 (98.57%), Loss: 0.0054\n",
      "Epoch 7, Step 1730/1750 (98.86%), Loss: 0.0006\n",
      "Epoch 7, Step 1735/1750 (99.14%), Loss: 0.4553\n",
      "Epoch 7, Step 1740/1750 (99.43%), Loss: 0.0361\n",
      "Epoch 7, Step 1745/1750 (99.71%), Loss: 0.0006\n",
      "Epoch 7, Step 1750/1750 (100.00%), Loss: 0.0008\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.41\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.09\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.00\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.84\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 0.84\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.43\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.90\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.69\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.47\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.58\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.01\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.00\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.32\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 0.88\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.12\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.00\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.44\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.03\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.78\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.75\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.91\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.00\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.60\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.00\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.09\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 1.17\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.41\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.60\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.00\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.00\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.00\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.84\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.00\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.44\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.50\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.03\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.01\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.14\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.35\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.09\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 1.12\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.42\n",
      "End of Epoch 7/10 - Average Train Loss: 0.01, Average Validation Loss: 0.38\n",
      "Starting Epoch 8/10\n",
      "Epoch 8, Step 5/1750 (0.29%), Loss: 0.0008\n",
      "Epoch 8, Step 10/1750 (0.57%), Loss: 0.0005\n",
      "Epoch 8, Step 15/1750 (0.86%), Loss: 0.0006\n",
      "Epoch 8, Step 20/1750 (1.14%), Loss: 0.0007\n",
      "Epoch 8, Step 25/1750 (1.43%), Loss: 0.0006\n",
      "Epoch 8, Step 30/1750 (1.71%), Loss: 0.0005\n",
      "Epoch 8, Step 35/1750 (2.00%), Loss: 0.0003\n",
      "Epoch 8, Step 40/1750 (2.29%), Loss: 0.0009\n",
      "Epoch 8, Step 45/1750 (2.57%), Loss: 0.0006\n",
      "Epoch 8, Step 50/1750 (2.86%), Loss: 0.0005\n",
      "Epoch 8, Step 55/1750 (3.14%), Loss: 0.0007\n",
      "Epoch 8, Step 60/1750 (3.43%), Loss: 0.0005\n",
      "Epoch 8, Step 65/1750 (3.71%), Loss: 0.0008\n",
      "Epoch 8, Step 70/1750 (4.00%), Loss: 0.0006\n",
      "Epoch 8, Step 75/1750 (4.29%), Loss: 0.0006\n",
      "Epoch 8, Step 80/1750 (4.57%), Loss: 0.0005\n",
      "Epoch 8, Step 85/1750 (4.86%), Loss: 0.0006\n",
      "Epoch 8, Step 90/1750 (5.14%), Loss: 0.0005\n",
      "Epoch 8, Step 95/1750 (5.43%), Loss: 0.0004\n",
      "Epoch 8, Step 100/1750 (5.71%), Loss: 0.0003\n",
      "Epoch 8, Step 105/1750 (6.00%), Loss: 0.0005\n",
      "Epoch 8, Step 110/1750 (6.29%), Loss: 0.0003\n",
      "Epoch 8, Step 115/1750 (6.57%), Loss: 0.0015\n",
      "Epoch 8, Step 120/1750 (6.86%), Loss: 0.0004\n",
      "Epoch 8, Step 125/1750 (7.14%), Loss: 0.0006\n",
      "Epoch 8, Step 130/1750 (7.43%), Loss: 0.0014\n",
      "Epoch 8, Step 135/1750 (7.71%), Loss: 0.0007\n",
      "Epoch 8, Step 140/1750 (8.00%), Loss: 0.0008\n",
      "Epoch 8, Step 145/1750 (8.29%), Loss: 0.0008\n",
      "Epoch 8, Step 150/1750 (8.57%), Loss: 0.0008\n",
      "Epoch 8, Step 155/1750 (8.86%), Loss: 0.0006\n",
      "Epoch 8, Step 160/1750 (9.14%), Loss: 0.0005\n",
      "Epoch 8, Step 165/1750 (9.43%), Loss: 0.0011\n",
      "Epoch 8, Step 170/1750 (9.71%), Loss: 0.0002\n",
      "Epoch 8, Step 175/1750 (10.00%), Loss: 0.0011\n",
      "Epoch 8, Step 180/1750 (10.29%), Loss: 0.0003\n",
      "Epoch 8, Step 185/1750 (10.57%), Loss: 0.0006\n",
      "Epoch 8, Step 190/1750 (10.86%), Loss: 0.0004\n",
      "Epoch 8, Step 195/1750 (11.14%), Loss: 0.0004\n",
      "Epoch 8, Step 200/1750 (11.43%), Loss: 0.0006\n",
      "Epoch 8, Step 205/1750 (11.71%), Loss: 0.0005\n",
      "Epoch 8, Step 210/1750 (12.00%), Loss: 0.0005\n",
      "Epoch 8, Step 215/1750 (12.29%), Loss: 0.0020\n",
      "Epoch 8, Step 220/1750 (12.57%), Loss: 0.0110\n",
      "Epoch 8, Step 225/1750 (12.86%), Loss: 0.0004\n",
      "Epoch 8, Step 230/1750 (13.14%), Loss: 0.0004\n",
      "Epoch 8, Step 235/1750 (13.43%), Loss: 0.0003\n",
      "Epoch 8, Step 240/1750 (13.71%), Loss: 0.0004\n",
      "Epoch 8, Step 245/1750 (14.00%), Loss: 0.0006\n",
      "Epoch 8, Step 250/1750 (14.29%), Loss: 0.0876\n",
      "Epoch 8, Step 255/1750 (14.57%), Loss: 0.0004\n",
      "Epoch 8, Step 260/1750 (14.86%), Loss: 0.0003\n",
      "Epoch 8, Step 265/1750 (15.14%), Loss: 0.0004\n",
      "Epoch 8, Step 270/1750 (15.43%), Loss: 0.0002\n",
      "Epoch 8, Step 275/1750 (15.71%), Loss: 0.0008\n",
      "Epoch 8, Step 280/1750 (16.00%), Loss: 0.0004\n",
      "Epoch 8, Step 285/1750 (16.29%), Loss: 0.0003\n",
      "Epoch 8, Step 290/1750 (16.57%), Loss: 0.0004\n",
      "Epoch 8, Step 295/1750 (16.86%), Loss: 0.0004\n",
      "Epoch 8, Step 300/1750 (17.14%), Loss: 0.0003\n",
      "Epoch 8, Step 305/1750 (17.43%), Loss: 0.0005\n",
      "Epoch 8, Step 310/1750 (17.71%), Loss: 0.0010\n",
      "Epoch 8, Step 315/1750 (18.00%), Loss: 0.0003\n",
      "Epoch 8, Step 320/1750 (18.29%), Loss: 0.0003\n",
      "Epoch 8, Step 325/1750 (18.57%), Loss: 0.0018\n",
      "Epoch 8, Step 330/1750 (18.86%), Loss: 0.0004\n",
      "Epoch 8, Step 335/1750 (19.14%), Loss: 0.0003\n",
      "Epoch 8, Step 340/1750 (19.43%), Loss: 0.0002\n",
      "Epoch 8, Step 345/1750 (19.71%), Loss: 0.0003\n",
      "Epoch 8, Step 350/1750 (20.00%), Loss: 0.0003\n",
      "Epoch 8, Step 355/1750 (20.29%), Loss: 0.0004\n",
      "Epoch 8, Step 360/1750 (20.57%), Loss: 0.0008\n",
      "Epoch 8, Step 365/1750 (20.86%), Loss: 0.0002\n",
      "Epoch 8, Step 370/1750 (21.14%), Loss: 0.0004\n",
      "Epoch 8, Step 375/1750 (21.43%), Loss: 0.0006\n",
      "Epoch 8, Step 380/1750 (21.71%), Loss: 0.4567\n",
      "Epoch 8, Step 385/1750 (22.00%), Loss: 0.0013\n",
      "Epoch 8, Step 390/1750 (22.29%), Loss: 0.0005\n",
      "Epoch 8, Step 395/1750 (22.57%), Loss: 0.0007\n",
      "Epoch 8, Step 400/1750 (22.86%), Loss: 0.0008\n",
      "Epoch 8, Step 405/1750 (23.14%), Loss: 0.0009\n",
      "Epoch 8, Step 410/1750 (23.43%), Loss: 0.0007\n",
      "Epoch 8, Step 415/1750 (23.71%), Loss: 0.0007\n",
      "Epoch 8, Step 420/1750 (24.00%), Loss: 0.0005\n",
      "Epoch 8, Step 425/1750 (24.29%), Loss: 0.0005\n",
      "Epoch 8, Step 430/1750 (24.57%), Loss: 0.0004\n",
      "Epoch 8, Step 435/1750 (24.86%), Loss: 0.0007\n",
      "Epoch 8, Step 440/1750 (25.14%), Loss: 0.0008\n",
      "Epoch 8, Step 445/1750 (25.43%), Loss: 0.0012\n",
      "Epoch 8, Step 450/1750 (25.71%), Loss: 0.0008\n",
      "Epoch 8, Step 455/1750 (26.00%), Loss: 0.0006\n",
      "Epoch 8, Step 460/1750 (26.29%), Loss: 0.0008\n",
      "Epoch 8, Step 465/1750 (26.57%), Loss: 0.0010\n",
      "Epoch 8, Step 470/1750 (26.86%), Loss: 0.0005\n",
      "Epoch 8, Step 475/1750 (27.14%), Loss: 0.0008\n",
      "Epoch 8, Step 480/1750 (27.43%), Loss: 0.0005\n",
      "Epoch 8, Step 485/1750 (27.71%), Loss: 0.4509\n",
      "Epoch 8, Step 490/1750 (28.00%), Loss: 0.0011\n",
      "Epoch 8, Step 495/1750 (28.29%), Loss: 0.0004\n",
      "Epoch 8, Step 500/1750 (28.57%), Loss: 0.0021\n",
      "Epoch 8, Step 505/1750 (28.86%), Loss: 0.0008\n",
      "Epoch 8, Step 510/1750 (29.14%), Loss: 0.0021\n",
      "Epoch 8, Step 515/1750 (29.43%), Loss: 0.0006\n",
      "Epoch 8, Step 520/1750 (29.71%), Loss: 0.0005\n",
      "Epoch 8, Step 525/1750 (30.00%), Loss: 0.0005\n",
      "Epoch 8, Step 530/1750 (30.29%), Loss: 0.0046\n",
      "Epoch 8, Step 535/1750 (30.57%), Loss: 0.0010\n",
      "Epoch 8, Step 540/1750 (30.86%), Loss: 0.0007\n",
      "Epoch 8, Step 545/1750 (31.14%), Loss: 0.0049\n",
      "Epoch 8, Step 550/1750 (31.43%), Loss: 0.0067\n",
      "Epoch 8, Step 555/1750 (31.71%), Loss: 0.0006\n",
      "Epoch 8, Step 560/1750 (32.00%), Loss: 0.0006\n",
      "Epoch 8, Step 565/1750 (32.29%), Loss: 0.0007\n",
      "Epoch 8, Step 570/1750 (32.57%), Loss: 0.0006\n",
      "Epoch 8, Step 575/1750 (32.86%), Loss: 0.0009\n",
      "Epoch 8, Step 580/1750 (33.14%), Loss: 0.0008\n",
      "Epoch 8, Step 585/1750 (33.43%), Loss: 0.0036\n",
      "Epoch 8, Step 590/1750 (33.71%), Loss: 0.0007\n",
      "Epoch 8, Step 595/1750 (34.00%), Loss: 0.0035\n",
      "Epoch 8, Step 600/1750 (34.29%), Loss: 0.0006\n",
      "Epoch 8, Step 605/1750 (34.57%), Loss: 0.0008\n",
      "Epoch 8, Step 610/1750 (34.86%), Loss: 0.0005\n",
      "Epoch 8, Step 615/1750 (35.14%), Loss: 0.0007\n",
      "Epoch 8, Step 620/1750 (35.43%), Loss: 0.0007\n",
      "Epoch 8, Step 625/1750 (35.71%), Loss: 0.0007\n",
      "Epoch 8, Step 630/1750 (36.00%), Loss: 0.4010\n",
      "Epoch 8, Step 635/1750 (36.29%), Loss: 0.0008\n",
      "Epoch 8, Step 640/1750 (36.57%), Loss: 0.0010\n",
      "Epoch 8, Step 645/1750 (36.86%), Loss: 0.0005\n",
      "Epoch 8, Step 650/1750 (37.14%), Loss: 0.0007\n",
      "Epoch 8, Step 655/1750 (37.43%), Loss: 0.0005\n",
      "Epoch 8, Step 660/1750 (37.71%), Loss: 0.0005\n",
      "Epoch 8, Step 665/1750 (38.00%), Loss: 0.0009\n",
      "Epoch 8, Step 670/1750 (38.29%), Loss: 0.0008\n",
      "Epoch 8, Step 675/1750 (38.57%), Loss: 0.0004\n",
      "Epoch 8, Step 680/1750 (38.86%), Loss: 0.0009\n",
      "Epoch 8, Step 685/1750 (39.14%), Loss: 0.0004\n",
      "Epoch 8, Step 690/1750 (39.43%), Loss: 0.0009\n",
      "Epoch 8, Step 695/1750 (39.71%), Loss: 0.0009\n",
      "Epoch 8, Step 700/1750 (40.00%), Loss: 0.0009\n",
      "Epoch 8, Step 705/1750 (40.29%), Loss: 0.0011\n",
      "Epoch 8, Step 710/1750 (40.57%), Loss: 0.0002\n",
      "Epoch 8, Step 715/1750 (40.86%), Loss: 0.0006\n",
      "Epoch 8, Step 720/1750 (41.14%), Loss: 0.0004\n",
      "Epoch 8, Step 725/1750 (41.43%), Loss: 0.0007\n",
      "Epoch 8, Step 730/1750 (41.71%), Loss: 0.0008\n",
      "Epoch 8, Step 735/1750 (42.00%), Loss: 0.0007\n",
      "Epoch 8, Step 740/1750 (42.29%), Loss: 0.0009\n",
      "Epoch 8, Step 745/1750 (42.57%), Loss: 0.0009\n",
      "Epoch 8, Step 750/1750 (42.86%), Loss: 0.0008\n",
      "Epoch 8, Step 755/1750 (43.14%), Loss: 0.0010\n",
      "Epoch 8, Step 760/1750 (43.43%), Loss: 0.0007\n",
      "Epoch 8, Step 765/1750 (43.71%), Loss: 0.0015\n",
      "Epoch 8, Step 770/1750 (44.00%), Loss: 0.0013\n",
      "Epoch 8, Step 775/1750 (44.29%), Loss: 0.0015\n",
      "Epoch 8, Step 780/1750 (44.57%), Loss: 0.0011\n",
      "Epoch 8, Step 785/1750 (44.86%), Loss: 0.0005\n",
      "Epoch 8, Step 790/1750 (45.14%), Loss: 0.0007\n",
      "Epoch 8, Step 795/1750 (45.43%), Loss: 0.0009\n",
      "Epoch 8, Step 800/1750 (45.71%), Loss: 0.0007\n",
      "Epoch 8, Step 805/1750 (46.00%), Loss: 0.0007\n",
      "Epoch 8, Step 810/1750 (46.29%), Loss: 0.0007\n",
      "Epoch 8, Step 815/1750 (46.57%), Loss: 0.0007\n",
      "Epoch 8, Step 820/1750 (46.86%), Loss: 0.0007\n",
      "Epoch 8, Step 825/1750 (47.14%), Loss: 0.0007\n",
      "Epoch 8, Step 830/1750 (47.43%), Loss: 0.0006\n",
      "Epoch 8, Step 835/1750 (47.71%), Loss: 0.0009\n",
      "Epoch 8, Step 840/1750 (48.00%), Loss: 0.0009\n",
      "Epoch 8, Step 845/1750 (48.29%), Loss: 0.0007\n",
      "Epoch 8, Step 850/1750 (48.57%), Loss: 0.0076\n",
      "Epoch 8, Step 855/1750 (48.86%), Loss: 0.0007\n",
      "Epoch 8, Step 860/1750 (49.14%), Loss: 0.0010\n",
      "Epoch 8, Step 865/1750 (49.43%), Loss: 0.0005\n",
      "Epoch 8, Step 870/1750 (49.71%), Loss: 0.0006\n",
      "Epoch 8, Step 875/1750 (50.00%), Loss: 0.0011\n",
      "Epoch 8, Step 880/1750 (50.29%), Loss: 0.0004\n",
      "Epoch 8, Step 885/1750 (50.57%), Loss: 0.0006\n",
      "Epoch 8, Step 890/1750 (50.86%), Loss: 0.0006\n",
      "Epoch 8, Step 895/1750 (51.14%), Loss: 0.0004\n",
      "Epoch 8, Step 900/1750 (51.43%), Loss: 0.0007\n",
      "Epoch 8, Step 905/1750 (51.71%), Loss: 0.0006\n",
      "Epoch 8, Step 910/1750 (52.00%), Loss: 0.0007\n",
      "Epoch 8, Step 915/1750 (52.29%), Loss: 0.0007\n",
      "Epoch 8, Step 920/1750 (52.57%), Loss: 0.0004\n",
      "Epoch 8, Step 925/1750 (52.86%), Loss: 0.0006\n",
      "Epoch 8, Step 930/1750 (53.14%), Loss: 0.0005\n",
      "Epoch 8, Step 935/1750 (53.43%), Loss: 0.0018\n",
      "Epoch 8, Step 940/1750 (53.71%), Loss: 0.0131\n",
      "Epoch 8, Step 945/1750 (54.00%), Loss: 0.0007\n",
      "Epoch 8, Step 950/1750 (54.29%), Loss: 0.0008\n",
      "Epoch 8, Step 955/1750 (54.57%), Loss: 0.0005\n",
      "Epoch 8, Step 960/1750 (54.86%), Loss: 0.0004\n",
      "Epoch 8, Step 965/1750 (55.14%), Loss: 0.0004\n",
      "Epoch 8, Step 970/1750 (55.43%), Loss: 0.0005\n",
      "Epoch 8, Step 975/1750 (55.71%), Loss: 0.0003\n",
      "Epoch 8, Step 980/1750 (56.00%), Loss: 0.0003\n",
      "Epoch 8, Step 985/1750 (56.29%), Loss: 0.0006\n",
      "Epoch 8, Step 990/1750 (56.57%), Loss: 0.0004\n",
      "Epoch 8, Step 995/1750 (56.86%), Loss: 0.0004\n",
      "Epoch 8, Step 1000/1750 (57.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1005/1750 (57.43%), Loss: 0.0011\n",
      "Epoch 8, Step 1010/1750 (57.71%), Loss: 0.0004\n",
      "Epoch 8, Step 1015/1750 (58.00%), Loss: 0.0009\n",
      "Epoch 8, Step 1020/1750 (58.29%), Loss: 0.0007\n",
      "Epoch 8, Step 1025/1750 (58.57%), Loss: 0.0004\n",
      "Epoch 8, Step 1030/1750 (58.86%), Loss: 0.0005\n",
      "Epoch 8, Step 1035/1750 (59.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1040/1750 (59.43%), Loss: 0.0006\n",
      "Epoch 8, Step 1045/1750 (59.71%), Loss: 0.0003\n",
      "Epoch 8, Step 1050/1750 (60.00%), Loss: 0.0003\n",
      "Epoch 8, Step 1055/1750 (60.29%), Loss: 0.0005\n",
      "Epoch 8, Step 1060/1750 (60.57%), Loss: 0.0004\n",
      "Epoch 8, Step 1065/1750 (60.86%), Loss: 0.0003\n",
      "Epoch 8, Step 1070/1750 (61.14%), Loss: 0.0004\n",
      "Epoch 8, Step 1075/1750 (61.43%), Loss: 0.0004\n",
      "Epoch 8, Step 1080/1750 (61.71%), Loss: 0.0009\n",
      "Epoch 8, Step 1085/1750 (62.00%), Loss: 0.0003\n",
      "Epoch 8, Step 1090/1750 (62.29%), Loss: 0.0002\n",
      "Epoch 8, Step 1095/1750 (62.57%), Loss: 0.0003\n",
      "Epoch 8, Step 1100/1750 (62.86%), Loss: 0.0002\n",
      "Epoch 8, Step 1105/1750 (63.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1110/1750 (63.43%), Loss: 0.0002\n",
      "Epoch 8, Step 1115/1750 (63.71%), Loss: 0.0003\n",
      "Epoch 8, Step 1120/1750 (64.00%), Loss: 0.0005\n",
      "Epoch 8, Step 1125/1750 (64.29%), Loss: 0.0003\n",
      "Epoch 8, Step 1130/1750 (64.57%), Loss: 0.0004\n",
      "Epoch 8, Step 1135/1750 (64.86%), Loss: 0.0005\n",
      "Epoch 8, Step 1140/1750 (65.14%), Loss: 0.0005\n",
      "Epoch 8, Step 1145/1750 (65.43%), Loss: 0.0117\n",
      "Epoch 8, Step 1150/1750 (65.71%), Loss: 0.0132\n",
      "Epoch 8, Step 1155/1750 (66.00%), Loss: 0.0006\n",
      "Epoch 8, Step 1160/1750 (66.29%), Loss: 0.0008\n",
      "Epoch 8, Step 1165/1750 (66.57%), Loss: 0.0008\n",
      "Epoch 8, Step 1170/1750 (66.86%), Loss: 0.0003\n",
      "Epoch 8, Step 1175/1750 (67.14%), Loss: 0.0008\n",
      "Epoch 8, Step 1180/1750 (67.43%), Loss: 0.0006\n",
      "Epoch 8, Step 1185/1750 (67.71%), Loss: 0.0008\n",
      "Epoch 8, Step 1190/1750 (68.00%), Loss: 0.0005\n",
      "Epoch 8, Step 1195/1750 (68.29%), Loss: 0.0007\n",
      "Epoch 8, Step 1200/1750 (68.57%), Loss: 0.0006\n",
      "Epoch 8, Step 1205/1750 (68.86%), Loss: 0.0006\n",
      "Epoch 8, Step 1210/1750 (69.14%), Loss: 0.0005\n",
      "Epoch 8, Step 1215/1750 (69.43%), Loss: 0.0005\n",
      "Epoch 8, Step 1220/1750 (69.71%), Loss: 0.0006\n",
      "Epoch 8, Step 1225/1750 (70.00%), Loss: 0.0005\n",
      "Epoch 8, Step 1230/1750 (70.29%), Loss: 0.0124\n",
      "Epoch 8, Step 1235/1750 (70.57%), Loss: 0.0038\n",
      "Epoch 8, Step 1240/1750 (70.86%), Loss: 0.0010\n",
      "Epoch 8, Step 1245/1750 (71.14%), Loss: 0.0008\n",
      "Epoch 8, Step 1250/1750 (71.43%), Loss: 0.0009\n",
      "Epoch 8, Step 1255/1750 (71.71%), Loss: 0.0006\n",
      "Epoch 8, Step 1260/1750 (72.00%), Loss: 0.0011\n",
      "Epoch 8, Step 1265/1750 (72.29%), Loss: 0.0007\n",
      "Epoch 8, Step 1270/1750 (72.57%), Loss: 0.0006\n",
      "Epoch 8, Step 1275/1750 (72.86%), Loss: 0.0008\n",
      "Epoch 8, Step 1280/1750 (73.14%), Loss: 0.0008\n",
      "Epoch 8, Step 1285/1750 (73.43%), Loss: 0.0007\n",
      "Epoch 8, Step 1290/1750 (73.71%), Loss: 0.0006\n",
      "Epoch 8, Step 1295/1750 (74.00%), Loss: 0.0004\n",
      "Epoch 8, Step 1300/1750 (74.29%), Loss: 0.0004\n",
      "Epoch 8, Step 1305/1750 (74.57%), Loss: 0.0006\n",
      "Epoch 8, Step 1310/1750 (74.86%), Loss: 0.0009\n",
      "Epoch 8, Step 1315/1750 (75.14%), Loss: 0.0008\n",
      "Epoch 8, Step 1320/1750 (75.43%), Loss: 0.0013\n",
      "Epoch 8, Step 1325/1750 (75.71%), Loss: 0.0005\n",
      "Epoch 8, Step 1330/1750 (76.00%), Loss: 0.0008\n",
      "Epoch 8, Step 1335/1750 (76.29%), Loss: 0.0006\n",
      "Epoch 8, Step 1340/1750 (76.57%), Loss: 0.0048\n",
      "Epoch 8, Step 1345/1750 (76.86%), Loss: 0.0010\n",
      "Epoch 8, Step 1350/1750 (77.14%), Loss: 0.0009\n",
      "Epoch 8, Step 1355/1750 (77.43%), Loss: 0.0008\n",
      "Epoch 8, Step 1360/1750 (77.71%), Loss: 0.0005\n",
      "Epoch 8, Step 1365/1750 (78.00%), Loss: 0.0008\n",
      "Epoch 8, Step 1370/1750 (78.29%), Loss: 0.0007\n",
      "Epoch 8, Step 1375/1750 (78.57%), Loss: 0.0009\n",
      "Epoch 8, Step 1380/1750 (78.86%), Loss: 0.0013\n",
      "Epoch 8, Step 1385/1750 (79.14%), Loss: 0.0009\n",
      "Epoch 8, Step 1390/1750 (79.43%), Loss: 0.0011\n",
      "Epoch 8, Step 1395/1750 (79.71%), Loss: 0.0007\n",
      "Epoch 8, Step 1400/1750 (80.00%), Loss: 0.0010\n",
      "Epoch 8, Step 1405/1750 (80.29%), Loss: 0.0006\n",
      "Epoch 8, Step 1410/1750 (80.57%), Loss: 0.0005\n",
      "Epoch 8, Step 1415/1750 (80.86%), Loss: 0.0076\n",
      "Epoch 8, Step 1420/1750 (81.14%), Loss: 0.0008\n",
      "Epoch 8, Step 1425/1750 (81.43%), Loss: 0.0006\n",
      "Epoch 8, Step 1430/1750 (81.71%), Loss: 0.0010\n",
      "Epoch 8, Step 1435/1750 (82.00%), Loss: 0.0003\n",
      "Epoch 8, Step 1440/1750 (82.29%), Loss: 0.0007\n",
      "Epoch 8, Step 1445/1750 (82.57%), Loss: 0.0007\n",
      "Epoch 8, Step 1450/1750 (82.86%), Loss: 0.0007\n",
      "Epoch 8, Step 1455/1750 (83.14%), Loss: 0.0006\n",
      "Epoch 8, Step 1460/1750 (83.43%), Loss: 0.0007\n",
      "Epoch 8, Step 1465/1750 (83.71%), Loss: 0.0006\n",
      "Epoch 8, Step 1470/1750 (84.00%), Loss: 0.0007\n",
      "Epoch 8, Step 1475/1750 (84.29%), Loss: 0.0007\n",
      "Epoch 8, Step 1480/1750 (84.57%), Loss: 0.0002\n",
      "Epoch 8, Step 1485/1750 (84.86%), Loss: 0.0010\n",
      "Epoch 8, Step 1490/1750 (85.14%), Loss: 0.0006\n",
      "Epoch 8, Step 1495/1750 (85.43%), Loss: 0.0004\n",
      "Epoch 8, Step 1500/1750 (85.71%), Loss: 0.0005\n",
      "Epoch 8, Step 1505/1750 (86.00%), Loss: 0.0004\n",
      "Epoch 8, Step 1510/1750 (86.29%), Loss: 0.0003\n",
      "Epoch 8, Step 1515/1750 (86.57%), Loss: 0.0005\n",
      "Epoch 8, Step 1520/1750 (86.86%), Loss: 0.0005\n",
      "Epoch 8, Step 1525/1750 (87.14%), Loss: 0.0004\n",
      "Epoch 8, Step 1530/1750 (87.43%), Loss: 0.0002\n",
      "Epoch 8, Step 1535/1750 (87.71%), Loss: 0.0005\n",
      "Epoch 8, Step 1540/1750 (88.00%), Loss: 0.0004\n",
      "Epoch 8, Step 1545/1750 (88.29%), Loss: 0.0003\n",
      "Epoch 8, Step 1550/1750 (88.57%), Loss: 0.0003\n",
      "Epoch 8, Step 1555/1750 (88.86%), Loss: 0.0005\n",
      "Epoch 8, Step 1560/1750 (89.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1565/1750 (89.43%), Loss: 0.0005\n",
      "Epoch 8, Step 1570/1750 (89.71%), Loss: 0.0003\n",
      "Epoch 8, Step 1575/1750 (90.00%), Loss: 0.0003\n",
      "Epoch 8, Step 1580/1750 (90.29%), Loss: 0.0004\n",
      "Epoch 8, Step 1585/1750 (90.57%), Loss: 0.0004\n",
      "Epoch 8, Step 1590/1750 (90.86%), Loss: 0.0003\n",
      "Epoch 8, Step 1595/1750 (91.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1600/1750 (91.43%), Loss: 0.0007\n",
      "Epoch 8, Step 1605/1750 (91.71%), Loss: 0.0003\n",
      "Epoch 8, Step 1610/1750 (92.00%), Loss: 0.0003\n",
      "Epoch 8, Step 1615/1750 (92.29%), Loss: 0.0003\n",
      "Epoch 8, Step 1620/1750 (92.57%), Loss: 0.2568\n",
      "Epoch 8, Step 1625/1750 (92.86%), Loss: 0.0003\n",
      "Epoch 8, Step 1630/1750 (93.14%), Loss: 0.4011\n",
      "Epoch 8, Step 1635/1750 (93.43%), Loss: 0.0005\n",
      "Epoch 8, Step 1640/1750 (93.71%), Loss: 0.0004\n",
      "Epoch 8, Step 1645/1750 (94.00%), Loss: 0.0004\n",
      "Epoch 8, Step 1650/1750 (94.29%), Loss: 0.0003\n",
      "Epoch 8, Step 1655/1750 (94.57%), Loss: 0.0145\n",
      "Epoch 8, Step 1660/1750 (94.86%), Loss: 0.0003\n",
      "Epoch 8, Step 1665/1750 (95.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1670/1750 (95.43%), Loss: 0.0003\n",
      "Epoch 8, Step 1675/1750 (95.71%), Loss: 0.0003\n",
      "Epoch 8, Step 1680/1750 (96.00%), Loss: 0.0004\n",
      "Epoch 8, Step 1685/1750 (96.29%), Loss: 0.0004\n",
      "Epoch 8, Step 1690/1750 (96.57%), Loss: 0.0005\n",
      "Epoch 8, Step 1695/1750 (96.86%), Loss: 0.0004\n",
      "Epoch 8, Step 1700/1750 (97.14%), Loss: 0.0003\n",
      "Epoch 8, Step 1705/1750 (97.43%), Loss: 0.0003\n",
      "Epoch 8, Step 1710/1750 (97.71%), Loss: 0.0002\n",
      "Epoch 8, Step 1715/1750 (98.00%), Loss: 0.0002\n",
      "Epoch 8, Step 1720/1750 (98.29%), Loss: 0.0004\n",
      "Epoch 8, Step 1725/1750 (98.57%), Loss: 0.0004\n",
      "Epoch 8, Step 1730/1750 (98.86%), Loss: 0.0002\n",
      "Epoch 8, Step 1735/1750 (99.14%), Loss: 0.0002\n",
      "Epoch 8, Step 1740/1750 (99.43%), Loss: 0.0004\n",
      "Epoch 8, Step 1745/1750 (99.71%), Loss: 0.0294\n",
      "Epoch 8, Step 1750/1750 (100.00%), Loss: 0.0011\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.74\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.07\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.00\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.63\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 1.37\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.52\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.61\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.69\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.33\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.58\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.21\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.00\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.13\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 1.16\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.38\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.00\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.49\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.00\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.52\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 1.08\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.51\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.00\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.64\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.00\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.40\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 1.23\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.61\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.46\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.00\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.03\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.12\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 1.03\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.02\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.50\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.78\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.00\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.00\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.15\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.53\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.28\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.91\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.42\n",
      "End of Epoch 8/10 - Average Train Loss: 0.01, Average Validation Loss: 0.40\n",
      "Starting Epoch 9/10\n",
      "Epoch 9, Step 5/1750 (0.29%), Loss: 0.0002\n",
      "Epoch 9, Step 10/1750 (0.57%), Loss: 0.0003\n",
      "Epoch 9, Step 15/1750 (0.86%), Loss: 0.0007\n",
      "Epoch 9, Step 20/1750 (1.14%), Loss: 0.0003\n",
      "Epoch 9, Step 25/1750 (1.43%), Loss: 0.0003\n",
      "Epoch 9, Step 30/1750 (1.71%), Loss: 0.0003\n",
      "Epoch 9, Step 35/1750 (2.00%), Loss: 0.0003\n",
      "Epoch 9, Step 40/1750 (2.29%), Loss: 0.0024\n",
      "Epoch 9, Step 45/1750 (2.57%), Loss: 0.0003\n",
      "Epoch 9, Step 50/1750 (2.86%), Loss: 0.0003\n",
      "Epoch 9, Step 55/1750 (3.14%), Loss: 0.0002\n",
      "Epoch 9, Step 60/1750 (3.43%), Loss: 0.0002\n",
      "Epoch 9, Step 65/1750 (3.71%), Loss: 0.0002\n",
      "Epoch 9, Step 70/1750 (4.00%), Loss: 0.0005\n",
      "Epoch 9, Step 75/1750 (4.29%), Loss: 0.0002\n",
      "Epoch 9, Step 80/1750 (4.57%), Loss: 0.0002\n",
      "Epoch 9, Step 85/1750 (4.86%), Loss: 0.0004\n",
      "Epoch 9, Step 90/1750 (5.14%), Loss: 0.0006\n",
      "Epoch 9, Step 95/1750 (5.43%), Loss: 0.0003\n",
      "Epoch 9, Step 100/1750 (5.71%), Loss: 0.0002\n",
      "Epoch 9, Step 105/1750 (6.00%), Loss: 0.0018\n",
      "Epoch 9, Step 110/1750 (6.29%), Loss: 0.0002\n",
      "Epoch 9, Step 115/1750 (6.57%), Loss: 0.0002\n",
      "Epoch 9, Step 120/1750 (6.86%), Loss: 0.0002\n",
      "Epoch 9, Step 125/1750 (7.14%), Loss: 0.0003\n",
      "Epoch 9, Step 130/1750 (7.43%), Loss: 0.0004\n",
      "Epoch 9, Step 135/1750 (7.71%), Loss: 0.0002\n",
      "Epoch 9, Step 140/1750 (8.00%), Loss: 0.0001\n",
      "Epoch 9, Step 145/1750 (8.29%), Loss: 0.0003\n",
      "Epoch 9, Step 150/1750 (8.57%), Loss: 0.0002\n",
      "Epoch 9, Step 155/1750 (8.86%), Loss: 0.0003\n",
      "Epoch 9, Step 160/1750 (9.14%), Loss: 0.0003\n",
      "Epoch 9, Step 165/1750 (9.43%), Loss: 0.0001\n",
      "Epoch 9, Step 170/1750 (9.71%), Loss: 0.0002\n",
      "Epoch 9, Step 175/1750 (10.00%), Loss: 0.0002\n",
      "Epoch 9, Step 180/1750 (10.29%), Loss: 0.0003\n",
      "Epoch 9, Step 185/1750 (10.57%), Loss: 0.0002\n",
      "Epoch 9, Step 190/1750 (10.86%), Loss: 0.0002\n",
      "Epoch 9, Step 195/1750 (11.14%), Loss: 0.0001\n",
      "Epoch 9, Step 200/1750 (11.43%), Loss: 0.0002\n",
      "Epoch 9, Step 205/1750 (11.71%), Loss: 0.0003\n",
      "Epoch 9, Step 210/1750 (12.00%), Loss: 0.0002\n",
      "Epoch 9, Step 215/1750 (12.29%), Loss: 0.0001\n",
      "Epoch 9, Step 220/1750 (12.57%), Loss: 0.0002\n",
      "Epoch 9, Step 225/1750 (12.86%), Loss: 0.0004\n",
      "Epoch 9, Step 230/1750 (13.14%), Loss: 0.0002\n",
      "Epoch 9, Step 235/1750 (13.43%), Loss: 0.0003\n",
      "Epoch 9, Step 240/1750 (13.71%), Loss: 0.0001\n",
      "Epoch 9, Step 245/1750 (14.00%), Loss: 0.0155\n",
      "Epoch 9, Step 250/1750 (14.29%), Loss: 0.0003\n",
      "Epoch 9, Step 255/1750 (14.57%), Loss: 0.0002\n",
      "Epoch 9, Step 260/1750 (14.86%), Loss: 0.0001\n",
      "Epoch 9, Step 265/1750 (15.14%), Loss: 0.0001\n",
      "Epoch 9, Step 270/1750 (15.43%), Loss: 0.0096\n",
      "Epoch 9, Step 275/1750 (15.71%), Loss: 0.0002\n",
      "Epoch 9, Step 280/1750 (16.00%), Loss: 0.0002\n",
      "Epoch 9, Step 285/1750 (16.29%), Loss: 0.0003\n",
      "Epoch 9, Step 290/1750 (16.57%), Loss: 0.0001\n",
      "Epoch 9, Step 295/1750 (16.86%), Loss: 0.0001\n",
      "Epoch 9, Step 300/1750 (17.14%), Loss: 0.0001\n",
      "Epoch 9, Step 305/1750 (17.43%), Loss: 0.0001\n",
      "Epoch 9, Step 310/1750 (17.71%), Loss: 0.0002\n",
      "Epoch 9, Step 315/1750 (18.00%), Loss: 0.0002\n",
      "Epoch 9, Step 320/1750 (18.29%), Loss: 0.0005\n",
      "Epoch 9, Step 325/1750 (18.57%), Loss: 0.0002\n",
      "Epoch 9, Step 330/1750 (18.86%), Loss: 0.0002\n",
      "Epoch 9, Step 335/1750 (19.14%), Loss: 0.0001\n",
      "Epoch 9, Step 340/1750 (19.43%), Loss: 0.0002\n",
      "Epoch 9, Step 345/1750 (19.71%), Loss: 0.0003\n",
      "Epoch 9, Step 350/1750 (20.00%), Loss: 0.0002\n",
      "Epoch 9, Step 355/1750 (20.29%), Loss: 0.0002\n",
      "Epoch 9, Step 360/1750 (20.57%), Loss: 0.0001\n",
      "Epoch 9, Step 365/1750 (20.86%), Loss: 0.0002\n",
      "Epoch 9, Step 370/1750 (21.14%), Loss: 0.0001\n",
      "Epoch 9, Step 375/1750 (21.43%), Loss: 0.0003\n",
      "Epoch 9, Step 380/1750 (21.71%), Loss: 0.0001\n",
      "Epoch 9, Step 385/1750 (22.00%), Loss: 0.0002\n",
      "Epoch 9, Step 390/1750 (22.29%), Loss: 0.0010\n",
      "Epoch 9, Step 395/1750 (22.57%), Loss: 0.0001\n",
      "Epoch 9, Step 400/1750 (22.86%), Loss: 0.0001\n",
      "Epoch 9, Step 405/1750 (23.14%), Loss: 0.0002\n",
      "Epoch 9, Step 410/1750 (23.43%), Loss: 0.0002\n",
      "Epoch 9, Step 415/1750 (23.71%), Loss: 0.0010\n",
      "Epoch 9, Step 420/1750 (24.00%), Loss: 0.0001\n",
      "Epoch 9, Step 425/1750 (24.29%), Loss: 0.0001\n",
      "Epoch 9, Step 430/1750 (24.57%), Loss: 0.0002\n",
      "Epoch 9, Step 435/1750 (24.86%), Loss: 0.0002\n",
      "Epoch 9, Step 440/1750 (25.14%), Loss: 0.0001\n",
      "Epoch 9, Step 445/1750 (25.43%), Loss: 0.0002\n",
      "Epoch 9, Step 450/1750 (25.71%), Loss: 0.0002\n",
      "Epoch 9, Step 455/1750 (26.00%), Loss: 0.0001\n",
      "Epoch 9, Step 460/1750 (26.29%), Loss: 0.0002\n",
      "Epoch 9, Step 465/1750 (26.57%), Loss: 0.0001\n",
      "Epoch 9, Step 470/1750 (26.86%), Loss: 0.0002\n",
      "Epoch 9, Step 475/1750 (27.14%), Loss: 0.0001\n",
      "Epoch 9, Step 480/1750 (27.43%), Loss: 0.0001\n",
      "Epoch 9, Step 485/1750 (27.71%), Loss: 0.0002\n",
      "Epoch 9, Step 490/1750 (28.00%), Loss: 0.0002\n",
      "Epoch 9, Step 495/1750 (28.29%), Loss: 0.0002\n",
      "Epoch 9, Step 500/1750 (28.57%), Loss: 0.0001\n",
      "Epoch 9, Step 505/1750 (28.86%), Loss: 0.0031\n",
      "Epoch 9, Step 510/1750 (29.14%), Loss: 0.0001\n",
      "Epoch 9, Step 515/1750 (29.43%), Loss: 0.0013\n",
      "Epoch 9, Step 520/1750 (29.71%), Loss: 0.0001\n",
      "Epoch 9, Step 525/1750 (30.00%), Loss: 0.0001\n",
      "Epoch 9, Step 530/1750 (30.29%), Loss: 0.0001\n",
      "Epoch 9, Step 535/1750 (30.57%), Loss: 0.0001\n",
      "Epoch 9, Step 540/1750 (30.86%), Loss: 0.0001\n",
      "Epoch 9, Step 545/1750 (31.14%), Loss: 0.0002\n",
      "Epoch 9, Step 550/1750 (31.43%), Loss: 0.0001\n",
      "Epoch 9, Step 555/1750 (31.71%), Loss: 0.5516\n",
      "Epoch 9, Step 560/1750 (32.00%), Loss: 0.0001\n",
      "Epoch 9, Step 565/1750 (32.29%), Loss: 0.4981\n",
      "Epoch 9, Step 570/1750 (32.57%), Loss: 0.0003\n",
      "Epoch 9, Step 575/1750 (32.86%), Loss: 0.0003\n",
      "Epoch 9, Step 580/1750 (33.14%), Loss: 0.0004\n",
      "Epoch 9, Step 585/1750 (33.43%), Loss: 0.0003\n",
      "Epoch 9, Step 590/1750 (33.71%), Loss: 0.0004\n",
      "Epoch 9, Step 595/1750 (34.00%), Loss: 0.0003\n",
      "Epoch 9, Step 600/1750 (34.29%), Loss: 0.0002\n",
      "Epoch 9, Step 605/1750 (34.57%), Loss: 0.0005\n",
      "Epoch 9, Step 610/1750 (34.86%), Loss: 0.0005\n",
      "Epoch 9, Step 615/1750 (35.14%), Loss: 0.0083\n",
      "Epoch 9, Step 620/1750 (35.43%), Loss: 0.0003\n",
      "Epoch 9, Step 625/1750 (35.71%), Loss: 0.0005\n",
      "Epoch 9, Step 630/1750 (36.00%), Loss: 0.0004\n",
      "Epoch 9, Step 635/1750 (36.29%), Loss: 0.0005\n",
      "Epoch 9, Step 640/1750 (36.57%), Loss: 0.0003\n",
      "Epoch 9, Step 645/1750 (36.86%), Loss: 0.0003\n",
      "Epoch 9, Step 650/1750 (37.14%), Loss: 0.0002\n",
      "Epoch 9, Step 655/1750 (37.43%), Loss: 0.0005\n",
      "Epoch 9, Step 660/1750 (37.71%), Loss: 0.0005\n",
      "Epoch 9, Step 665/1750 (38.00%), Loss: 0.0003\n",
      "Epoch 9, Step 670/1750 (38.29%), Loss: 0.0004\n",
      "Epoch 9, Step 675/1750 (38.57%), Loss: 0.0003\n",
      "Epoch 9, Step 680/1750 (38.86%), Loss: 0.0003\n",
      "Epoch 9, Step 685/1750 (39.14%), Loss: 0.0005\n",
      "Epoch 9, Step 690/1750 (39.43%), Loss: 0.0003\n",
      "Epoch 9, Step 695/1750 (39.71%), Loss: 0.0004\n",
      "Epoch 9, Step 700/1750 (40.00%), Loss: 0.0003\n",
      "Epoch 9, Step 705/1750 (40.29%), Loss: 0.0005\n",
      "Epoch 9, Step 710/1750 (40.57%), Loss: 0.0003\n",
      "Epoch 9, Step 715/1750 (40.86%), Loss: 0.0005\n",
      "Epoch 9, Step 720/1750 (41.14%), Loss: 0.0002\n",
      "Epoch 9, Step 725/1750 (41.43%), Loss: 0.0003\n",
      "Epoch 9, Step 730/1750 (41.71%), Loss: 0.0003\n",
      "Epoch 9, Step 735/1750 (42.00%), Loss: 0.0004\n",
      "Epoch 9, Step 740/1750 (42.29%), Loss: 0.0004\n",
      "Epoch 9, Step 745/1750 (42.57%), Loss: 0.0007\n",
      "Epoch 9, Step 750/1750 (42.86%), Loss: 0.0004\n",
      "Epoch 9, Step 755/1750 (43.14%), Loss: 0.0005\n",
      "Epoch 9, Step 760/1750 (43.43%), Loss: 0.0003\n",
      "Epoch 9, Step 765/1750 (43.71%), Loss: 0.0004\n",
      "Epoch 9, Step 770/1750 (44.00%), Loss: 0.0005\n",
      "Epoch 9, Step 775/1750 (44.29%), Loss: 0.0005\n",
      "Epoch 9, Step 780/1750 (44.57%), Loss: 0.0005\n",
      "Epoch 9, Step 785/1750 (44.86%), Loss: 0.0002\n",
      "Epoch 9, Step 790/1750 (45.14%), Loss: 0.0009\n",
      "Epoch 9, Step 795/1750 (45.43%), Loss: 0.0004\n",
      "Epoch 9, Step 800/1750 (45.71%), Loss: 0.0007\n",
      "Epoch 9, Step 805/1750 (46.00%), Loss: 0.1210\n",
      "Epoch 9, Step 810/1750 (46.29%), Loss: 0.0003\n",
      "Epoch 9, Step 815/1750 (46.57%), Loss: 0.0006\n",
      "Epoch 9, Step 820/1750 (46.86%), Loss: 0.0002\n",
      "Epoch 9, Step 825/1750 (47.14%), Loss: 0.0014\n",
      "Epoch 9, Step 830/1750 (47.43%), Loss: 0.0008\n",
      "Epoch 9, Step 835/1750 (47.71%), Loss: 0.0004\n",
      "Epoch 9, Step 840/1750 (48.00%), Loss: 0.0007\n",
      "Epoch 9, Step 845/1750 (48.29%), Loss: 0.0009\n",
      "Epoch 9, Step 850/1750 (48.57%), Loss: 0.0039\n",
      "Epoch 9, Step 855/1750 (48.86%), Loss: 0.0012\n",
      "Epoch 9, Step 860/1750 (49.14%), Loss: 0.0007\n",
      "Epoch 9, Step 865/1750 (49.43%), Loss: 0.0009\n",
      "Epoch 9, Step 870/1750 (49.71%), Loss: 0.0004\n",
      "Epoch 9, Step 875/1750 (50.00%), Loss: 0.0010\n",
      "Epoch 9, Step 880/1750 (50.29%), Loss: 0.0007\n",
      "Epoch 9, Step 885/1750 (50.57%), Loss: 0.0008\n",
      "Epoch 9, Step 890/1750 (50.86%), Loss: 0.0008\n",
      "Epoch 9, Step 895/1750 (51.14%), Loss: 0.0004\n",
      "Epoch 9, Step 900/1750 (51.43%), Loss: 0.0004\n",
      "Epoch 9, Step 905/1750 (51.71%), Loss: 0.0005\n",
      "Epoch 9, Step 910/1750 (52.00%), Loss: 0.0004\n",
      "Epoch 9, Step 915/1750 (52.29%), Loss: 0.0006\n",
      "Epoch 9, Step 920/1750 (52.57%), Loss: 0.0005\n",
      "Epoch 9, Step 925/1750 (52.86%), Loss: 0.0019\n",
      "Epoch 9, Step 930/1750 (53.14%), Loss: 0.0006\n",
      "Epoch 9, Step 935/1750 (53.43%), Loss: 0.0006\n",
      "Epoch 9, Step 940/1750 (53.71%), Loss: 0.0005\n",
      "Epoch 9, Step 945/1750 (54.00%), Loss: 0.0007\n",
      "Epoch 9, Step 950/1750 (54.29%), Loss: 0.0006\n",
      "Epoch 9, Step 955/1750 (54.57%), Loss: 0.0229\n",
      "Epoch 9, Step 960/1750 (54.86%), Loss: 0.0009\n",
      "Epoch 9, Step 965/1750 (55.14%), Loss: 0.0005\n",
      "Epoch 9, Step 970/1750 (55.43%), Loss: 0.0005\n",
      "Epoch 9, Step 975/1750 (55.71%), Loss: 0.0006\n",
      "Epoch 9, Step 980/1750 (56.00%), Loss: 0.0007\n",
      "Epoch 9, Step 985/1750 (56.29%), Loss: 0.0006\n",
      "Epoch 9, Step 990/1750 (56.57%), Loss: 0.0010\n",
      "Epoch 9, Step 995/1750 (56.86%), Loss: 0.0005\n",
      "Epoch 9, Step 1000/1750 (57.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1005/1750 (57.43%), Loss: 0.0575\n",
      "Epoch 9, Step 1010/1750 (57.71%), Loss: 0.0006\n",
      "Epoch 9, Step 1015/1750 (58.00%), Loss: 0.0006\n",
      "Epoch 9, Step 1020/1750 (58.29%), Loss: 0.0004\n",
      "Epoch 9, Step 1025/1750 (58.57%), Loss: 0.0006\n",
      "Epoch 9, Step 1030/1750 (58.86%), Loss: 0.0009\n",
      "Epoch 9, Step 1035/1750 (59.14%), Loss: 0.0005\n",
      "Epoch 9, Step 1040/1750 (59.43%), Loss: 0.0006\n",
      "Epoch 9, Step 1045/1750 (59.71%), Loss: 0.0007\n",
      "Epoch 9, Step 1050/1750 (60.00%), Loss: 0.0006\n",
      "Epoch 9, Step 1055/1750 (60.29%), Loss: 0.0006\n",
      "Epoch 9, Step 1060/1750 (60.57%), Loss: 0.0005\n",
      "Epoch 9, Step 1065/1750 (60.86%), Loss: 0.0006\n",
      "Epoch 9, Step 1070/1750 (61.14%), Loss: 0.0008\n",
      "Epoch 9, Step 1075/1750 (61.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1080/1750 (61.71%), Loss: 0.0008\n",
      "Epoch 9, Step 1085/1750 (62.00%), Loss: 0.0005\n",
      "Epoch 9, Step 1090/1750 (62.29%), Loss: 0.0005\n",
      "Epoch 9, Step 1095/1750 (62.57%), Loss: 0.0004\n",
      "Epoch 9, Step 1100/1750 (62.86%), Loss: 0.0004\n",
      "Epoch 9, Step 1105/1750 (63.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1110/1750 (63.43%), Loss: 0.0004\n",
      "Epoch 9, Step 1115/1750 (63.71%), Loss: 0.0005\n",
      "Epoch 9, Step 1120/1750 (64.00%), Loss: 0.0003\n",
      "Epoch 9, Step 1125/1750 (64.29%), Loss: 0.0003\n",
      "Epoch 9, Step 1130/1750 (64.57%), Loss: 0.0004\n",
      "Epoch 9, Step 1135/1750 (64.86%), Loss: 0.0005\n",
      "Epoch 9, Step 1140/1750 (65.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1145/1750 (65.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1150/1750 (65.71%), Loss: 0.0002\n",
      "Epoch 9, Step 1155/1750 (66.00%), Loss: 0.0003\n",
      "Epoch 9, Step 1160/1750 (66.29%), Loss: 0.0200\n",
      "Epoch 9, Step 1165/1750 (66.57%), Loss: 0.0005\n",
      "Epoch 9, Step 1170/1750 (66.86%), Loss: 0.0003\n",
      "Epoch 9, Step 1175/1750 (67.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1180/1750 (67.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1185/1750 (67.71%), Loss: 0.0002\n",
      "Epoch 9, Step 1190/1750 (68.00%), Loss: 0.0001\n",
      "Epoch 9, Step 1195/1750 (68.29%), Loss: 0.0003\n",
      "Epoch 9, Step 1200/1750 (68.57%), Loss: 0.0003\n",
      "Epoch 9, Step 1205/1750 (68.86%), Loss: 0.0001\n",
      "Epoch 9, Step 1210/1750 (69.14%), Loss: 0.0006\n",
      "Epoch 9, Step 1215/1750 (69.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1220/1750 (69.71%), Loss: 0.0003\n",
      "Epoch 9, Step 1225/1750 (70.00%), Loss: 0.0004\n",
      "Epoch 9, Step 1230/1750 (70.29%), Loss: 0.0002\n",
      "Epoch 9, Step 1235/1750 (70.57%), Loss: 0.0002\n",
      "Epoch 9, Step 1240/1750 (70.86%), Loss: 0.0002\n",
      "Epoch 9, Step 1245/1750 (71.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1250/1750 (71.43%), Loss: 0.0003\n",
      "Epoch 9, Step 1255/1750 (71.71%), Loss: 0.0003\n",
      "Epoch 9, Step 1260/1750 (72.00%), Loss: 0.0003\n",
      "Epoch 9, Step 1265/1750 (72.29%), Loss: 0.0004\n",
      "Epoch 9, Step 1270/1750 (72.57%), Loss: 0.0005\n",
      "Epoch 9, Step 1275/1750 (72.86%), Loss: 0.0003\n",
      "Epoch 9, Step 1280/1750 (73.14%), Loss: 0.0002\n",
      "Epoch 9, Step 1285/1750 (73.43%), Loss: 0.0004\n",
      "Epoch 9, Step 1290/1750 (73.71%), Loss: 0.0002\n",
      "Epoch 9, Step 1295/1750 (74.00%), Loss: 0.0003\n",
      "Epoch 9, Step 1300/1750 (74.29%), Loss: 0.0003\n",
      "Epoch 9, Step 1305/1750 (74.57%), Loss: 0.0006\n",
      "Epoch 9, Step 1310/1750 (74.86%), Loss: 0.0004\n",
      "Epoch 9, Step 1315/1750 (75.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1320/1750 (75.43%), Loss: 0.0003\n",
      "Epoch 9, Step 1325/1750 (75.71%), Loss: 0.0003\n",
      "Epoch 9, Step 1330/1750 (76.00%), Loss: 0.0005\n",
      "Epoch 9, Step 1335/1750 (76.29%), Loss: 0.0005\n",
      "Epoch 9, Step 1340/1750 (76.57%), Loss: 0.0006\n",
      "Epoch 9, Step 1345/1750 (76.86%), Loss: 0.0005\n",
      "Epoch 9, Step 1350/1750 (77.14%), Loss: 0.0004\n",
      "Epoch 9, Step 1355/1750 (77.43%), Loss: 0.0002\n",
      "Epoch 9, Step 1360/1750 (77.71%), Loss: 0.0005\n",
      "Epoch 9, Step 1365/1750 (78.00%), Loss: 0.0010\n",
      "Epoch 9, Step 1370/1750 (78.29%), Loss: 0.0004\n",
      "Epoch 9, Step 1375/1750 (78.57%), Loss: 0.0005\n",
      "Epoch 9, Step 1380/1750 (78.86%), Loss: 0.0003\n",
      "Epoch 9, Step 1385/1750 (79.14%), Loss: 0.0072\n",
      "Epoch 9, Step 1390/1750 (79.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1395/1750 (79.71%), Loss: 0.0005\n",
      "Epoch 9, Step 1400/1750 (80.00%), Loss: 0.0004\n",
      "Epoch 9, Step 1405/1750 (80.29%), Loss: 0.0004\n",
      "Epoch 9, Step 1410/1750 (80.57%), Loss: 0.0003\n",
      "Epoch 9, Step 1415/1750 (80.86%), Loss: 0.0002\n",
      "Epoch 9, Step 1420/1750 (81.14%), Loss: 0.0004\n",
      "Epoch 9, Step 1425/1750 (81.43%), Loss: 0.0004\n",
      "Epoch 9, Step 1430/1750 (81.71%), Loss: 0.0002\n",
      "Epoch 9, Step 1435/1750 (82.00%), Loss: 0.0004\n",
      "Epoch 9, Step 1440/1750 (82.29%), Loss: 0.0003\n",
      "Epoch 9, Step 1445/1750 (82.57%), Loss: 0.0005\n",
      "Epoch 9, Step 1450/1750 (82.86%), Loss: 0.0004\n",
      "Epoch 9, Step 1455/1750 (83.14%), Loss: 0.0004\n",
      "Epoch 9, Step 1460/1750 (83.43%), Loss: 0.0003\n",
      "Epoch 9, Step 1465/1750 (83.71%), Loss: 0.0002\n",
      "Epoch 9, Step 1470/1750 (84.00%), Loss: 0.0007\n",
      "Epoch 9, Step 1475/1750 (84.29%), Loss: 0.0005\n",
      "Epoch 9, Step 1480/1750 (84.57%), Loss: 0.0005\n",
      "Epoch 9, Step 1485/1750 (84.86%), Loss: 0.0004\n",
      "Epoch 9, Step 1490/1750 (85.14%), Loss: 0.0004\n",
      "Epoch 9, Step 1495/1750 (85.43%), Loss: 0.0004\n",
      "Epoch 9, Step 1500/1750 (85.71%), Loss: 0.0006\n",
      "Epoch 9, Step 1505/1750 (86.00%), Loss: 0.0005\n",
      "Epoch 9, Step 1510/1750 (86.29%), Loss: 0.0011\n",
      "Epoch 9, Step 1515/1750 (86.57%), Loss: 0.0004\n",
      "Epoch 9, Step 1520/1750 (86.86%), Loss: 0.0004\n",
      "Epoch 9, Step 1525/1750 (87.14%), Loss: 0.0005\n",
      "Epoch 9, Step 1530/1750 (87.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1535/1750 (87.71%), Loss: 0.0003\n",
      "Epoch 9, Step 1540/1750 (88.00%), Loss: 0.0005\n",
      "Epoch 9, Step 1545/1750 (88.29%), Loss: 0.0003\n",
      "Epoch 9, Step 1550/1750 (88.57%), Loss: 0.0003\n",
      "Epoch 9, Step 1555/1750 (88.86%), Loss: 0.0004\n",
      "Epoch 9, Step 1560/1750 (89.14%), Loss: 0.0003\n",
      "Epoch 9, Step 1565/1750 (89.43%), Loss: 0.0004\n",
      "Epoch 9, Step 1570/1750 (89.71%), Loss: 0.0004\n",
      "Epoch 9, Step 1575/1750 (90.00%), Loss: 0.0003\n",
      "Epoch 9, Step 1580/1750 (90.29%), Loss: 0.0017\n",
      "Epoch 9, Step 1585/1750 (90.57%), Loss: 0.0003\n",
      "Epoch 9, Step 1590/1750 (90.86%), Loss: 0.0005\n",
      "Epoch 9, Step 1595/1750 (91.14%), Loss: 0.0004\n",
      "Epoch 9, Step 1600/1750 (91.43%), Loss: 0.0003\n",
      "Epoch 9, Step 1605/1750 (91.71%), Loss: 0.0003\n",
      "Epoch 9, Step 1610/1750 (92.00%), Loss: 0.0001\n",
      "Epoch 9, Step 1615/1750 (92.29%), Loss: 0.0006\n",
      "Epoch 9, Step 1620/1750 (92.57%), Loss: 0.0002\n",
      "Epoch 9, Step 1625/1750 (92.86%), Loss: 0.0003\n",
      "Epoch 9, Step 1630/1750 (93.14%), Loss: 0.0005\n",
      "Epoch 9, Step 1635/1750 (93.43%), Loss: 0.0004\n",
      "Epoch 9, Step 1640/1750 (93.71%), Loss: 0.0004\n",
      "Epoch 9, Step 1645/1750 (94.00%), Loss: 0.0002\n",
      "Epoch 9, Step 1650/1750 (94.29%), Loss: 0.0004\n",
      "Epoch 9, Step 1655/1750 (94.57%), Loss: 0.0015\n",
      "Epoch 9, Step 1660/1750 (94.86%), Loss: 0.1548\n",
      "Epoch 9, Step 1665/1750 (95.14%), Loss: 0.0005\n",
      "Epoch 9, Step 1670/1750 (95.43%), Loss: 0.0002\n",
      "Epoch 9, Step 1675/1750 (95.71%), Loss: 0.0004\n",
      "Epoch 9, Step 1680/1750 (96.00%), Loss: 0.0005\n",
      "Epoch 9, Step 1685/1750 (96.29%), Loss: 0.0005\n",
      "Epoch 9, Step 1690/1750 (96.57%), Loss: 0.0004\n",
      "Epoch 9, Step 1695/1750 (96.86%), Loss: 0.0003\n",
      "Epoch 9, Step 1700/1750 (97.14%), Loss: 0.4594\n",
      "Epoch 9, Step 1705/1750 (97.43%), Loss: 0.0006\n",
      "Epoch 9, Step 1710/1750 (97.71%), Loss: 0.0004\n",
      "Epoch 9, Step 1715/1750 (98.00%), Loss: 0.0009\n",
      "Epoch 9, Step 1720/1750 (98.29%), Loss: 0.0006\n",
      "Epoch 9, Step 1725/1750 (98.57%), Loss: 0.0006\n",
      "Epoch 9, Step 1730/1750 (98.86%), Loss: 0.0006\n",
      "Epoch 9, Step 1735/1750 (99.14%), Loss: 0.0006\n",
      "Epoch 9, Step 1740/1750 (99.43%), Loss: 0.0005\n",
      "Epoch 9, Step 1745/1750 (99.71%), Loss: 0.0005\n",
      "Epoch 9, Step 1750/1750 (100.00%), Loss: 0.0004\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.77\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.06\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.00\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.62\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 1.28\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.23\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.58\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.64\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.81\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.59\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.01\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.00\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.02\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 1.10\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.46\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.00\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.45\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.00\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.55\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.86\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.43\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.00\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.44\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.00\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.45\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 1.31\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.57\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.47\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.00\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.03\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.00\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.89\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.06\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.47\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.89\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.07\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.00\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.03\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.40\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.04\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 0.94\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.46\n",
      "End of Epoch 9/10 - Average Train Loss: 0.00, Average Validation Loss: 0.39\n",
      "Starting Epoch 10/10\n",
      "Epoch 10, Step 5/1750 (0.29%), Loss: 0.0004\n",
      "Epoch 10, Step 10/1750 (0.57%), Loss: 0.0008\n",
      "Epoch 10, Step 15/1750 (0.86%), Loss: 0.0007\n",
      "Epoch 10, Step 20/1750 (1.14%), Loss: 0.0006\n",
      "Epoch 10, Step 25/1750 (1.43%), Loss: 0.0006\n",
      "Epoch 10, Step 30/1750 (1.71%), Loss: 0.0015\n",
      "Epoch 10, Step 35/1750 (2.00%), Loss: 0.0009\n",
      "Epoch 10, Step 40/1750 (2.29%), Loss: 0.0005\n",
      "Epoch 10, Step 45/1750 (2.57%), Loss: 0.0005\n",
      "Epoch 10, Step 50/1750 (2.86%), Loss: 0.0005\n",
      "Epoch 10, Step 55/1750 (3.14%), Loss: 0.0008\n",
      "Epoch 10, Step 60/1750 (3.43%), Loss: 0.0006\n",
      "Epoch 10, Step 65/1750 (3.71%), Loss: 0.0004\n",
      "Epoch 10, Step 70/1750 (4.00%), Loss: 0.0005\n",
      "Epoch 10, Step 75/1750 (4.29%), Loss: 0.0004\n",
      "Epoch 10, Step 80/1750 (4.57%), Loss: 0.0006\n",
      "Epoch 10, Step 85/1750 (4.86%), Loss: 0.0004\n",
      "Epoch 10, Step 90/1750 (5.14%), Loss: 0.0004\n",
      "Epoch 10, Step 95/1750 (5.43%), Loss: 0.0006\n",
      "Epoch 10, Step 100/1750 (5.71%), Loss: 0.0004\n",
      "Epoch 10, Step 105/1750 (6.00%), Loss: 0.0006\n",
      "Epoch 10, Step 110/1750 (6.29%), Loss: 0.0004\n",
      "Epoch 10, Step 115/1750 (6.57%), Loss: 0.0005\n",
      "Epoch 10, Step 120/1750 (6.86%), Loss: 0.0004\n",
      "Epoch 10, Step 125/1750 (7.14%), Loss: 0.0004\n",
      "Epoch 10, Step 130/1750 (7.43%), Loss: 0.0004\n",
      "Epoch 10, Step 135/1750 (7.71%), Loss: 0.0008\n",
      "Epoch 10, Step 140/1750 (8.00%), Loss: 0.0005\n",
      "Epoch 10, Step 145/1750 (8.29%), Loss: 0.0003\n",
      "Epoch 10, Step 150/1750 (8.57%), Loss: 0.0004\n",
      "Epoch 10, Step 155/1750 (8.86%), Loss: 0.0004\n",
      "Epoch 10, Step 160/1750 (9.14%), Loss: 0.0004\n",
      "Epoch 10, Step 165/1750 (9.43%), Loss: 0.0003\n",
      "Epoch 10, Step 170/1750 (9.71%), Loss: 0.0005\n",
      "Epoch 10, Step 175/1750 (10.00%), Loss: 0.0004\n",
      "Epoch 10, Step 180/1750 (10.29%), Loss: 0.0007\n",
      "Epoch 10, Step 185/1750 (10.57%), Loss: 0.0007\n",
      "Epoch 10, Step 190/1750 (10.86%), Loss: 0.0006\n",
      "Epoch 10, Step 195/1750 (11.14%), Loss: 0.0004\n",
      "Epoch 10, Step 200/1750 (11.43%), Loss: 0.0004\n",
      "Epoch 10, Step 205/1750 (11.71%), Loss: 0.0002\n",
      "Epoch 10, Step 210/1750 (12.00%), Loss: 0.0005\n",
      "Epoch 10, Step 215/1750 (12.29%), Loss: 0.0042\n",
      "Epoch 10, Step 220/1750 (12.57%), Loss: 0.0005\n",
      "Epoch 10, Step 225/1750 (12.86%), Loss: 0.0007\n",
      "Epoch 10, Step 230/1750 (13.14%), Loss: 0.0758\n",
      "Epoch 10, Step 235/1750 (13.43%), Loss: 0.0005\n",
      "Epoch 10, Step 240/1750 (13.71%), Loss: 0.0006\n",
      "Epoch 10, Step 245/1750 (14.00%), Loss: 0.0005\n",
      "Epoch 10, Step 250/1750 (14.29%), Loss: 0.0033\n",
      "Epoch 10, Step 255/1750 (14.57%), Loss: 0.0004\n",
      "Epoch 10, Step 260/1750 (14.86%), Loss: 0.0005\n",
      "Epoch 10, Step 265/1750 (15.14%), Loss: 0.0003\n",
      "Epoch 10, Step 270/1750 (15.43%), Loss: 0.0006\n",
      "Epoch 10, Step 275/1750 (15.71%), Loss: 0.0005\n",
      "Epoch 10, Step 280/1750 (16.00%), Loss: 0.0011\n",
      "Epoch 10, Step 285/1750 (16.29%), Loss: 0.0005\n",
      "Epoch 10, Step 290/1750 (16.57%), Loss: 0.0007\n",
      "Epoch 10, Step 295/1750 (16.86%), Loss: 0.0004\n",
      "Epoch 10, Step 300/1750 (17.14%), Loss: 0.0011\n",
      "Epoch 10, Step 305/1750 (17.43%), Loss: 0.0009\n",
      "Epoch 10, Step 310/1750 (17.71%), Loss: 0.0010\n",
      "Epoch 10, Step 315/1750 (18.00%), Loss: 0.0012\n",
      "Epoch 10, Step 320/1750 (18.29%), Loss: 0.0005\n",
      "Epoch 10, Step 325/1750 (18.57%), Loss: 0.0007\n",
      "Epoch 10, Step 330/1750 (18.86%), Loss: 0.0007\n",
      "Epoch 10, Step 335/1750 (19.14%), Loss: 0.0010\n",
      "Epoch 10, Step 340/1750 (19.43%), Loss: 0.0008\n",
      "Epoch 10, Step 345/1750 (19.71%), Loss: 0.0004\n",
      "Epoch 10, Step 350/1750 (20.00%), Loss: 0.0008\n",
      "Epoch 10, Step 355/1750 (20.29%), Loss: 0.0003\n",
      "Epoch 10, Step 360/1750 (20.57%), Loss: 0.0009\n",
      "Epoch 10, Step 365/1750 (20.86%), Loss: 0.0007\n",
      "Epoch 10, Step 370/1750 (21.14%), Loss: 0.0006\n",
      "Epoch 10, Step 375/1750 (21.43%), Loss: 0.0011\n",
      "Epoch 10, Step 380/1750 (21.71%), Loss: 0.0009\n",
      "Epoch 10, Step 385/1750 (22.00%), Loss: 0.0004\n",
      "Epoch 10, Step 390/1750 (22.29%), Loss: 0.0007\n",
      "Epoch 10, Step 395/1750 (22.57%), Loss: 0.0008\n",
      "Epoch 10, Step 400/1750 (22.86%), Loss: 0.0007\n",
      "Epoch 10, Step 405/1750 (23.14%), Loss: 0.0007\n",
      "Epoch 10, Step 410/1750 (23.43%), Loss: 0.0011\n",
      "Epoch 10, Step 415/1750 (23.71%), Loss: 0.0004\n",
      "Epoch 10, Step 420/1750 (24.00%), Loss: 0.0004\n",
      "Epoch 10, Step 425/1750 (24.29%), Loss: 0.0008\n",
      "Epoch 10, Step 430/1750 (24.57%), Loss: 0.0006\n",
      "Epoch 10, Step 435/1750 (24.86%), Loss: 0.0006\n",
      "Epoch 10, Step 440/1750 (25.14%), Loss: 0.0005\n",
      "Epoch 10, Step 445/1750 (25.43%), Loss: 0.0004\n",
      "Epoch 10, Step 450/1750 (25.71%), Loss: 0.0005\n",
      "Epoch 10, Step 455/1750 (26.00%), Loss: 0.0005\n",
      "Epoch 10, Step 460/1750 (26.29%), Loss: 0.0007\n",
      "Epoch 10, Step 465/1750 (26.57%), Loss: 0.0005\n",
      "Epoch 10, Step 470/1750 (26.86%), Loss: 0.0004\n",
      "Epoch 10, Step 475/1750 (27.14%), Loss: 0.0008\n",
      "Epoch 10, Step 480/1750 (27.43%), Loss: 0.0005\n",
      "Epoch 10, Step 485/1750 (27.71%), Loss: 0.0007\n",
      "Epoch 10, Step 490/1750 (28.00%), Loss: 0.0004\n",
      "Epoch 10, Step 495/1750 (28.29%), Loss: 0.0004\n",
      "Epoch 10, Step 500/1750 (28.57%), Loss: 0.0004\n",
      "Epoch 10, Step 505/1750 (28.86%), Loss: 0.0008\n",
      "Epoch 10, Step 510/1750 (29.14%), Loss: 0.0006\n",
      "Epoch 10, Step 515/1750 (29.43%), Loss: 0.0004\n",
      "Epoch 10, Step 520/1750 (29.71%), Loss: 0.0007\n",
      "Epoch 10, Step 525/1750 (30.00%), Loss: 0.0004\n",
      "Epoch 10, Step 530/1750 (30.29%), Loss: 0.0005\n",
      "Epoch 10, Step 535/1750 (30.57%), Loss: 0.0005\n",
      "Epoch 10, Step 540/1750 (30.86%), Loss: 0.0005\n",
      "Epoch 10, Step 545/1750 (31.14%), Loss: 0.0009\n",
      "Epoch 10, Step 550/1750 (31.43%), Loss: 0.0004\n",
      "Epoch 10, Step 555/1750 (31.71%), Loss: 0.0005\n",
      "Epoch 10, Step 560/1750 (32.00%), Loss: 0.0004\n",
      "Epoch 10, Step 565/1750 (32.29%), Loss: 0.0004\n",
      "Epoch 10, Step 570/1750 (32.57%), Loss: 0.0005\n",
      "Epoch 10, Step 575/1750 (32.86%), Loss: 0.0003\n",
      "Epoch 10, Step 580/1750 (33.14%), Loss: 0.0005\n",
      "Epoch 10, Step 585/1750 (33.43%), Loss: 0.0003\n",
      "Epoch 10, Step 590/1750 (33.71%), Loss: 0.0004\n",
      "Epoch 10, Step 595/1750 (34.00%), Loss: 0.0015\n",
      "Epoch 10, Step 600/1750 (34.29%), Loss: 0.0004\n",
      "Epoch 10, Step 605/1750 (34.57%), Loss: 0.0004\n",
      "Epoch 10, Step 610/1750 (34.86%), Loss: 0.0004\n",
      "Epoch 10, Step 615/1750 (35.14%), Loss: 0.0002\n",
      "Epoch 10, Step 620/1750 (35.43%), Loss: 0.0004\n",
      "Epoch 10, Step 625/1750 (35.71%), Loss: 0.0002\n",
      "Epoch 10, Step 630/1750 (36.00%), Loss: 0.0005\n",
      "Epoch 10, Step 635/1750 (36.29%), Loss: 0.0003\n",
      "Epoch 10, Step 640/1750 (36.57%), Loss: 0.0005\n",
      "Epoch 10, Step 645/1750 (36.86%), Loss: 0.0004\n",
      "Epoch 10, Step 650/1750 (37.14%), Loss: 0.0003\n",
      "Epoch 10, Step 655/1750 (37.43%), Loss: 0.0004\n",
      "Epoch 10, Step 660/1750 (37.71%), Loss: 0.0004\n",
      "Epoch 10, Step 665/1750 (38.00%), Loss: 0.0001\n",
      "Epoch 10, Step 670/1750 (38.29%), Loss: 0.0002\n",
      "Epoch 10, Step 675/1750 (38.57%), Loss: 0.0004\n",
      "Epoch 10, Step 680/1750 (38.86%), Loss: 0.0003\n",
      "Epoch 10, Step 685/1750 (39.14%), Loss: 0.0004\n",
      "Epoch 10, Step 690/1750 (39.43%), Loss: 0.0003\n",
      "Epoch 10, Step 695/1750 (39.71%), Loss: 0.0006\n",
      "Epoch 10, Step 700/1750 (40.00%), Loss: 0.0005\n",
      "Epoch 10, Step 705/1750 (40.29%), Loss: 0.0003\n",
      "Epoch 10, Step 710/1750 (40.57%), Loss: 0.0003\n",
      "Epoch 10, Step 715/1750 (40.86%), Loss: 0.0003\n",
      "Epoch 10, Step 720/1750 (41.14%), Loss: 0.0006\n",
      "Epoch 10, Step 725/1750 (41.43%), Loss: 0.0003\n",
      "Epoch 10, Step 730/1750 (41.71%), Loss: 0.0003\n",
      "Epoch 10, Step 735/1750 (42.00%), Loss: 0.0007\n",
      "Epoch 10, Step 740/1750 (42.29%), Loss: 0.0006\n",
      "Epoch 10, Step 745/1750 (42.57%), Loss: 0.0004\n",
      "Epoch 10, Step 750/1750 (42.86%), Loss: 0.0006\n",
      "Epoch 10, Step 755/1750 (43.14%), Loss: 0.0005\n",
      "Epoch 10, Step 760/1750 (43.43%), Loss: 0.0064\n",
      "Epoch 10, Step 765/1750 (43.71%), Loss: 0.0003\n",
      "Epoch 10, Step 770/1750 (44.00%), Loss: 0.0006\n",
      "Epoch 10, Step 775/1750 (44.29%), Loss: 0.0010\n",
      "Epoch 10, Step 780/1750 (44.57%), Loss: 0.0005\n",
      "Epoch 10, Step 785/1750 (44.86%), Loss: 0.0004\n",
      "Epoch 10, Step 790/1750 (45.14%), Loss: 0.0004\n",
      "Epoch 10, Step 795/1750 (45.43%), Loss: 0.0003\n",
      "Epoch 10, Step 800/1750 (45.71%), Loss: 0.0007\n",
      "Epoch 10, Step 805/1750 (46.00%), Loss: 0.0005\n",
      "Epoch 10, Step 810/1750 (46.29%), Loss: 0.0003\n",
      "Epoch 10, Step 815/1750 (46.57%), Loss: 0.0004\n",
      "Epoch 10, Step 820/1750 (46.86%), Loss: 0.0005\n",
      "Epoch 10, Step 825/1750 (47.14%), Loss: 0.0004\n",
      "Epoch 10, Step 830/1750 (47.43%), Loss: 0.0006\n",
      "Epoch 10, Step 835/1750 (47.71%), Loss: 0.0003\n",
      "Epoch 10, Step 840/1750 (48.00%), Loss: 0.0002\n",
      "Epoch 10, Step 845/1750 (48.29%), Loss: 0.0006\n",
      "Epoch 10, Step 850/1750 (48.57%), Loss: 0.0002\n",
      "Epoch 10, Step 855/1750 (48.86%), Loss: 0.0004\n",
      "Epoch 10, Step 860/1750 (49.14%), Loss: 0.0003\n",
      "Epoch 10, Step 865/1750 (49.43%), Loss: 0.0003\n",
      "Epoch 10, Step 870/1750 (49.71%), Loss: 0.0003\n",
      "Epoch 10, Step 875/1750 (50.00%), Loss: 0.0005\n",
      "Epoch 10, Step 880/1750 (50.29%), Loss: 0.0005\n",
      "Epoch 10, Step 885/1750 (50.57%), Loss: 0.0006\n",
      "Epoch 10, Step 890/1750 (50.86%), Loss: 0.0007\n",
      "Epoch 10, Step 895/1750 (51.14%), Loss: 0.0006\n",
      "Epoch 10, Step 900/1750 (51.43%), Loss: 0.0005\n",
      "Epoch 10, Step 905/1750 (51.71%), Loss: 0.0004\n",
      "Epoch 10, Step 910/1750 (52.00%), Loss: 0.0005\n",
      "Epoch 10, Step 915/1750 (52.29%), Loss: 0.0004\n",
      "Epoch 10, Step 920/1750 (52.57%), Loss: 0.0005\n",
      "Epoch 10, Step 925/1750 (52.86%), Loss: 0.0006\n",
      "Epoch 10, Step 930/1750 (53.14%), Loss: 0.0004\n",
      "Epoch 10, Step 935/1750 (53.43%), Loss: 0.0005\n",
      "Epoch 10, Step 940/1750 (53.71%), Loss: 0.0004\n",
      "Epoch 10, Step 945/1750 (54.00%), Loss: 0.0003\n",
      "Epoch 10, Step 950/1750 (54.29%), Loss: 0.0003\n",
      "Epoch 10, Step 955/1750 (54.57%), Loss: 0.0006\n",
      "Epoch 10, Step 960/1750 (54.86%), Loss: 0.0003\n",
      "Epoch 10, Step 965/1750 (55.14%), Loss: 0.0005\n",
      "Epoch 10, Step 970/1750 (55.43%), Loss: 0.0004\n",
      "Epoch 10, Step 975/1750 (55.71%), Loss: 0.0004\n",
      "Epoch 10, Step 980/1750 (56.00%), Loss: 0.0012\n",
      "Epoch 10, Step 985/1750 (56.29%), Loss: 0.0003\n",
      "Epoch 10, Step 990/1750 (56.57%), Loss: 0.0004\n",
      "Epoch 10, Step 995/1750 (56.86%), Loss: 0.0002\n",
      "Epoch 10, Step 1000/1750 (57.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1005/1750 (57.43%), Loss: 0.0002\n",
      "Epoch 10, Step 1010/1750 (57.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1015/1750 (58.00%), Loss: 0.0005\n",
      "Epoch 10, Step 1020/1750 (58.29%), Loss: 0.0005\n",
      "Epoch 10, Step 1025/1750 (58.57%), Loss: 0.0109\n",
      "Epoch 10, Step 1030/1750 (58.86%), Loss: 0.0002\n",
      "Epoch 10, Step 1035/1750 (59.14%), Loss: 0.0002\n",
      "Epoch 10, Step 1040/1750 (59.43%), Loss: 0.0003\n",
      "Epoch 10, Step 1045/1750 (59.71%), Loss: 0.0005\n",
      "Epoch 10, Step 1050/1750 (60.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1055/1750 (60.29%), Loss: 0.0002\n",
      "Epoch 10, Step 1060/1750 (60.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1065/1750 (60.86%), Loss: 0.0004\n",
      "Epoch 10, Step 1070/1750 (61.14%), Loss: 0.0004\n",
      "Epoch 10, Step 1075/1750 (61.43%), Loss: 0.0006\n",
      "Epoch 10, Step 1080/1750 (61.71%), Loss: 0.0013\n",
      "Epoch 10, Step 1085/1750 (62.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1090/1750 (62.29%), Loss: 0.0006\n",
      "Epoch 10, Step 1095/1750 (62.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1100/1750 (62.86%), Loss: 0.0003\n",
      "Epoch 10, Step 1105/1750 (63.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1110/1750 (63.43%), Loss: 0.0003\n",
      "Epoch 10, Step 1115/1750 (63.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1120/1750 (64.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1125/1750 (64.29%), Loss: 0.0004\n",
      "Epoch 10, Step 1130/1750 (64.57%), Loss: 0.0004\n",
      "Epoch 10, Step 1135/1750 (64.86%), Loss: 0.0007\n",
      "Epoch 10, Step 1140/1750 (65.14%), Loss: 0.0006\n",
      "Epoch 10, Step 1145/1750 (65.43%), Loss: 0.0011\n",
      "Epoch 10, Step 1150/1750 (65.71%), Loss: 0.0005\n",
      "Epoch 10, Step 1155/1750 (66.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1160/1750 (66.29%), Loss: 0.0003\n",
      "Epoch 10, Step 1165/1750 (66.57%), Loss: 0.0002\n",
      "Epoch 10, Step 1170/1750 (66.86%), Loss: 0.0021\n",
      "Epoch 10, Step 1175/1750 (67.14%), Loss: 0.0004\n",
      "Epoch 10, Step 1180/1750 (67.43%), Loss: 0.0009\n",
      "Epoch 10, Step 1185/1750 (67.71%), Loss: 0.4170\n",
      "Epoch 10, Step 1190/1750 (68.00%), Loss: 0.0006\n",
      "Epoch 10, Step 1195/1750 (68.29%), Loss: 0.0005\n",
      "Epoch 10, Step 1200/1750 (68.57%), Loss: 0.0003\n",
      "Epoch 10, Step 1205/1750 (68.86%), Loss: 0.0007\n",
      "Epoch 10, Step 1210/1750 (69.14%), Loss: 0.0004\n",
      "Epoch 10, Step 1215/1750 (69.43%), Loss: 0.0003\n",
      "Epoch 10, Step 1220/1750 (69.71%), Loss: 0.0005\n",
      "Epoch 10, Step 1225/1750 (70.00%), Loss: 0.0005\n",
      "Epoch 10, Step 1230/1750 (70.29%), Loss: 0.0018\n",
      "Epoch 10, Step 1235/1750 (70.57%), Loss: 0.0009\n",
      "Epoch 10, Step 1240/1750 (70.86%), Loss: 0.0005\n",
      "Epoch 10, Step 1245/1750 (71.14%), Loss: 0.0007\n",
      "Epoch 10, Step 1250/1750 (71.43%), Loss: 0.0008\n",
      "Epoch 10, Step 1255/1750 (71.71%), Loss: 0.0006\n",
      "Epoch 10, Step 1260/1750 (72.00%), Loss: 0.0005\n",
      "Epoch 10, Step 1265/1750 (72.29%), Loss: 0.0003\n",
      "Epoch 10, Step 1270/1750 (72.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1275/1750 (72.86%), Loss: 0.0007\n",
      "Epoch 10, Step 1280/1750 (73.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1285/1750 (73.43%), Loss: 0.0006\n",
      "Epoch 10, Step 1290/1750 (73.71%), Loss: 0.0006\n",
      "Epoch 10, Step 1295/1750 (74.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1300/1750 (74.29%), Loss: 0.0006\n",
      "Epoch 10, Step 1305/1750 (74.57%), Loss: 0.0004\n",
      "Epoch 10, Step 1310/1750 (74.86%), Loss: 0.0006\n",
      "Epoch 10, Step 1315/1750 (75.14%), Loss: 0.0007\n",
      "Epoch 10, Step 1320/1750 (75.43%), Loss: 0.0004\n",
      "Epoch 10, Step 1325/1750 (75.71%), Loss: 0.0006\n",
      "Epoch 10, Step 1330/1750 (76.00%), Loss: 0.0007\n",
      "Epoch 10, Step 1335/1750 (76.29%), Loss: 0.0005\n",
      "Epoch 10, Step 1340/1750 (76.57%), Loss: 0.0004\n",
      "Epoch 10, Step 1345/1750 (76.86%), Loss: 0.0008\n",
      "Epoch 10, Step 1350/1750 (77.14%), Loss: 0.0006\n",
      "Epoch 10, Step 1355/1750 (77.43%), Loss: 0.0004\n",
      "Epoch 10, Step 1360/1750 (77.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1365/1750 (78.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1370/1750 (78.29%), Loss: 0.0005\n",
      "Epoch 10, Step 1375/1750 (78.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1380/1750 (78.86%), Loss: 0.0004\n",
      "Epoch 10, Step 1385/1750 (79.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1390/1750 (79.43%), Loss: 0.0007\n",
      "Epoch 10, Step 1395/1750 (79.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1400/1750 (80.00%), Loss: 0.0005\n",
      "Epoch 10, Step 1405/1750 (80.29%), Loss: 0.0005\n",
      "Epoch 10, Step 1410/1750 (80.57%), Loss: 0.0005\n",
      "Epoch 10, Step 1415/1750 (80.86%), Loss: 0.0005\n",
      "Epoch 10, Step 1420/1750 (81.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1425/1750 (81.43%), Loss: 0.0006\n",
      "Epoch 10, Step 1430/1750 (81.71%), Loss: 0.0006\n",
      "Epoch 10, Step 1435/1750 (82.00%), Loss: 0.0006\n",
      "Epoch 10, Step 1440/1750 (82.29%), Loss: 0.0006\n",
      "Epoch 10, Step 1445/1750 (82.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1450/1750 (82.86%), Loss: 0.0006\n",
      "Epoch 10, Step 1455/1750 (83.14%), Loss: 0.0008\n",
      "Epoch 10, Step 1460/1750 (83.43%), Loss: 0.0004\n",
      "Epoch 10, Step 1465/1750 (83.71%), Loss: 0.0107\n",
      "Epoch 10, Step 1470/1750 (84.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1475/1750 (84.29%), Loss: 0.0004\n",
      "Epoch 10, Step 1480/1750 (84.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1485/1750 (84.86%), Loss: 0.0004\n",
      "Epoch 10, Step 1490/1750 (85.14%), Loss: 0.0007\n",
      "Epoch 10, Step 1495/1750 (85.43%), Loss: 0.0005\n",
      "Epoch 10, Step 1500/1750 (85.71%), Loss: 0.0005\n",
      "Epoch 10, Step 1505/1750 (86.00%), Loss: 0.0005\n",
      "Epoch 10, Step 1510/1750 (86.29%), Loss: 0.0004\n",
      "Epoch 10, Step 1515/1750 (86.57%), Loss: 0.0006\n",
      "Epoch 10, Step 1520/1750 (86.86%), Loss: 0.0002\n",
      "Epoch 10, Step 1525/1750 (87.14%), Loss: 0.0004\n",
      "Epoch 10, Step 1530/1750 (87.43%), Loss: 0.0006\n",
      "Epoch 10, Step 1535/1750 (87.71%), Loss: 0.0006\n",
      "Epoch 10, Step 1540/1750 (88.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1545/1750 (88.29%), Loss: 0.0003\n",
      "Epoch 10, Step 1550/1750 (88.57%), Loss: 0.0003\n",
      "Epoch 10, Step 1555/1750 (88.86%), Loss: 0.0005\n",
      "Epoch 10, Step 1560/1750 (89.14%), Loss: 0.0006\n",
      "Epoch 10, Step 1565/1750 (89.43%), Loss: 0.0003\n",
      "Epoch 10, Step 1570/1750 (89.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1575/1750 (90.00%), Loss: 0.0006\n",
      "Epoch 10, Step 1580/1750 (90.29%), Loss: 0.0005\n",
      "Epoch 10, Step 1585/1750 (90.57%), Loss: 0.0004\n",
      "Epoch 10, Step 1590/1750 (90.86%), Loss: 0.0004\n",
      "Epoch 10, Step 1595/1750 (91.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1600/1750 (91.43%), Loss: 0.0004\n",
      "Epoch 10, Step 1605/1750 (91.71%), Loss: 0.0007\n",
      "Epoch 10, Step 1610/1750 (92.00%), Loss: 0.0006\n",
      "Epoch 10, Step 1615/1750 (92.29%), Loss: 0.0004\n",
      "Epoch 10, Step 1620/1750 (92.57%), Loss: 0.0005\n",
      "Epoch 10, Step 1625/1750 (92.86%), Loss: 0.0007\n",
      "Epoch 10, Step 1630/1750 (93.14%), Loss: 0.0005\n",
      "Epoch 10, Step 1635/1750 (93.43%), Loss: 0.0004\n",
      "Epoch 10, Step 1640/1750 (93.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1645/1750 (94.00%), Loss: 0.0003\n",
      "Epoch 10, Step 1650/1750 (94.29%), Loss: 0.0006\n",
      "Epoch 10, Step 1655/1750 (94.57%), Loss: 0.0007\n",
      "Epoch 10, Step 1660/1750 (94.86%), Loss: 0.0002\n",
      "Epoch 10, Step 1665/1750 (95.14%), Loss: 0.0004\n",
      "Epoch 10, Step 1670/1750 (95.43%), Loss: 0.0003\n",
      "Epoch 10, Step 1675/1750 (95.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1680/1750 (96.00%), Loss: 0.0005\n",
      "Epoch 10, Step 1685/1750 (96.29%), Loss: 0.0004\n",
      "Epoch 10, Step 1690/1750 (96.57%), Loss: 0.0004\n",
      "Epoch 10, Step 1695/1750 (96.86%), Loss: 0.0003\n",
      "Epoch 10, Step 1700/1750 (97.14%), Loss: 0.0019\n",
      "Epoch 10, Step 1705/1750 (97.43%), Loss: 0.0004\n",
      "Epoch 10, Step 1710/1750 (97.71%), Loss: 0.0004\n",
      "Epoch 10, Step 1715/1750 (98.00%), Loss: 0.0004\n",
      "Epoch 10, Step 1720/1750 (98.29%), Loss: 0.0003\n",
      "Epoch 10, Step 1725/1750 (98.57%), Loss: 0.0002\n",
      "Epoch 10, Step 1730/1750 (98.86%), Loss: 0.0003\n",
      "Epoch 10, Step 1735/1750 (99.14%), Loss: 0.0006\n",
      "Epoch 10, Step 1740/1750 (99.43%), Loss: 0.0005\n",
      "Epoch 10, Step 1745/1750 (99.71%), Loss: 0.0005\n",
      "Epoch 10, Step 1750/1750 (100.00%), Loss: 0.0006\n",
      "Validation Step 10/438 (2.28%), Validation Loss: 0.77\n",
      "Validation Step 20/438 (4.57%), Validation Loss: 0.09\n",
      "Validation Step 30/438 (6.85%), Validation Loss: 0.00\n",
      "Validation Step 40/438 (9.13%), Validation Loss: 0.64\n",
      "Validation Step 50/438 (11.42%), Validation Loss: 1.23\n",
      "Validation Step 60/438 (13.70%), Validation Loss: 0.20\n",
      "Validation Step 70/438 (15.98%), Validation Loss: 0.60\n",
      "Validation Step 80/438 (18.26%), Validation Loss: 0.63\n",
      "Validation Step 90/438 (20.55%), Validation Loss: 0.82\n",
      "Validation Step 100/438 (22.83%), Validation Loss: 0.62\n",
      "Validation Step 110/438 (25.11%), Validation Loss: 0.00\n",
      "Validation Step 120/438 (27.40%), Validation Loss: 0.00\n",
      "Validation Step 130/438 (29.68%), Validation Loss: 0.00\n",
      "Validation Step 140/438 (31.96%), Validation Loss: 0.03\n",
      "Validation Step 150/438 (34.25%), Validation Loss: 1.26\n",
      "Validation Step 160/438 (36.53%), Validation Loss: 0.46\n",
      "Validation Step 170/438 (38.81%), Validation Loss: 0.00\n",
      "Validation Step 180/438 (41.10%), Validation Loss: 0.48\n",
      "Validation Step 190/438 (43.38%), Validation Loss: 0.00\n",
      "Validation Step 200/438 (45.66%), Validation Loss: 0.75\n",
      "Validation Step 210/438 (47.95%), Validation Loss: 0.88\n",
      "Validation Step 220/438 (50.23%), Validation Loss: 0.58\n",
      "Validation Step 230/438 (52.51%), Validation Loss: 0.00\n",
      "Validation Step 240/438 (54.79%), Validation Loss: 0.43\n",
      "Validation Step 250/438 (57.08%), Validation Loss: 0.00\n",
      "Validation Step 260/438 (59.36%), Validation Loss: 0.47\n",
      "Validation Step 270/438 (61.64%), Validation Loss: 1.27\n",
      "Validation Step 280/438 (63.93%), Validation Loss: 0.51\n",
      "Validation Step 290/438 (66.21%), Validation Loss: 0.48\n",
      "Validation Step 300/438 (68.49%), Validation Loss: 0.00\n",
      "Validation Step 310/438 (70.78%), Validation Loss: 0.02\n",
      "Validation Step 320/438 (73.06%), Validation Loss: 0.01\n",
      "Validation Step 330/438 (75.34%), Validation Loss: 0.95\n",
      "Validation Step 340/438 (77.63%), Validation Loss: 0.03\n",
      "Validation Step 350/438 (79.91%), Validation Loss: 0.48\n",
      "Validation Step 360/438 (82.19%), Validation Loss: 1.94\n",
      "Validation Step 370/438 (84.47%), Validation Loss: 0.09\n",
      "Validation Step 380/438 (86.76%), Validation Loss: 0.00\n",
      "Validation Step 390/438 (89.04%), Validation Loss: 0.09\n",
      "Validation Step 400/438 (91.32%), Validation Loss: 0.38\n",
      "Validation Step 410/438 (93.61%), Validation Loss: 0.09\n",
      "Validation Step 420/438 (95.89%), Validation Loss: 1.02\n",
      "Validation Step 430/438 (98.17%), Validation Loss: 0.47\n",
      "End of Epoch 10/10 - Average Train Loss: 0.00, Average Validation Loss: 0.40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "# get cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "#use data loader \n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, num_workers=0)\n",
    "\n",
    "# set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 20  \n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_steps = len(train_loader)\n",
    "    print(f\"Starting Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  \n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % 5 == 0:  \n",
    "            progress = (step + 1) / total_train_steps * 100\n",
    "            print(f\"Epoch {epoch + 1}, Step {step + 1}/{total_train_steps} ({progress:.2f}%), Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_steps = len(val_loader)\n",
    "\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "#print status \n",
    "        if (step + 1) % 10 == 0:  \n",
    "            progress = (step + 1) / total_val_steps * 100\n",
    "            print(f\"Validation Step {step + 1}/{total_val_steps} ({progress:.2f}%), Validation Loss: {loss.item():.2f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_train_steps\n",
    "    avg_val_loss = total_val_loss / total_val_steps\n",
    "\n",
    "    print(f\"End of Epoch {epoch + 1}/{epochs} - Average Train Loss: {avg_train_loss:.2f}, Average Validation Loss: {avg_val_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved fine-tuned model\n",
    "torch.save(model, 'db10.pt')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Comments JSON to Parquet",
   "widgets": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
